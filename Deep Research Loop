"""
title: Deep Research Loop
id: deepresearch.loop
author: OpenWebUI Deep Research
version: 12.6.22.0
license: MIT
required_open_webui_version: 0.5.0
description: |
  Deep Research Pipe (v12.6.22.0):
  - Scope (main model decides if confirm/ask is needed; avoid repetitive questioning)
  - Plan steps (internal, no blocking questions)
  - Execute: tool-loop generation
  - Judge: recommendation + dynamic next-prompt/plan update (tools disabled)
  - Decider: main model makes final decision: done / continue / ask (single critical question only)
  - Chunk long outputs (reply "继续" to get next part; will show status when detected)
  - Robust base_model recovery:
      metadata.deep_research.base_model / metadata.dr_original_model / <dr_router> tag / fallback_base_model
  - Strong session isolation by deep_research.session_id (prevents cross-chat leakage)
  - Manual stop detection: emit "暂停原因：用户暂停" and treat next input as supplemental
  - Persist chart snippets from tools and auto-insert into final output (CHART_MARKDOWN markers)
  - Finalize deliverable (not report-only): model chooses best deliverable form (report / code / plan / etc.)
  - Multi-pass stitching for ultra-long code (EOF marker + auto-continue)
  - Stitch review & patch for ultra-long code: LLM generates patch ops (replace/delete/insert) to fix broken stitching (no full regeneration)
  - Artifact Edit Mode (交付物修改模式): read baseline -> plan patch stages -> generate multiple patches -> apply to original (no full rewrite) -> output final full artifact
  - Artifact Edit Mode: per-stage retry + strict-anchor escalation; critical-stage blocking (no silent skip); attempt-level status logs
  - Artifact Edit Mode: auto-preserve & auto-recover EOF marker (DR_EOF) after patch application (prevents endless eof_marker_missing retries)
  - Multimodal images: do NOT inline image_url/base64 into text (prevents mis-detecting screenshots as pasted code)
  - Artifact cache: persist last deliverable to disk and recover it when chat history is truncated (chunked; no truncation; avoids asking for baseline again)

  - Timing statistics (status + optional append to final)

  Important:
  - Internal LLM calls use generate_chat_completion(..., bypass_filter=False) so other filters run.
  - Recursion avoided by setting metadata.dr_internal_call=True in internal calls.
"""

import asyncio
import json
import re
import time
import inspect
import hashlib
import os
import tempfile
from copy import deepcopy
from types import SimpleNamespace
from typing import Any, Dict, List, Optional, Tuple, Callable, Awaitable
from datetime import datetime, timezone

from pydantic import BaseModel, Field
from open_webui.utils.chat import generate_chat_completion

try:
    from open_webui.models.users import Users
except Exception:
    Users = None


# ----------------------------
# In-process session memory
# ----------------------------
_SESS: Dict[str, Dict[str, Any]] = {}
_SESS_TTL_SEC = 8 * 60 * 60  # 8h


# ----------------------------
# Time helpers
# ----------------------------
def _now() -> float:
    return time.time()


def _cleanup_sess():
    t = _now()
    dead = []
    for k, v in list(_SESS.items()):
        try:
            if t - float(v.get("updated_at", 0)) > _SESS_TTL_SEC:
                dead.append(k)
        except Exception:
            dead.append(k)
    for k in dead:
        _SESS.pop(k, None)


def _utc_iso() -> str:
    try:
        return datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    except Exception:
        return ""


def _local_iso() -> str:
    try:
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        return ""


def _looks_time_sensitive(text: str) -> bool:
    t = (text or "").strip().lower()
    if not t:
        return False
    keys = [
        "今天",
        "现在",
        "当前",
        "此刻",
        "刚刚",
        "最新",
        "最近",
        "近一周",
        "近一月",
        "近1个月",
        "近3个月",
        "近半年",
        "本周",
        "本月",
        "今年",
        "昨日",
        "昨天",
        "明天",
        "实时",
        "live",
        "latest",
        "today",
        "now",
        "current",
        "recent",
        "as of",
        "year to date",
        "ytd",
    ]
    return any(k in t for k in keys)


def _build_time_context_block() -> str:
    loc = _local_iso()
    utc = _utc_iso()
    parts = []
    if loc:
        parts.append(f"本机时间：{loc}")
    if utc:
        parts.append(f"UTC：{utc}")
    if not parts:
        return ""
    return (
        "【时间锚点】\n"
        + "\n".join([f"- {p}" for p in parts])
        + "\n（如涉及“今天/最近/近X个月/最新/实时”等，请以该时间为准表述“截至/截至近似时间”。）\n"
    )


# ----------------------------
# Content helpers
# ----------------------------
def _norm_content(c: Any) -> str:
    """Normalize message content to plain text.

    IMPORTANT: In OpenWebUI / OpenAI-style multimodal messages, images are often
    carried as data URLs (e.g. data:image/png;base64,....). We must NOT inline
    those base64 payloads into text, otherwise they may be mistaken as "user
    pasted a huge code blob".

    Strategy:
      - Keep text parts.
      - Replace image parts with a tiny placeholder like "[图片]".
      - Avoid stringifying dict parts that contain image_url/base64.
    """
    if c is None:
        return ""
    if isinstance(c, str):
        # Avoid exploding the prompt when a frontend embeds an image as a data URL string.
        # Only collapse when the entire content is an image data URL / markdown image.
        s = c.strip()
        if (
            (s.startswith("data:image") and "base64," in s and len(s) > 200)
            or (s.startswith("![") and "data:image" in s and len(s) > 200)
            or (s.startswith("![](") and "data:image" in s and len(s) > 200)
        ):
            return "[图片]"
        return c
    if isinstance(c, bytes):
        try:
            return c.decode("utf-8", errors="ignore")
        except Exception:
            return str(c)

    # OpenAI-style multimodal list: [{"type":"text","text":...}, {"type":"image_url", ...}]
    if isinstance(c, list):
        out = []
        for x in c:
            if x is None:
                continue
            if isinstance(x, str):
                if x.strip():
                    out.append(x)
                continue
            if isinstance(x, dict):
                t = str(x.get("type") or "").strip().lower()

                # Image parts: never dump the base64/url
                if t in (
                    "image_url",
                    "input_image",
                    "image",
                    "image_base64",
                    "imageurl",
                    "image-data",
                ):
                    out.append("[图片]")
                    continue
                if ("image_url" in x and isinstance(x.get("image_url"), dict)) or (
                    "image" in x and x.get("image") is not None
                ):
                    out.append("[图片]")
                    continue

                # Text parts
                if t in ("text", "input_text"):
                    if isinstance(x.get("text"), str):
                        out.append(x.get("text") or "")
                    elif isinstance(x.get("content"), str):
                        out.append(x.get("content") or "")
                    else:
                        out.append(_norm_content(x.get("text")))
                    continue

                # Fallback: only take text-like fields; avoid dumping the whole dict
                if isinstance(x.get("text"), str):
                    out.append(x.get("text") or "")
                elif isinstance(x.get("content"), str):
                    out.append(x.get("content") or "")
                else:
                    # Last resort: stringify, but strip giant data URLs
                    s = str(x)
                    if ("data:image" in s and "base64" in s and len(s) > 300) or (
                        "image_url" in s and "data:image" in s
                    ):
                        out.append("[图片]")
                    else:
                        out.append(s)
                continue

            out.append(str(x))
        return "\n".join([s for s in out if str(s).strip()])

    if isinstance(c, dict):
        t = str(c.get("type") or "").strip().lower()
        if t in (
            "image_url",
            "input_image",
            "image",
            "image_base64",
            "imageurl",
            "image-data",
        ):
            return "[图片]"
        if "image_url" in c and isinstance(c.get("image_url"), dict):
            return "[图片]"
        if "image" in c and c.get("image") is not None:
            return "[图片]"
        if isinstance(c.get("text"), str):
            return c["text"]
        if isinstance(c.get("content"), str):
            return c["content"]

    return str(c)


def _safe_last_user_message(messages: List[dict]) -> Optional[dict]:
    for m in reversed(messages or []):
        if isinstance(m, dict) and m.get("role") == "user":
            return m
    return None


def _safe_last_user_text(messages: List[dict]) -> str:
    m = _safe_last_user_message(messages)
    if not m:
        return ""
    return _norm_content(m.get("content")).strip()


def _smart_truncate_middle(text: str, max_chars: int = 1200) -> str:
    """Truncate long texts but keep both head and tail.

    This is intentionally newline-preserving so code snippets remain recognizable.
    """
    s = _norm_content(text)
    if max_chars <= 0 or len(s) <= max_chars:
        return s
    marker = "\n…（中间省略）…\n"
    # keep enough context on both sides
    keep = max_chars - len(marker)
    if keep <= 200:
        return s[: max_chars - 1] + "…"
    head = max(200, keep // 2)
    tail = max(200, keep - head)
    return s[:head].rstrip() + marker + s[-tail:].lstrip()


def _build_conversation_context(
    messages: List[dict],
    *,
    max_messages: int = 12,
    max_chars: int = 9000,
    per_message_max_chars: int = 1200,
    include_assistant: bool = True,
    exclude_last_user: bool = True,
) -> str:
    """Build a compact labeled chat context block.

    Used when Deep Research is enabled *mid-conversation* so the scope step can
    see prior context instead of only the latest user message.
    """
    if not isinstance(messages, list) or not messages:
        return ""

    # Take tail window
    tail = (
        messages[-max_messages:]
        if (max_messages and len(messages) > max_messages)
        else list(messages)
    )

    # Optionally remove the last user message (it will be provided separately as user_text)
    if exclude_last_user:
        for i in range(len(tail) - 1, -1, -1):
            m = tail[i]
            if isinstance(m, dict) and m.get("role") == "user":
                tail = tail[:i]
                break

    lines: List[str] = []
    for m in tail:
        if not isinstance(m, dict):
            continue
        role = str(m.get("role") or "").strip().lower()
        if role not in {"user", "assistant"}:
            continue
        if role == "assistant" and not include_assistant:
            continue
        txt = _norm_content(m.get("content")).strip()
        if not txt:
            continue
        if per_message_max_chars and len(txt) > per_message_max_chars:
            txt = _smart_truncate_middle(txt, int(per_message_max_chars))
        prefix = "用户" if role == "user" else "助手"
        lines.append(f"{prefix}：\n{txt}")

    ctx = "\n\n".join(lines).strip()
    if not ctx:
        return ""
    if max_chars and len(ctx) > max_chars:
        ctx = "…（上文略）…\n" + ctx[-int(max_chars) :]
    return ctx


def _messages_context_stats(messages: Any) -> Dict[str, Any]:
    # Stats about what context we actually received from the frontend.
    stats: Dict[str, Any] = {
        "msg_n": 0,
        "user_n": 0,
        "assistant_n": 0,
        "system_n": 0,
        "other_n": 0,
        "text_chars": 0,
        "user_chars": 0,
        "assistant_chars": 0,
        "image_placeholders": 0,
    }
    if not isinstance(messages, list):
        return stats
    stats["msg_n"] = len(messages)
    for m in messages:
        if not isinstance(m, dict):
            continue
        role = str(m.get("role") or "")
        txt = _norm_content(m.get("content"))
        ln = len(txt)
        stats["text_chars"] += ln
        stats["image_placeholders"] += txt.count("[图片]")
        if role == "user":
            stats["user_n"] += 1
            stats["user_chars"] += ln
        elif role == "assistant":
            stats["assistant_n"] += 1
            stats["assistant_chars"] += ln
        elif role == "system":
            stats["system_n"] += 1
        else:
            stats["other_n"] += 1
    return stats


# ----------------------------
# Context stitch helpers (history store + retrieval)
# ----------------------------

_DATA_URI_INLINE_RE = re.compile(
    r"data:(?:image|audio|video)/[a-z0-9.+-]+;base64,[A-Za-z0-9+/=]{200,}",
    re.IGNORECASE,
)

_TOKEN_RETRIEVE_RE = re.compile(r"[a-z0-9_]+|[\u4e00-\u9fff]+", re.IGNORECASE)


def _collapse_inline_data_uris(text: str) -> str:
    """Collapse huge inline data:*;base64,... blobs inside a longer text string."""
    if not isinstance(text, str) or not text:
        return text if isinstance(text, str) else ""
    try:
        return _DATA_URI_INLINE_RE.sub("data:<omitted>;base64,[...]", text)
    except Exception:
        return text


def _smart_truncate_middle(text: str, max_chars: int) -> str:
    if not isinstance(text, str):
        text = str(text)
    if not isinstance(max_chars, int) or max_chars <= 0:
        return text
    if len(text) <= max_chars:
        return text
    # Keep more head than tail (head is usually more informative for intents)
    head = int(max_chars * 0.62)
    tail = max(0, max_chars - head - 40)
    if tail <= 0:
        return text[:max_chars]
    return text[:head] + "\n…[TRUNCATED]…\n" + text[-tail:]


def _history_store_make_id(role: str, text: str) -> str:
    try:
        h = hashlib.sha1(
            (str(role) + "\n" + (text or "")).encode("utf-8", "ignore")
        ).hexdigest()
        return h[:16]
    except Exception:
        return str(abs(hash((role, text))))[:16]


def _history_store_ingest(
    store: Any,
    messages: Any,
    *,
    max_messages: int = 240,
    max_chars: int = 240000,
    per_message_max_chars: int = 12000,
) -> List[Dict[str, Any]]:
    """Append messages into a rolling store so we can still 'see all' later."""
    if not isinstance(store, list):
        store = []
    if not isinstance(messages, list):
        return store

    seen: set = set()
    for e in store:
        if isinstance(e, dict) and e.get("id"):
            seen.add(str(e["id"]))

    for m in messages:
        if not isinstance(m, dict):
            continue
        role = str(m.get("role") or "").strip().lower()
        if role not in ("user", "assistant"):
            continue
        txt = _norm_content(m.get("content"))
        if not isinstance(txt, str):
            txt = str(txt)
        txt = _collapse_inline_data_uris(txt).strip()
        if not txt:
            continue
        if per_message_max_chars and len(txt) > per_message_max_chars:
            txt = _smart_truncate_middle(txt, int(per_message_max_chars))
        mid = _history_store_make_id(role, txt)
        if mid in seen:
            continue
        store.append({"id": mid, "role": role, "text": txt})
        seen.add(mid)

    # trim by count
    if isinstance(max_messages, int) and max_messages > 0 and len(store) > max_messages:
        store = store[-max_messages:]

    # trim by chars
    if isinstance(max_chars, int) and max_chars > 0:
        total = 0
        for e in store:
            if isinstance(e, dict) and isinstance(e.get("text"), str):
                total += len(e["text"])
        while store and total > max_chars:
            e0 = store.pop(0)
            if isinstance(e0, dict) and isinstance(e0.get("text"), str):
                total -= len(e0["text"])

    return store


def _tokenize_for_retrieval(text: str) -> List[str]:
    if not isinstance(text, str) or not text.strip():
        return []
    s = text.lower()
    tokens: List[str] = []
    try:
        parts = _TOKEN_RETRIEVE_RE.findall(s)
    except Exception:
        parts = s.split()

    for p in parts:
        if not p:
            continue
        if re.fullmatch(r"[a-z0-9_]+", p):
            tokens.append(p)
        else:
            # Chinese segment -> bigrams help partial match
            if len(p) <= 2:
                tokens.append(p)
            else:
                tokens.extend([p[i : i + 2] for i in range(len(p) - 1)])
    return tokens


def _bm25_rank_docs(
    docs: List[str], query: str, *, top_k: int = 8
) -> List[Tuple[int, float]]:
    """Return list of (doc_index, score) sorted desc."""
    if not docs or not isinstance(docs, list):
        return []
    q_tokens = _tokenize_for_retrieval(query)
    if not q_tokens:
        return []

    # local import to avoid global dependency issues
    import math

    N = len(docs)
    # df
    df: Dict[str, int] = {}
    doc_tokens_list: List[List[str]] = []
    doc_lens: List[int] = []

    for d in docs:
        dt = _tokenize_for_retrieval((d or "")[:8000])  # cap per doc for speed
        doc_tokens_list.append(dt)
        dl = len(dt)
        doc_lens.append(dl)
        for t in set(dt):
            df[t] = df.get(t, 0) + 1

    avgdl = (sum(doc_lens) / N) if N else 0.0
    if avgdl <= 0:
        avgdl = 1.0

    k1 = 1.4
    b = 0.75

    # query tf
    q_tf: Dict[str, int] = {}
    for t in q_tokens:
        q_tf[t] = q_tf.get(t, 0) + 1

    scores: List[Tuple[int, float]] = []
    for i, dt in enumerate(doc_tokens_list):
        if not dt:
            continue
        tf: Dict[str, int] = {}
        for t in dt:
            tf[t] = tf.get(t, 0) + 1
        dl = doc_lens[i] or 1
        score = 0.0
        for t, qn in q_tf.items():
            f = tf.get(t, 0)
            if f <= 0:
                continue
            dft = df.get(t, 0)
            # idf: BM25+
            idf = math.log(1.0 + (N - dft + 0.5) / (dft + 0.5))
            denom = f + k1 * (1.0 - b + b * (dl / avgdl))
            score += idf * (f * (k1 + 1.0) / (denom or 1.0)) * (1.0 + 0.15 * (qn - 1))
        if score > 0:
            scores.append((i, float(score)))

    scores.sort(key=lambda x: x[1], reverse=True)
    if isinstance(top_k, int) and top_k > 0:
        return scores[:top_k]
    return scores


def _select_relevant_history_excerpt(
    store: Any,
    query: str,
    *,
    top_k: int = 8,
    max_chars: int = 7000,
    per_msg_max_chars: int = 900,
) -> str:
    """Select top-k relevant messages (from cached history) and build an excerpt."""
    if not isinstance(store, list) or not store:
        return ""

    docs: List[str] = []
    for e in store:
        if isinstance(e, dict):
            docs.append(str(e.get("text") or ""))
        else:
            docs.append(str(e))

    ranked = _bm25_rank_docs(docs, query, top_k=max(1, int(top_k)) * 2)
    if not ranked:
        return ""

    picked: List[int] = []
    for idx, score in ranked:
        # avoid picking the exact current query itself (often last user msg)
        try:
            if docs[idx].strip() == (query or "").strip():
                continue
        except Exception:
            pass
        picked.append(idx)
        if len(picked) >= max(1, int(top_k)):
            break

    if not picked:
        return ""

    picked = sorted(set(picked))

    blocks: List[str] = []
    for i in picked:
        e = store[i] if i < len(store) else None
        if not isinstance(e, dict):
            continue
        role = str(e.get("role") or "")
        txt = str(e.get("text") or "").strip()
        if not txt:
            continue
        if per_msg_max_chars and len(txt) > per_msg_max_chars:
            txt = _smart_truncate_middle(txt, int(per_msg_max_chars))
        label = "用户" if role == "user" else ("助手" if role == "assistant" else role)
        blocks.append(f"[{i+1}/{len(store)} {label}]\n{txt}")

    out = "\n\n".join(blocks).strip()
    if not out:
        return ""

    if isinstance(max_chars, int) and max_chars > 0 and len(out) > max_chars:
        out = _smart_truncate_middle(out, max_chars)
    return out


_ARTIFACT_SLICE_KEYMAP: List[Tuple[str, List[str]]] = [
    (
        "文字",
        ["fillText", "strokeText", "measureText", "font", "textBaseline", "textAlign"],
    ),
    (
        "重影",
        [
            "shadowBlur",
            "shadowColor",
            "globalAlpha",
            "ctx.save",
            "ctx.restore",
            "clearRect",
            "filter",
        ],
    ),
    (
        "按钮",
        [
            "addEventListener",
            "onclick",
            "touchstart",
            "touchend",
            "pointerdown",
            "button",
            "classList",
        ],
    ),
    ("点击", ["addEventListener", "click", "touchstart", "pointerdown"]),
    (
        "启动",
        [
            "DOMContentLoaded",
            "init",
            "initGame",
            "loadAssets",
            "requestAnimationFrame",
            "gameLoop",
        ],
    ),
    ("速度", ["playerSpeed", "moveSpeed", "speed", "vx", "vy", "deltaTime"]),
    ("关卡", ["levels", "initLevel", "level", "stage"]),
    ("剧情", ["dialogue", "story", "showDialogue", "dialog", "cutscene"]),
    ("音效", ["Audio", "play", "volume", "currentTime"]),
    ("碰撞", ["checkCollision", "collision", "isColliding"]),
]


def _artifact_slice_keywords(user_msg: str) -> List[str]:
    if not isinstance(user_msg, str) or not user_msg.strip():
        return []
    kws: List[str] = []
    seen = set()

    # map CN intent words -> code-ish terms
    for cn, terms in _ARTIFACT_SLICE_KEYMAP:
        if cn and cn in user_msg:
            for t in terms:
                tl = t.lower()
                if tl not in seen:
                    kws.append(t)
                    seen.add(tl)

    # raw ascii identifiers from user msg (function names etc.)
    for t in re.findall(r"[A-Za-z_][A-Za-z0-9_]{2,}", user_msg):
        tl = t.lower()
        if tl not in seen:
            kws.append(t)
            seen.add(tl)

    # a few evergreen anchors
    for t in ("DR_EOF", "<script", "</script>", "<canvas", "canvas", "ctx."):
        tl = t.lower()
        if tl not in seen:
            kws.append(t)
            seen.add(tl)

    # cap
    return kws[:18]


def _select_artifact_slices(
    artifact_text: str,
    user_msg: str,
    *,
    max_slices: int = 8,
    window_lines: int = 34,
) -> List[Dict[str, Any]]:
    if not isinstance(artifact_text, str) or not artifact_text.strip():
        return []
    if not isinstance(user_msg, str) or not user_msg.strip():
        return []

    kws = _artifact_slice_keywords(user_msg)
    if not kws:
        return []

    try:
        lines = artifact_text.splitlines()
    except Exception:
        return []

    if not lines:
        return []

    # compile regex for speed
    pat_terms = []
    for k in kws:
        if not k:
            continue
        try:
            pat_terms.append(re.escape(k))
        except Exception:
            continue
    if not pat_terms:
        return []

    try:
        pat = re.compile("|".join(pat_terms), re.IGNORECASE)
    except Exception:
        pat = None

    half = max(6, int(window_lines) // 2)
    out: List[Dict[str, Any]] = []
    seen_spans = set()

    for i, line in enumerate(lines):
        if pat is None:
            continue
        m = pat.search(line or "")
        if not m:
            continue
        start = max(0, i - half)
        end = min(len(lines), i + half + 1)
        key = (start, end)
        if key in seen_spans:
            continue
        seen_spans.add(key)
        excerpt = "\n".join(lines[start:end])
        out.append(
            {
                "match": m.group(0),
                "start_line": start + 1,
                "end_line": end,
                "excerpt": excerpt,
            }
        )
        if len(out) >= max(1, int(max_slices)):
            break

    return out


def _summarize_messages_tail(
    messages: Any, max_items: int = 8, snippet_chars: int = 80
) -> str:
    # One-line tail preview for debugging context truncation.
    if not isinstance(messages, list) or not messages:
        return ""
    max_items = max(1, min(30, int(max_items or 8)))
    snippet_chars = max(10, min(240, int(snippet_chars or 80)))
    start = max(0, len(messages) - max_items)
    parts: List[str] = []
    for i in range(start, len(messages)):
        m = messages[i]
        if not isinstance(m, dict):
            continue
        role = str(m.get("role") or "?")
        raw = _norm_content(m.get("content"))
        one = " ".join(str(raw).split())
        if len(one) > snippet_chars:
            one = one[:snippet_chars].rstrip() + "…"
        parts.append(f"{i}:{role}({len(raw)}c){(' ' + one) if one else ''}")
    return " | ".join(parts)


def _extract_prev_deliverable_from_chat_history(
    messages: List[dict], *, max_scan: int = 60
) -> Dict[str, Any]:
    """Recover the most likely last *code deliverable* from chat history.

    Why this exists:
    - Deep Research can be enabled *mid-chat* (no in-memory session yet).
    - The server may restart and lose _SESS.
    - Some upstreams don't provide a stable session_id.

    We try to be conservative and prefer strong signals first:
    1) Code containing an explicit DR_EOF marker
    2) A complete HTML document (<!DOCTYPE html> ... </html>)
    3) The largest fenced code block in recent assistant messages

    Returns: {"deliverable": str, "spec": dict, "from": str} or {}.
    """

    def _as_spec(lang: str, *, need_eof: bool = True) -> Dict[str, Any]:
        l = (lang or "").strip().lower() or "text"
        return {
            "deliverable_type": "code",
            "code_language": l,
            "need_eof_marker": bool(need_eof),
            "eof_marker": "DR_EOF" if need_eof else "",
        }

    def _is_full_html(s: str) -> bool:
        low = (s or "").lower()
        return ("<!doctype" in low or "<html" in low) and ("</html>" in low)

    def _looks_like_code_container(txt: str) -> bool:
        if not txt:
            return False
        low = txt.lower()
        if "```" in txt:
            return True
        if "<!doctype" in low or "<html" in low or "</html>" in low:
            return True
        # light heuristics for code-ish messages
        if "function" in low or "class " in low or "import " in low:
            return True
        if "{" in txt and "}" in txt and txt.count("\n") >= 10:
            return True
        return False

    try:
        rev = list(reversed(messages or []))
    except Exception:
        rev = []

    # Pass 1: explicit DR_EOF marker
    seen = 0
    for m in rev:
        if not isinstance(m, dict) or m.get("role") not in ("assistant", "user"):
            continue
        txt = _norm_content(m.get("content")).strip()
        if not txt:
            continue
        seen += 1
        if seen > int(max_scan or 60):
            break
        if "DR_EOF" not in txt:
            continue
        try:
            lang, code = _extract_code_candidate(txt)
            code_norm = _normalize_code_candidate(code, lang)
            if code_norm and "DR_EOF" in code_norm:
                return {
                    "deliverable": code_norm,
                    "spec": _as_spec(lang, need_eof=True),
                    "from": "assistant_with_eof",
                }
        except Exception:
            # If extraction fails, still return raw so user can recover.
            return {
                "deliverable": txt,
                "spec": {"deliverable_type": "text"},
                "from": "assistant_with_eof_raw",
            }

    # Pass 2: complete HTML document
    seen = 0
    for m in rev:
        if not isinstance(m, dict) or m.get("role") not in ("assistant", "user"):
            continue
        txt = _norm_content(m.get("content")).strip()
        if not txt:
            continue
        seen += 1
        if seen > int(max_scan or 60):
            break
        if not _looks_like_code_container(txt):
            continue
        try:
            lang, code = _extract_code_candidate(txt, prefer_lang="html")
            code_norm = _normalize_code_candidate(code, "html")
            if code_norm and _is_full_html(code_norm):
                need_eof = "DR_EOF" in code_norm
                return {
                    "deliverable": code_norm,
                    "spec": _as_spec("html", need_eof=True),
                    "from": "assistant_full_html",
                }
        except Exception:
            pass

    # Pass 3: largest code block candidate
    best: Dict[str, Any] = {}
    best_len = 0
    seen = 0
    for m in rev:
        if not isinstance(m, dict) or m.get("role") not in ("assistant", "user"):
            continue
        txt = _norm_content(m.get("content")).strip()
        if not txt:
            continue
        seen += 1
        if seen > int(max_scan or 60):
            break
        if not _looks_like_code_container(txt):
            continue
        try:
            lang, code = _extract_code_candidate(txt)
            code_norm = _normalize_code_candidate(code, lang)
            if not code_norm:
                continue
            # Avoid treating short prose as "code"
            if len(code_norm) < 400 and code_norm.count("\n") < 6:
                continue
            if len(code_norm) > best_len:
                best_len = len(code_norm)
                best = {
                    "deliverable": code_norm,
                    "spec": _as_spec(lang, need_eof=True),
                    "from": "assistant_largest_code",
                }
        except Exception:
            continue

    return best or {}


def _looks_like_continue(text: str) -> bool:
    """Strict continue command detector.

    IMPORTANT: Do NOT treat long messages like "继续增加内容..." as a continue command.
    Only return True for short, near-exact continue commands (optionally with punctuation).
    """
    t = (text or "").strip().lower()
    if not t:
        return False

    # Normalize whitespace and common trailing punctuation
    t2 = re.sub(r"\s+", "", t)
    t2 = t2.strip("。.!！?？…·,，;；:")

    # Exact/near-exact commands
    exact = {
        "继续",
        "继续输出",
        "继续生成",
        "继续写",
        "继续执行",
        "接着",
        "接着来",
        "下一段",
        "下一个",
        "续写",
        "next",
        "continue",
        "goon",
        "more",
    }
    if t2 in exact:
        return True

    # Allow very short variants that start with a command and are still short.
    # e.g. "继续吧" / "继续一下" / "继续哦". Reject if it contains obvious task verbs.
    if len(t2) <= 6 and (t2.startswith("继续") or t2.startswith("接着")):
        # If user is asking to modify something (e.g., 增加/修复), it's NOT a pure continue.
        if any(k in t2 for k in ("增加", "修改", "更新", "修复", "优化", "改", "补充")):
            return False
        return True

    return False


def _looks_like_followup_request(text: str) -> bool:
    """Heuristic: user is referring to prior content and wants to continue/modify/fix it."""
    t = (text or "").strip().lower()
    if not t:
        return False

    if _looks_like_continue(t):
        return True

    # Strong follow-up / patch verbs
    keys = [
        "继续",
        "接着",
        "续",
        "补充",
        "追加",
        "增加",
        "完善",
        "优化",
        "调整",
        "改",
        "修改",
        "更新",
        "重构",
        "修复",
        "fix",
        "bug",
        "报错",
        "error",
        "启动不了",
        "无法启动",
        "不能启动",
        "运行不了",
        "无法运行",
        "崩溃",
        "闪退",
        "卡住",
    ]
    if any(k in t for k in keys):
        return True

    # References to previous outputs
    refs = [
        "上面",
        "上述",
        "刚才",
        "之前",
        "上一版",
        "前一版",
        "上一个",
        "那份",
        "这份代码",
        "这段代码",
        "那段代码",
        "那个文件",
        "这个文件",
    ]
    if any(r in t for r in refs):
        return True

    return False


def _looks_like_reset(text: str) -> bool:
    t = (text or "").strip().lower()
    keys = {"/reset", "reset", "重置", "清空", "重新开始", "从头开始"}
    return t in keys


def _looks_like_accept_any(text: str) -> bool:
    t = (text or "").strip().lower()
    if not t:
        return False
    keys = [
        "随便",
        "都可以",
        "你决定",
        "你看着办",
        "按你说的",
        "我接受",
        "全部可以",
        "都行",
        "无所谓",
        "ok",
        "okay",
        "可以",
        "whatever",
        "you decide",
        "any",
    ]
    return any(k in t for k in keys)


def _looks_like_affirm(text: str) -> bool:
    t = (text or "").strip().lower()
    keys = [
        "是",
        "对",
        "没错",
        "正确",
        "可以",
        "行",
        "好",
        "好的",
        "ok",
        "okay",
        "y",
        "yes",
        "继续",
        "随便",
        "都可以",
    ]
    return any((t == k or k in t) for k in keys)


def _looks_like_negative(text: str) -> bool:
    """Detect explicit 'cancel/stop' intent (NOT any sentence containing '不').

    We intentionally keep this strict to avoid misclassifying phrases like '不太敢买' as 'no'.
    """
    t = (text or "").strip().lower()
    if not t:
        return False

    # explicit cancellation / stop words (exact)
    exact = {
        "取消",
        "取消吧",
        "撤销",
        "撤销吧",
        "停止",
        "停止吧",
        "停",
        "停一下",
        "先停",
        "先停止",
        "不做了",
        "算了",
        "不用了",
        "不需要",
        "不需要了",
        "不继续",
        "别做了",
        "退出",
        "cancel",
        "abort",
        "stop",
    }
    if t in exact:
        return True

    # prefix patterns
    prefixes = (
        "取消",
        "撤销",
        "停止",
        "先停",
        "先停止",
        "不继续",
        "别做",
        "不要做",
        "不用做",
        "退出",
    )
    if any(t.startswith(p) for p in prefixes):
        return True

    # contains patterns (still strict)
    contains = (
        "取消研究",
        "停止研究",
        "停止深度研究",
        "不用研究",
        "不用做了",
        "不想研究了",
        "不继续了",
    )
    if any(c in t for c in contains):
        return True

    return False


def _safe_json_loads(s: Any) -> Any:
    if not isinstance(s, str):
        return None
    try:
        return json.loads(s)
    except Exception:
        return None


def _extract_json_object(text: str) -> Optional[dict]:
    if not text or not isinstance(text, str):
        return None
    # fenced json
    m = re.search(r"```(?:json)?\s*({[\s\S]*?})\s*```", text, re.IGNORECASE)
    if m:
        try:
            return json.loads(m.group(1))
        except Exception:
            pass
    # <json>...</json>
    m = re.search(r"<json>\s*({[\s\S]*?})\s*</json>", text, re.IGNORECASE)
    if m:
        try:
            return json.loads(m.group(1))
        except Exception:
            pass
    # first {...}
    m = re.search(r"({[\s\S]*})", text)
    if m:
        try:
            return json.loads(m.group(1))
        except Exception:
            return None
    return None


def _as_bool(v: Any, default: bool = False) -> bool:
    if isinstance(v, bool):
        return v
    if isinstance(v, int):
        return bool(v)
    if isinstance(v, str):
        s = v.strip().lower()
        if s in ("true", "1", "yes", "y", "on"):
            return True
        if s in ("false", "0", "no", "n", "off"):
            return False
    return bool(default)


def _as_int(v: Any, default: int) -> int:
    try:
        if v is None:
            return int(default)
        if isinstance(v, bool):
            return int(default)
        return int(str(v).strip())
    except Exception:
        return int(default)


def _as_float(v: Any, default: float) -> float:
    try:
        if v is None:
            return float(default)
        if isinstance(v, bool):
            return float(default)
        return float(str(v).strip())
    except Exception:
        return float(default)


# ----------------------------
# Artifact cache (persist last deliverable to disk)
# ----------------------------


def _is_writable_dir(path: str) -> bool:
    try:
        if not path:
            return False
        if not os.path.isdir(path):
            return False
        test_path = os.path.join(path, ".dr_write_test")
        with open(test_path, "w", encoding="utf-8") as f:
            f.write("ok")
        os.remove(test_path)
        return True
    except Exception:
        return False


def _ensure_dir(path: str) -> bool:
    try:
        if not path:
            return False
        os.makedirs(path, exist_ok=True)
        return True
    except Exception:
        return False


def _pick_artifact_store_root(explicit: str = "") -> str:
    """Pick a writable root directory for artifact cache.

    Priority:
    1) explicit dir from open_webui.config
    2) env dirs
    3) common OpenWebUI data dirs
    4) home
    5) temp

    Returns empty string if everything fails.
    """

    if explicit and isinstance(explicit, str):
        p = explicit.strip()
        if p:
            try:
                os.makedirs(p, exist_ok=True)
                return p
            except Exception:
                pass

    for env in (
        "OPENWEBUI_DATA_DIR",
        "DATA_DIR",
        "APP_DATA_DIR",
        "OPENWEBUI_APP_DATA_DIR",
    ):
        d = os.environ.get(env)
        if isinstance(d, str) and d.strip():
            base = d.strip()
            try:
                os.makedirs(base, exist_ok=True)
            except Exception:
                pass
            if os.path.isdir(base):
                root = os.path.join(base, "deepresearch_artifacts")
                try:
                    os.makedirs(root, exist_ok=True)
                    return root
                except Exception:
                    pass

    for base in (
        "/app/backend/data",
        "/data",
        "/mnt/data",
        os.path.join(os.getcwd(), "data"),
        os.getcwd(),
    ):
        try:
            if base and os.path.isdir(base):
                root = os.path.join(base, "deepresearch_artifacts")
                os.makedirs(root, exist_ok=True)
                return root
        except Exception:
            continue

    try:
        home = os.path.expanduser("~")
        if home and os.path.isdir(home):
            root = os.path.join(home, ".openwebui", "deepresearch_artifacts")
            os.makedirs(root, exist_ok=True)
            return root
    except Exception:
        pass

    try:
        root = os.path.join(tempfile.gettempdir(), "deepresearch_artifacts")
        os.makedirs(root, exist_ok=True)
        return root
    except Exception:
        return ""


def _artifact_index_paths(root: str):
    idx_path = os.path.join(root, "index.json")
    art_dir = os.path.join(root, "artifacts")
    return idx_path, art_dir


def _artifact_index_load(root: str) -> Dict[str, Any]:
    idx_path, art_dir = _artifact_index_paths(root)
    try:
        os.makedirs(art_dir, exist_ok=True)
    except Exception:
        pass

    if not idx_path or not os.path.exists(idx_path):
        return {"version": 1, "sessions": {}, "by_chat_id": {}, "artifacts": {}}

    try:
        with open(idx_path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        if not isinstance(obj, dict):
            raise ValueError("index not dict")
        if not isinstance(obj.get("version"), int):
            obj["version"] = 1
        if not isinstance(obj.get("sessions"), dict):
            obj["sessions"] = {}
        if not isinstance(obj.get("by_chat_id"), dict):
            obj["by_chat_id"] = {}
        if not isinstance(obj.get("artifacts"), dict):
            obj["artifacts"] = {}
        return obj
    except Exception:
        return {"version": 1, "sessions": {}, "by_chat_id": {}, "artifacts": {}}


def _artifact_index_save(root: str, index: Dict[str, Any]) -> None:
    idx_path, _ = _artifact_index_paths(root)
    if not idx_path:
        return
    try:
        tmp = idx_path + ".tmp"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        os.replace(tmp, idx_path)
    except Exception:
        try:
            with open(idx_path, "w", encoding="utf-8") as f:
                json.dump(index, f, ensure_ascii=False, indent=2)
        except Exception:
            pass


def _artifact_id_for_text(text: str) -> str:
    try:
        h = hashlib.sha256((text or "").encode("utf-8", "ignore")).hexdigest()
        return h[:16]
    except Exception:
        return str(abs(hash(text or "")))[:16]


def _artifact_cache_write(
    *,
    root: str,
    session_id: str,
    chat_id: str,
    content: str,
    spec: Optional[dict],
    max_chars: int = 0,
    history_max: int = 20,
) -> Optional[Dict[str, Any]]:
    """Persist the latest deliverable to disk.

    IMPORTANT (v12.6.22.0+):
    - `max_chars` is treated as *chunk size per file* (soft limit).
    - If content exceeds `max_chars`, we split into multiple parts instead of truncating.
    - This prevents the classic "after N rounds code becomes half" failure mode when the
      chat history is truncated and we must recover from cache.
    """
    if not root or not session_id:
        return None

    _, art_dir = _artifact_index_paths(root)
    os.makedirs(art_dir, exist_ok=True)

    content = content or ""
    spec = spec or {}

    # Choose extension / mime for convenience
    ext = ".txt"
    mime = "text/plain"
    content_hint = (content[:2000] or "").lower()
    if "<!doctype html" in content_hint or "<html" in content_hint:
        ext = ".html"
        mime = "text/html"
    elif (
        "function " in content_hint
        or "const " in content_hint
        or "let " in content_hint
    ):
        ext = ".js"
        mime = "text/javascript"
    elif (
        "{" in content_hint
        and "}" in content_hint
        and ("color" in content_hint or "body" in content_hint)
    ):
        ext = ".css"
        mime = "text/css"

    aid = _artifact_id_for_text(content)

    # Chunking helper (prefer splitting on newlines to preserve anchors)
    chunk_chars = int(max_chars or 0)
    if chunk_chars < 0:
        chunk_chars = 0

    def _chunk_by_newline(s: str, n: int) -> List[str]:
        if n <= 0 or len(s) <= n:
            return [s]
        parts: List[str] = []
        i = 0
        total = len(s)
        # Avoid micro-chunks
        min_tail = max(256, int(n * 0.25))
        while i < total:
            j = min(total, i + n)
            if j < total:
                # try cut at newline close to the end
                cut = s.rfind("\n", i, j)
                if cut != -1 and cut >= i + min_tail:
                    j = cut + 1
            parts.append(s[i:j])
            i = j
        return parts

    chunks = _chunk_by_newline(content, chunk_chars)

    now = _now()
    meta_obj: Dict[str, Any] = {
        "id": aid,
        "created_at": now,
        "session_id": session_id,
        "chat_id": chat_id or "",
        "mime": mime,
        "ext": ext,
        "chars": len(content),
        "spec": spec,
        "chunked": bool(len(chunks) > 1),
        "chunk_chars": int(chunk_chars) if chunk_chars > 0 else 0,
    }

    try:
        # Write content files
        if len(chunks) == 1:
            fname = f"{aid}{ext}"
            path = os.path.join(art_dir, fname)
            with open(path, "w", encoding="utf-8") as f:
                f.write(chunks[0])
            meta_obj["content_file"] = fname
            meta_obj["parts"] = []
            meta_obj["parts_count"] = 1
            preview_path = path
        else:
            part_files: List[str] = []
            for idx, chunk in enumerate(chunks):
                pf = f"{aid}.part{idx:03d}{ext}"
                ppath = os.path.join(art_dir, pf)
                with open(ppath, "w", encoding="utf-8") as f:
                    f.write(chunk)
                part_files.append(pf)
            meta_obj["content_file"] = ""
            meta_obj["parts"] = part_files
            meta_obj["parts_count"] = len(part_files)
            preview_path = os.path.join(art_dir, part_files[0])

        # Write meta
        meta_path = os.path.join(art_dir, f"{aid}.meta.json")
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta_obj, f, ensure_ascii=False, indent=2)

        # Update index
        index = _artifact_index_load(root)
        sessions = index.get("sessions", {})
        if not isinstance(sessions, dict):
            sessions = {}
        artifacts = index.get("artifacts", {})
        if not isinstance(artifacts, dict):
            artifacts = {}

        sess = sessions.get(session_id, {})
        if not isinstance(sess, dict):
            sess = {}
        hist = sess.get("history", [])
        if not isinstance(hist, list):
            hist = []
        # Most-recent-first, de-dup
        hist = [aid] + [x for x in hist if str(x).strip() and str(x).strip() != aid]
        if history_max and history_max > 0:
            hist = hist[: int(history_max)]

        sessions[session_id] = {
            "last": aid,
            "history": hist,
            "updated_at": now,
            "chat_id": chat_id or sess.get("chat_id") or "",
        }
        index["sessions"] = sessions

        if chat_id:
            by_chat = index.get("by_chat_id", {})
            if not isinstance(by_chat, dict):
                by_chat = {}
            by_chat[str(chat_id)] = aid
            index["by_chat_id"] = by_chat

        artifacts[aid] = {
            "created_at": now,
            "meta_path": os.path.basename(meta_path),
            "session_id": session_id,
            "chat_id": chat_id or "",
            "chars": int(meta_obj.get("chars") or 0),
            "chunked": bool(meta_obj.get("chunked")),
            "parts_count": int(meta_obj.get("parts_count") or 1),
            "ext": ext,
        }
        index["artifacts"] = artifacts

        _artifact_index_save(root, index)
        return {
            "id": aid,
            "path": preview_path,
            "meta_path": meta_path,
            "chars": int(meta_obj.get("chars") or 0),
        }
    except Exception:
        return None


def _artifact_cache_read(*, root: str, artifact_id: str) -> Tuple[str, Dict[str, Any]]:
    """Read an artifact from disk cache.

    Supports both:
    - single-file artifacts (meta.content_file)
    - multi-part chunked artifacts (meta.parts)
    """
    if not root or not artifact_id:
        return "", {}

    _, art_dir = _artifact_index_paths(root)

    meta_obj: Dict[str, Any] = {}
    meta_path = os.path.join(art_dir, f"{artifact_id}.meta.json")
    try:
        if os.path.exists(meta_path):
            with open(meta_path, "r", encoding="utf-8") as f:
                mo = json.load(f)
            if isinstance(mo, dict):
                meta_obj = mo
    except Exception:
        meta_obj = {}

    def _read_file(path: str) -> str:
        try:
            if path and os.path.exists(path):
                with open(path, "r", encoding="utf-8") as f:
                    return f.read()
        except Exception:
            return ""
        return ""

    # 1) Chunked multi-part artifact
    try:
        parts = meta_obj.get("parts")
        part_files: List[str] = []
        if isinstance(parts, list) and parts:
            for p in parts:
                if isinstance(p, str) and p.strip():
                    part_files.append(p.strip())
                elif isinstance(p, dict):
                    pf = p.get("file") or p.get("path")
                    if isinstance(pf, str) and pf.strip():
                        part_files.append(pf.strip())

        if part_files:
            buf: List[str] = []
            missing: List[str] = []
            for pf in part_files:
                ppath = os.path.join(art_dir, pf)
                s = _read_file(ppath)
                if s == "":
                    missing.append(pf)
                buf.append(s)
            content = "".join(buf)
            if missing:
                meta_obj["missing_parts"] = missing[:10]
            # legacy truncation marker detection
            if "DR_TRUNCATED_BY_CACHE" in content:
                meta_obj["was_truncated_by_cache"] = True
            return content, meta_obj
    except Exception:
        # fall through to single-file mode
        pass

    # 2) single-file artifact
    content_path = ""
    try:
        cf = meta_obj.get("content_file")
        if isinstance(cf, str) and cf.strip():
            content_path = os.path.join(art_dir, cf.strip())
        else:
            for ext in (".html", ".txt", ".js", ".css"):
                p = os.path.join(art_dir, f"{artifact_id}{ext}")
                if os.path.exists(p):
                    content_path = p
                    break

        content = _read_file(content_path) if content_path else ""
        if "DR_TRUNCATED_BY_CACHE" in content:
            meta_obj["was_truncated_by_cache"] = True
        return content, meta_obj
    except Exception:
        return "", meta_obj


def _artifact_cache_load_last(
    *, root: str, session_id: str, chat_id: str = ""
) -> Tuple[str, Dict[str, Any]]:
    """Load last artifact for this session/chat, with anti-truncation fallback."""
    if not root:
        return "", {}

    index = _artifact_index_load(root)

    aid = ""
    try:
        if session_id:
            sess = index.get("sessions", {}).get(session_id)
            if isinstance(sess, dict):
                aid = str(sess.get("last") or "").strip()
    except Exception:
        aid = ""

    if (not aid) and chat_id:
        try:
            aid = str(index.get("by_chat_id", {}).get(str(chat_id)) or "").strip()
        except Exception:
            aid = ""

    if not aid:
        return "", {}

    content, meta = _artifact_cache_read(root=root, artifact_id=aid)

    # If the last artifact was truncated by older versions, try to recover the latest non-truncated one.
    try:
        was_trunc = ("DR_TRUNCATED_BY_CACHE" in (content or "")) or bool(
            meta.get("was_truncated_by_cache")
        )
        if was_trunc and session_id:
            sess = index.get("sessions", {}).get(session_id)
            hist: List[str] = []
            if isinstance(sess, dict) and isinstance(sess.get("history"), list):
                hist = [str(x).strip() for x in sess.get("history") if str(x).strip()]
            # Iterate history (most recent first), find the first better candidate
            best_content = content
            best_meta = meta
            for hid in hist:
                if not hid or hid == aid:
                    continue
                c2, m2 = _artifact_cache_read(root=root, artifact_id=hid)
                if not c2:
                    continue
                if ("DR_TRUNCATED_BY_CACHE" in c2) or bool(
                    m2.get("was_truncated_by_cache")
                ):
                    continue
                if len(c2) >= len(best_content or ""):
                    best_content, best_meta = c2, dict(m2 or {})
                    best_meta["recovered_from_truncated"] = aid
                    best_meta["recovered_artifact_id"] = hid
                    break
            content, meta = best_content, best_meta
    except Exception:
        pass

    return content, meta


# ----------------------------
# Render snippet helpers (Charts)
# ----------------------------
_CHART_BEGIN = "[CHART_MARKDOWN_BEGIN]"
_CHART_END = "[CHART_MARKDOWN_END]"

# ----------------------------
# State cache (persist history store / lightweight context)
# ----------------------------


def _state_cache_dir(root: str) -> str:
    d = os.path.join(root, "state")
    os.makedirs(d, exist_ok=True)
    return d


def _state_cache_path(root: str, session_id: str) -> str:
    safe = re.sub(r"[^a-zA-Z0-9_.-]+", "_", session_id or "session")[:120]
    if not safe:
        safe = "session"
    return os.path.join(_state_cache_dir(root), f"{safe}.json")


def _state_cache_load(root: str, session_id: str) -> Dict[str, Any]:
    try:
        p = _state_cache_path(root, session_id)
        if not os.path.isfile(p):
            return {}
        with open(p, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, dict) else {}
    except Exception:
        return {}


def _state_cache_save(root: str, session_id: str, data: Dict[str, Any]) -> None:
    if not isinstance(data, dict):
        return
    try:
        p = _state_cache_path(root, session_id)
        tmp = p + ".tmp"
        payload = json.dumps(data, ensure_ascii=False, indent=2)
        with open(tmp, "w", encoding="utf-8") as f:
            f.write(payload)
        os.replace(tmp, p)
    except Exception:
        try:
            if os.path.isfile(tmp):
                os.remove(tmp)
        except Exception:
            pass


def _iter_strings(obj: Any):
    if isinstance(obj, str):
        yield obj
        return
    if isinstance(obj, dict):
        for _, v in obj.items():
            yield from _iter_strings(v)
        return
    if isinstance(obj, list):
        for v in obj:
            yield from _iter_strings(v)
        return
    return


def _extract_marked_blocks(text: str, begin: str, end: str) -> List[str]:
    blocks: List[str] = []
    if not isinstance(text, str) or not text:
        return blocks
    start = 0
    while True:
        b = text.find(begin, start)
        if b == -1:
            break
        b2 = b + len(begin)
        e = text.find(end, b2)
        if e == -1:
            break
        blocks.append(text[b2:e])
        start = e + len(end)
    return blocks


def _dedup_str_list(xs: List[str]) -> List[str]:
    out: List[str] = []
    seen = set()
    for x in xs or []:
        if not isinstance(x, str):
            continue
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _extract_chart_markdown_blocks(content: str) -> List[str]:
    if not isinstance(content, str) or not content:
        return []
    # If tool returns JSON string, prefer parsing to avoid escaped newlines.
    obj = _safe_json_loads(content)
    blocks: List[str] = []
    if isinstance(obj, (dict, list)):
        for s in _iter_strings(obj):
            blocks.extend(_extract_marked_blocks(s, _CHART_BEGIN, _CHART_END))
        if blocks:
            return _dedup_str_list(blocks)

    # fallback: raw text
    blocks = _extract_marked_blocks(content, _CHART_BEGIN, _CHART_END)
    return _dedup_str_list(blocks)


def _build_chart_render_system_hint(blocks: List[str]) -> str:
    blocks = _dedup_str_list([b for b in (blocks or []) if isinstance(b, str) and b])
    if not blocks:
        return ""
    msg = (
        "【可用图表（pipe 将自动插入）】\n"
        "你不需要把工具返回的图表 Markdown 片段原样贴到正文。\n"
        "请在正文需要出现图表的位置放置占位符：[[CHART]]、[[CHART_2]]…（每个只出现一次；不要放进代码块）。\n"
        "pipe 会在最终交付阶段自动用真实 Markdown 替换这些占位符。\n"
    )
    for i, b in enumerate(blocks, start=1):
        # Try to extract a readable title from "![alt](...)".
        title = ""
        try:
            m = re.search(r"!\[([^\]]+)\]", b)
            if m:
                title = (m.group(1) or "").strip()
        except Exception:
            title = ""
        tag = "[[CHART]]" if i == 1 else f"[[CHART_{i}]]"
        if title:
            msg += f"- {tag}：{title}\n"
        else:
            msg += f"- {tag}：图表 {i}\n"
    return msg.strip()


def _inject_chart_blocks(answer: str, blocks: List[str]) -> str:
    if not isinstance(answer, str):
        answer = _norm_content(answer)
    blocks = _dedup_str_list(
        [b for b in (blocks or []) if isinstance(b, str) and b.strip()]
    )
    if not blocks:
        return (answer or "").strip()

    out = answer or ""

    # If answer accidentally ends inside an open code fence, close it before inserting charts.
    try:
        if _code_fence_unbalanced(out):
            out = out.rstrip() + "\n```\n"
    except Exception:
        pass

    # Replace placeholders first.
    for i, b in enumerate(blocks, start=1):
        ph = "[[CHART]]" if i == 1 else f"[[CHART_{i}]]"
        if ph in out:
            out = out.replace(ph, b, 1)

    # Append any missing blocks (better to show somewhere than lose the chart).
    for b in blocks:
        bb = b.strip()
        if not bb:
            continue
        if bb in out:
            continue
        out = out.rstrip() + "\n\n" + b
        if not out.endswith("\n"):
            out += "\n"

    return out.strip()


# ----------------------------
# Code stitch helpers
# ----------------------------


def _extract_fenced_code_blocks(text: str) -> List[Dict[str, Any]]:
    """Extract markdown fenced code blocks.

    Returns a list of dicts: {lang, code, start, end} where start/end are char offsets.

    Why this parser:
    - It requires the closing fence to be a dedicated line that contains ONLY backticks
      (plus optional whitespace). This prevents the common failure mode where a *new*
      opening fence like ```html is mistakenly treated as the closing fence for a
      previous empty fence.
    - It still tolerates accidental nesting by capturing the outermost block; later
      normalization will strip any inner fences.
    """
    if not text:
        return []

    # Match an opening fence at line start, capture optional language, then capture
    # everything until a closing fence line that is exactly ``` (optionally spaced).
    pattern = re.compile(
        r"^[ \t]*```([a-zA-Z0-9_+-]*)[ \t]*\n([\s\S]*?)^[ \t]*```[ \t]*$",
        re.MULTILINE,
    )

    blocks: List[Dict[str, Any]] = []
    for m in pattern.finditer(text):
        lang = (m.group(1) or "").strip().lower()
        code = m.group(2)
        blocks.append({"lang": lang, "code": code, "start": m.start(), "end": m.end()})
    return blocks


def _pick_best_code_block(
    blocks: List[Dict[str, Any]], prefer_lang: str = "", file_kind: str = ""
) -> Optional[Dict[str, Any]]:
    """Pick the most likely primary block."""
    if not blocks:
        return None
    prefer = (prefer_lang or "").strip().lower()

    def score(b: Dict[str, Any]) -> int:
        lang = (b.get("lang") or "").lower()
        code = b.get("code") or ""
        s = len(code)
        if prefer and lang == prefer:
            s += 20000
        if file_kind == "html":
            low = code.lower()
            if "<!doctype" in low:
                s += 8000
            if "<html" in low and "</html>" in low:
                s += 8000
            if "<canvas" in low:
                s += 1000
        return s

    best = max(blocks, key=score)
    return best


def _strip_markdown_fences(text: str) -> str:
    if not text:
        return ""
    # Remove stray triple-backticks if any.
    return re.sub(r"```[a-zA-Z0-9_+-]*\s*\n|```", "", text)


def _extract_best_html_document(text: str) -> str:
    """Try to keep the most complete HTML document if multiple were concatenated."""
    if not text:
        return ""
    low = text.lower()
    # Prefer documents that start with <!DOCTYPE html> and end with </html>
    starts = [m.start() for m in re.finditer(r"<!doctype\s+html", low)]
    candidates: List[str] = []
    for st in starts:
        end = low.find("</html>", st)
        if end != -1:
            candidates.append(text[st : end + len("</html>")])
    if candidates:
        # Choose the longest (usually the most complete) document.
        return max(candidates, key=len).strip()

    # Fallback: from first <html to last </html>
    st2 = low.find("<html")
    end2 = low.rfind("</html>")
    if st2 != -1 and end2 != -1 and end2 > st2:
        return text[st2 : end2 + len("</html>")].strip()
    return text.strip()


def _extract_code_candidate(text: str, prefer_lang: str = "") -> Tuple[str, str]:
    """Extract the most likely primary code from a mixed assistant output.

    Returns (lang, code).

    This is intentionally defensive: model outputs sometimes contain *stray* fences such as:

        <!-- DR_EOF -->
        ```

        ```HTML
        ...
        ```

    A naive regex will capture only the empty block and lose the real code. We therefore:
    - parse fenced blocks with a stricter close-fence rule (see _extract_fenced_code_blocks)
    - if the best fenced block looks suspiciously small, fall back to a fence-stripped
      extraction and try to recover a complete HTML document.
    """
    prefer = (prefer_lang or "").strip().lower()
    raw = text or ""

    blocks = _extract_fenced_code_blocks(raw)
    if blocks:
        file_kind = "html" if prefer == "html" else ""
        best = _pick_best_code_block(blocks, prefer_lang=prefer, file_kind=file_kind)
        if best:
            lang = (best.get("lang") or prefer or "text").strip().lower() or "text"
            code = str(best.get("code") or "")

            # If the extracted block is empty/tiny but the raw text clearly contains a real
            # deliverable, fall back to a fence-stripped recovery.
            if len(code.strip()) < 200:
                fb = _strip_markdown_fences(raw)
                fb_low = fb.lower()
                if prefer == "html" or "<!doctype" in fb_low or "<html" in fb_low:
                    recovered = _extract_best_html_document(fb)
                    if recovered and len(recovered) > len(code):
                        return "html", recovered

            return lang, code

    # No fenced blocks: treat the whole text as code.
    lang = prefer or "text"
    code = _strip_markdown_fences(raw)

    # If it's HTML-like, try to keep only the most complete document.
    try:
        low = code.lower()
        if prefer == "html" or "<!doctype" in low or "<html" in low:
            lang = "html"
            code = _extract_best_html_document(code)
    except Exception:
        pass

    return lang, code


def _normalize_code_candidate(code: str, lang: str = "") -> str:
    """Normalize extracted code so it's more likely to be runnable."""
    s = _norm_content(code)
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    s = _strip_markdown_fences(s)
    l = (lang or "").strip().lower()
    if l == "html":
        s = _extract_best_html_document(s)
        # Trim junk after the last </html> if present.
        low = s.lower()
        last_html = low.rfind("</html>")
        if last_html != -1:
            s = s[: last_html + len("</html>")]
    return s.strip() + "\n"


def _build_code_repair_excerpt(
    code: str,
    max_chars: int = 45000,
    *,
    lang: str = "",
    eof_marker: str = "",
) -> str:
    """Build a bounded excerpt for the patch-review model.

    We try to keep both the head and the tail (where stitching mistakes are common).
    If the content is still too long, we also keep a small middle segment around EOF marker
    (if present) or around the middle.

    NOTE: The excerpt may include boundary comments like "[DR_EXCERPT_CUT]" which do NOT
    exist in the original code; patch anchors must NOT use them.
    """
    s = code or ""
    try:
        max_chars_i = int(max_chars)
    except Exception:
        max_chars_i = 45000
    max_chars_i = max(4000, max_chars_i)
    if len(s) <= max_chars_i:
        return s

    head_n = int(max_chars_i * 0.45)
    tail_n = int(max_chars_i * 0.45)
    mid_n = max_chars_i - head_n - tail_n
    head = s[:head_n]
    tail = s[-tail_n:]

    center = len(s) // 2
    try:
        if eof_marker and eof_marker in s:
            center = s.rfind(eof_marker)
        elif (lang or "").lower() == "html" and "</html>" in s.lower():
            center = s.lower().rfind("</html>")
    except Exception:
        center = len(s) // 2
    start = max(0, center - mid_n // 2)
    mid = s[start : start + mid_n]

    # Use a language-appropriate boundary marker.
    boundary = "\n\n/* [DR_EXCERPT_CUT] */\n\n"
    if (lang or "").lower() in {"html", "xml"}:
        boundary = "\n\n<!-- [DR_EXCERPT_CUT] -->\n\n"

    return head + boundary + mid + boundary + tail


def _format_eof_marker_line(eof_marker: str, lang: str = "") -> str:
    """Render a language-appropriate comment line that still contains the raw marker text."""
    m = (eof_marker or "").strip()
    if not m:
        return ""
    l = (lang or "").strip().lower()

    # If caller already provided a wrapped marker, keep it.
    if any(tok in m for tok in ("<!--", "-->", "//", "#", "/*", "*/")):
        return m

    if l in {"html", "xml"}:
        return f"<!-- {m} -->"

    if l in {
        "js",
        "javascript",
        "ts",
        "typescript",
        "java",
        "c",
        "cpp",
        "c++",
        "cs",
        "c#",
        "go",
        "rust",
        "php",
        "kotlin",
        "swift",
    }:
        return f"// {m}"

    if l in {"python", "py", "ruby", "rb", "bash", "sh", "shell", "yaml", "yml"}:
        return f"# {m}"

    if l in {"lua"}:
        return f"-- {m}"

    return m


def _is_markdown_fence_line(line: str) -> bool:
    return (line or "").strip().startswith("```")


def _marker_literal_is_terminal(text: str, marker_line: str) -> bool:
    """Return True if `marker_line` appears as a standalone line and only whitespace / markdown fences follow."""
    s = text or ""
    ml = (marker_line or "").strip()
    if not ml:
        return True
    lines = s.splitlines()
    last_idx = None
    for i in range(len(lines) - 1, -1, -1):
        if lines[i].strip() == ml:
            last_idx = i
            break
    if last_idx is None:
        return False
    for ln in lines[last_idx + 1 :]:
        if not (ln or "").strip():
            continue
        if _is_markdown_fence_line(ln):
            continue
        return False
    return True


def _line_matches_eof_marker(line: str, eof_marker: str, marker_line: str) -> bool:
    """Heuristic match for an EOF marker *line*.

    We avoid substring matching on the bare token (e.g., 'DR_EOF') to prevent accidental early matches.
    """
    l = (line or "").strip()
    if not l:
        return False

    ml = (marker_line or "").strip()
    token = (eof_marker or "").strip()

    if ml and l == ml:
        return True

    if not token:
        return False

    # Infer comment style from canonical marker line
    if ml.startswith("<!--") or "<!--" in ml:
        return l.startswith("<!--") and l.endswith("-->") and token in l
    if ml.startswith("//"):
        return l.startswith("//") and token in l
    if ml.startswith("#"):
        return l.startswith("#") and token in l
    if ml.startswith("/*"):
        return l.startswith("/*") and l.endswith("*/") and token in l
    if ml.startswith("--"):
        return l.startswith("--") and token in l

    # Fallback: standalone token line
    return l == token


def _ensure_eof_marker(
    code: str, eof_marker: str, *, eof_line: str | None = None
) -> str:
    """Ensure a *single, terminal* EOF marker line.

    Behavior:
    - Prefer matching a full marker line (e.g., '<!-- DR_EOF -->') rather than the bare token.
    - If an existing marker line is terminal, trim any trailing markdown fence noise and normalize the line.
    - If a marker line exists but is *not* terminal, treat it as accidental: remove marker lines and append a canonical one at the end.
    - If no marker line exists, append a canonical one at the end.

    This protects against cases where the model mentions 'DR_EOF' early (e.g., in prose), which would otherwise truncate output.
    """
    s = (code or "").rstrip("\n")

    marker_line = (eof_line or eof_marker or "").strip()
    token = (eof_marker or "").strip()
    if not marker_line:
        return s + ("\n" if not s.endswith("\n") else "")

    # If no token provided, we can only append the marker line.
    if not token:
        if s.rstrip().endswith(marker_line):
            return s.rstrip("\n") + "\n"
        return (s.rstrip("\n") + "\n" if s else "") + marker_line + "\n"

    lines = s.splitlines()

    # Find last marker line (robust against whitespace variations).
    last_idx = None
    for i in range(len(lines) - 1, -1, -1):
        if _line_matches_eof_marker(lines[i], token, marker_line):
            last_idx = i
            break

    if last_idx is not None:
        # Determine whether everything after is just whitespace / markdown fences.
        terminal = True
        for ln in lines[last_idx + 1 :]:
            if not (ln or "").strip():
                continue
            if _is_markdown_fence_line(ln):
                continue
            terminal = False
            break

        if terminal:
            kept = lines[: last_idx + 1]
            kept[-1] = marker_line
            return "\n".join(kept).rstrip("\n") + "\n"

        # Non-terminal marker line(s): remove them and append a canonical marker line at the end.
        filtered = [
            ln for ln in lines if not _line_matches_eof_marker(ln, token, marker_line)
        ]
        filtered_str = "\n".join(filtered).rstrip("\n")
        if filtered_str:
            return filtered_str + "\n" + marker_line + "\n"
        return marker_line + "\n"

    # No marker line found: append canonical line
    if s:
        return s.rstrip("\n") + "\n" + marker_line + "\n"
    return marker_line + "\n"


def _code_integrity_issues(
    code: str, lang: str = "", eof_marker: str = ""
) -> List[str]:
    issues: List[str] = []
    s = code or ""
    low = s.lower()

    if "```" in s:
        issues.append("contains_markdown_fences")

    # Manual continuation artifacts
    if (
        re.search(r"第\s*\d+\s*/\s*\d+\s*段", s)
        or ("输出下一段" in s)
        or ("回复“继续”" in s)
        or ('回复"继续"' in s)
    ):
        issues.append("contains_manual_continue_text")

    if eof_marker:
        marker_line = _format_eof_marker_line(eof_marker, lang).strip()
        lines = s.splitlines()
        marker_idxs = [
            i
            for i, ln in enumerate(lines)
            if _line_matches_eof_marker(ln, eof_marker, marker_line)
        ]
        if not marker_idxs:
            issues.append("missing_eof_marker")
        elif len(marker_idxs) > 1:
            issues.append("duplicate_eof_marker")
        if (
            marker_line
            and marker_line in s
            and not _marker_literal_is_terminal(s, marker_line)
        ):
            issues.append("eof_marker_not_at_end")

    if (lang or "").lower() == "html":
        doctype_n = len(re.findall(r"<!doctype", low))
        html_open = len(re.findall(r"<html\b", low))
        html_close = low.count("</html>")
        body_open = len(re.findall(r"<body\b", low))
        body_close = low.count("</body>")
        script_open = len(re.findall(r"<script\b", low))
        script_close = low.count("</script>")

        if doctype_n == 0 and html_open == 0:
            issues.append("missing_html_root")
        if html_open == 0 or html_close == 0:
            issues.append("html_tag_unbalanced")
        if doctype_n > 1 or html_open > 1 or html_close > 1:
            issues.append("duplicate_html_document")
        if body_open != body_close:
            issues.append("body_tag_unbalanced")
        if script_open != script_close:
            issues.append("script_tag_unbalanced")

    return issues


def _text_integrity_issues(text: str) -> List[str]:
    """Heuristic integrity checks for long-form deliverables (reports/analysis).

    This is intentionally lightweight; the heavy lifting is done by the final audit LLM pass.
    """
    issues: List[str] = []
    s = _norm_content(text)
    low = s.lower()

    try:
        if _code_fence_unbalanced(s):
            issues.append("markdown_fence_unbalanced")
    except Exception:
        pass

    # Chunking / manual-continue artifacts that sometimes leak into the content.
    if "回复“继续" in s or '回复"继续"' in s or "下一段" in s:
        issues.append("contains_manual_continue_text")
    if re.search(r"第\s*\d+\s*/\s*\d+\s*段", s):
        issues.append("contains_chunk_footer")

    # Internal tags should never appear in final deliverable.
    if "<dr_action>" in low or "</dr_action>" in low:
        issues.append("contains_dr_action")

    # Chart placeholders should have been injected already (unless user explicitly wants them).
    if "[[chart" in low:
        issues.append("unresolved_chart_placeholders")

    # Obvious truncation markers.
    if s.strip().endswith("…") or s.strip().endswith("..."):
        issues.append("may_be_truncated")

    return sorted(set(issues))


def _build_text_repair_excerpt(text: str, max_chars: int = 60000) -> str:
    """Build a bounded excerpt for final-audit of long reports.

    NOTE: The boundary markers do NOT exist in the original content.
    """
    s = text or ""
    try:
        max_chars_i = int(max_chars)
    except Exception:
        max_chars_i = 60000
    max_chars_i = max(4000, max_chars_i)
    if len(s) <= max_chars_i:
        return s

    head_n = int(max_chars_i * 0.5)
    tail_n = max_chars_i - head_n
    head = s[:head_n]
    tail = s[-tail_n:]
    boundary = "\n\n[DR_EXCERPT_CUT]\n\n"
    return head + boundary + tail


def _nth_index(haystack: str, needle: str, n: int) -> int:
    if n <= 0:
        return haystack.find(needle)
    start = 0
    idx = -1
    for _ in range(n):
        idx = haystack.find(needle, start)
        if idx == -1:
            return -1
        start = idx + len(needle)
    return idx


# ----------------------------
# Patch application helpers (robust)
# ----------------------------


def _ws_regex_from_literal(lit: str) -> str:
    """Build a whitespace-tolerant regex from a literal string.

    Any run of whitespace in the literal becomes r"\s+".
    Other characters are escaped.
    """
    if not isinstance(lit, str) or not lit:
        return ""
    out = []
    i = 0
    n = len(lit)
    while i < n:
        ch = lit[i]
        if ch.isspace():
            # collapse consecutive whitespace
            while i < n and lit[i].isspace():
                i += 1
            out.append(r"\s+")
            continue
        out.append(re.escape(ch))
        i += 1
    return "".join(out)


def _regex_find_nth(text: str, pattern: str, occurrence: int, flags: int = 0):
    """Return (start,end) span for nth regex match (1-based)."""
    try:
        occ = int(occurrence)
    except Exception:
        occ = 1
    if occ < 1:
        occ = 1
    try:
        rx = re.compile(pattern, flags)
    except Exception:
        return None
    k = 0
    for m in rx.finditer(text):
        k += 1
        if k == occ:
            return (m.start(), m.end())
    return None


def _fuzzy_find_best_span(
    text: str,
    pattern: str,
    *,
    occurrence: int = 1,
    min_ratio: float = 0.985,
    max_candidates: int = 120,
    short_min_len: int = 24,
    ambiguity_margin: float = 0.01,
):
    """Best-effort fuzzy substring search.

    Returns a dict: {start,end,ratio,matched} or None.

    Design goal: handle small drift (1-3 chars) between patch anchors and current text,
    while avoiding dangerous replacements.
    """
    if not isinstance(text, str) or not isinstance(pattern, str):
        return None
    pat = pattern
    if not pat:
        return None
    if len(pat) < max(1, int(short_min_len)):
        return None

    # seeds: pick several non-whitespace chunks (more stable than including spaces/newlines)
    seeds = []
    try:
        chunks = [m.group(0) for m in re.finditer(r"\S{20,80}", pat)]
        if chunks:
            seeds.append(chunks[0])
            seeds.append(chunks[len(chunks) // 2])
            seeds.append(chunks[-1])
    except Exception:
        chunks = []
    if not seeds:
        # fallback seeds
        seed_len = min(60, max(20, len(pat) // 6))
        seeds = [pat[:seed_len], pat[-seed_len:]]

    # collect candidate starts using seeds
    cand_starts = []
    seen = set()
    for seed in seeds:
        if not seed:
            continue
        off = pat.find(seed)
        if off < 0:
            off = 0
        pos = 0
        hits = 0
        while hits < max_candidates:
            idx = text.find(seed, pos)
            if idx == -1:
                break
            # estimate start
            st = idx - off
            if st < 0:
                st = 0
            # small jitter around
            for d in (-8, -4, 0, 4, 8):
                st2 = st + d
                if st2 < 0:
                    continue
                if st2 in seen:
                    continue
                seen.add(st2)
                cand_starts.append(st2)
                if len(cand_starts) >= max_candidates:
                    break
            if len(cand_starts) >= max_candidates:
                break
            hits += 1
            pos = idx + 1
        if len(cand_starts) >= max_candidates:
            break

    if not cand_starts:
        return None

    import difflib

    pat_len = len(pat)
    scored = []
    for st in cand_starts:
        if st < 0:
            continue
        # try a few length variants to tolerate small insert/delete drift
        best_local = None
        for dl in (0, -2, 2, -4, 4):
            ln = pat_len + dl
            if ln <= 0:
                continue
            ed = st + ln
            if ed > len(text):
                continue
            seg = text[st:ed]
            try:
                r = difflib.SequenceMatcher(a=pat, b=seg).ratio()
            except Exception:
                continue
            if (best_local is None) or (r > best_local[0]):
                best_local = (r, seg, ed)
        if not best_local:
            continue
        r, seg, ed = best_local
        if r >= float(min_ratio) - 1e-9:
            scored.append((r, st, ed, seg))

    if not scored:
        return None

    scored.sort(key=lambda x: (-x[0], x[1]))

    # If occurrence >1, pick by start-order among high-score candidates.
    try:
        occ = int(occurrence)
    except Exception:
        occ = 1
    if occ < 1:
        occ = 1

    if occ == 1:
        best = scored[0]
        # ambiguity guard: if second best is too close, reject
        if len(scored) >= 2:
            if (best[0] - scored[1][0]) < float(ambiguity_margin):
                return None
        r, st, ed, seg = best
        return {"start": st, "end": ed, "ratio": float(r), "matched": seg}

    # occurrence >=2
    # take candidates within tiny epsilon of best ratio to preserve semantics
    top_r = scored[0][0]
    near = [x for x in scored if (top_r - x[0]) < 0.02]
    near.sort(key=lambda x: x[1])
    if len(near) < occ:
        return None
    r, st, ed, seg = near[occ - 1]
    return {"start": st, "end": ed, "ratio": float(r), "matched": seg}


def _short_preview(s: str, max_len: int = 180) -> str:
    if not isinstance(s, str):
        s = str(s)
    t = s.replace("\r\n", "\n").replace("\r", "\n")
    t = t.strip("\n")
    if len(t) <= max_len:
        return t
    return t[: max_len // 2] + " … " + t[-max_len // 2 :]


def _apply_code_patches(
    code: str, patches: List[Dict[str, Any]], policy: Optional[Dict[str, Any]] = None
) -> Tuple[str, List[str], List[str]]:
    """Apply patches to code/text.

    Supports robust matching:
    - exact literal match
    - whitespace-tolerant regex fallback
    - fuzzy fallback (high threshold) for small drift (1-3 chars)

    Returns: (new_code, applied_logs, skipped_logs)
    """
    out = code
    applied: List[str] = []
    skipped: List[str] = []

    pol = policy if isinstance(policy, dict) else {}
    enable_ws_regex = bool(pol.get("enable_ws_regex", True))
    enable_fuzzy = bool(pol.get("enable_fuzzy", True))
    fuzzy_min_ratio = float(pol.get("fuzzy_min_ratio", 0.985))
    fuzzy_max_candidates = int(pol.get("fuzzy_max_candidates", 120))
    fuzzy_short_min_len = int(pol.get("fuzzy_short_min_len", 24))
    fuzzy_ambiguity_margin = float(pol.get("fuzzy_ambiguity_margin", 0.01))

    if fuzzy_min_ratio < 0.90:
        fuzzy_min_ratio = 0.90
    if fuzzy_min_ratio > 0.999:
        fuzzy_min_ratio = 0.999
    if fuzzy_max_candidates < 20:
        fuzzy_max_candidates = 20
    if fuzzy_max_candidates > 600:
        fuzzy_max_candidates = 600
    if fuzzy_short_min_len < 8:
        fuzzy_short_min_len = 8

    if not isinstance(patches, list):
        return out, applied, skipped

    def nth_index(hay: str, needle: str, occ: int) -> int:
        if occ <= 1:
            return hay.find(needle)
        start = 0
        k = 0
        while True:
            idx = hay.find(needle, start)
            if idx == -1:
                return -1
            k += 1
            if k == occ:
                return idx
            start = idx + max(1, len(needle))

    for i, p in enumerate(patches, 1):
        if not isinstance(p, dict):
            skipped.append(f"patch#{i}: invalid")
            continue
        op = str(p.get("op") or "").strip()
        if not op:
            skipped.append(f"patch#{i}: missing op")
            continue

        # --- helpers for matching ---
        def _find_span_literal_or_regex_or_fuzzy(literal: str, occ: int, label: str):
            """Find span in current out for a literal string.

            Returns (start,end, how) where how is 'exact'|'ws_regex'|'fuzzy(r=...)', or None.
            """
            if not isinstance(literal, str) or not literal:
                return None
            # exact
            idx = nth_index(out, literal, occ)
            if idx != -1:
                return (idx, idx + len(literal), "exact")

            # whitespace-tolerant regex
            if enable_ws_regex:
                rx_pat = _ws_regex_from_literal(literal)
                if rx_pat:
                    span = _regex_find_nth(out, rx_pat, occ, flags=re.DOTALL)
                    if span:
                        return (span[0], span[1], "ws_regex")

            # fuzzy (only for single-occurrence or when occ >1 but still possible)
            if enable_fuzzy:
                best = _fuzzy_find_best_span(
                    out,
                    literal,
                    occurrence=occ,
                    min_ratio=fuzzy_min_ratio,
                    max_candidates=fuzzy_max_candidates,
                    short_min_len=fuzzy_short_min_len,
                    ambiguity_margin=fuzzy_ambiguity_margin,
                )
                if best:
                    return (
                        int(best["start"]),
                        int(best["end"]),
                        f"fuzzy(r={best['ratio']:.4f})",
                    )

            return None

        try:
            if op == "replace":
                find = p.get("find")
                repl = p.get("replace")
                occurrence = p.get("occurrence", 1)

                if not isinstance(find, str) or not isinstance(repl, str):
                    skipped.append(f"patch#{i}: replace invalid")
                    continue

                if occurrence == "all":
                    if find in out:
                        out = out.replace(find, repl)
                        applied.append(f"patch#{i}: replace all (exact)")
                    elif enable_ws_regex:
                        rx_pat = _ws_regex_from_literal(find)
                        if rx_pat:
                            try:
                                rx = re.compile(rx_pat, re.DOTALL)
                                new_out, n = rx.subn(repl, out)
                                if n > 0:
                                    out = new_out
                                    applied.append(
                                        f"patch#{i}: replace all (ws_regex) x{n}"
                                    )
                                else:
                                    skipped.append(f"patch#{i}: replace all not found")
                            except Exception:
                                skipped.append(f"patch#{i}: replace all regex failed")
                        else:
                            skipped.append(f"patch#{i}: replace all not found")
                    else:
                        skipped.append(f"patch#{i}: replace all not found")
                    continue

                # integer occurrence
                try:
                    occ = int(occurrence)
                except Exception:
                    occ = 1
                if occ < 1:
                    occ = 1

                span = _find_span_literal_or_regex_or_fuzzy(find, occ, "find")
                if not span:
                    skipped.append(f"patch#{i}: replace occurrence {occ} not found")
                    continue
                s, e, how = span
                out = out[:s] + repl + out[e:]
                applied.append(f"patch#{i}: replace occurrence {occ} ({how})")

            elif op == "delete_between":
                start_s = p.get("start")
                end_s = p.get("end")
                include_end = bool(p.get("include_end", True))
                occurrence = p.get("occurrence", 1)

                if not isinstance(start_s, str) or not isinstance(end_s, str):
                    skipped.append(f"patch#{i}: delete_between invalid")
                    continue

                try:
                    occ = int(occurrence)
                except Exception:
                    occ = 1
                if occ < 1:
                    occ = 1

                st_span = _find_span_literal_or_regex_or_fuzzy(start_s, occ, "start")
                if not st_span:
                    skipped.append(
                        f"patch#{i}: delete_between start occurrence {occ} not found"
                    )
                    continue
                st0, st1, st_how = st_span

                # search end after start
                sub = out[st1:]

                # try exact end first
                end_idx = sub.find(end_s)
                end_how = "exact"
                end_len = len(end_s)
                end_off = 0

                if end_idx == -1 and enable_ws_regex:
                    rx_pat = _ws_regex_from_literal(end_s)
                    if rx_pat:
                        span = _regex_find_nth(sub, rx_pat, 1, flags=re.DOTALL)
                        if span:
                            end_idx = span[0]
                            end_len = span[1] - span[0]
                            end_how = "ws_regex"

                if end_idx == -1 and enable_fuzzy:
                    best = _fuzzy_find_best_span(
                        sub,
                        end_s,
                        occurrence=1,
                        min_ratio=fuzzy_min_ratio,
                        max_candidates=fuzzy_max_candidates,
                        short_min_len=fuzzy_short_min_len,
                        ambiguity_margin=fuzzy_ambiguity_margin,
                    )
                    if best:
                        end_idx = int(best["start"])
                        end_len = int(best["end"]) - int(best["start"])
                        end_how = f"fuzzy(r={best['ratio']:.4f})"

                if end_idx == -1:
                    skipped.append(f"patch#{i}: delete_between end not found")
                    continue

                cut_start = st0
                cut_end = st1 + end_idx + (end_len if include_end else 0)
                if cut_end < cut_start:
                    skipped.append(f"patch#{i}: delete_between invalid span")
                    continue

                out = out[:cut_start] + out[cut_end:]
                applied.append(f"patch#{i}: delete_between ({st_how} -> {end_how})")

            elif op in ("insert_after", "insert_before"):
                anchor = p.get("anchor")
                ins = p.get("insert")
                occurrence = p.get("occurrence", 1)

                if not isinstance(anchor, str) or not isinstance(ins, str):
                    skipped.append(f"patch#{i}: {op} invalid")
                    continue

                try:
                    occ = int(occurrence)
                except Exception:
                    occ = 1
                if occ < 1:
                    occ = 1

                span = _find_span_literal_or_regex_or_fuzzy(anchor, occ, "anchor")
                if not span:
                    skipped.append(f"patch#{i}: {op} anchor occurrence {occ} not found")
                    continue
                s, e, how = span

                if op == "insert_after":
                    pos = e
                else:
                    pos = s

                out = out[:pos] + ins + out[pos:]
                applied.append(f"patch#{i}: {op} occurrence {occ} ({how})")

            elif op == "append":
                ins = p.get("insert")
                if not isinstance(ins, str):
                    skipped.append(f"patch#{i}: append invalid")
                    continue
                out = out + ins
                applied.append(f"patch#{i}: append")

            elif op == "prepend":
                ins = p.get("insert")
                if not isinstance(ins, str):
                    skipped.append(f"patch#{i}: prepend invalid")
                    continue
                out = ins + out
                applied.append(f"patch#{i}: prepend")

            else:
                skipped.append(f"patch#{i}: unknown op {op}")

        except Exception as e:
            skipped.append(f"patch#{i}: exception {type(e).__name__}")

    return out, applied, skipped


def _build_patch_apply_feedback(
    current_text: str,
    patches,
    applied_logs,
    skipped_logs,
    notes: str = "",
    max_items: int = 8,
    policy: Optional[Dict[str, Any]] = None,
) -> dict:
    """Build a compact feedback object to help the model retry patch generation.

    Key enhancement: when a literal anchor is not found, we also try to provide
    a *closest* snippet from the current text (fuzzy), so the model can pick an
    exact anchor next attempt.
    """

    def _count_occurrences(hay: str, needle: str) -> int:
        if not needle:
            return 0
        c = 0
        start = 0
        while True:
            idx = hay.find(needle, start)
            if idx == -1:
                return c
            c += 1
            start = idx + max(1, len(needle))

    def _best_anchor_hint(hay: str, needle: str) -> Optional[Dict[str, Any]]:
        # Use a lower threshold for *suggestions* (not for automatic apply)
        pol = policy if isinstance(policy, dict) else {}
        enable_fuzzy = bool(pol.get("enable_fuzzy", True))
        if not enable_fuzzy:
            return None
        try:
            best = _fuzzy_find_best_span(
                hay,
                needle,
                occurrence=1,
                min_ratio=float(pol.get("fuzzy_hint_min_ratio", 0.90)),
                max_candidates=int(pol.get("fuzzy_max_candidates", 120)),
                short_min_len=int(pol.get("fuzzy_short_min_len", 24)),
                ambiguity_margin=float(pol.get("fuzzy_ambiguity_margin", 0.01)),
            )
        except Exception:
            best = None
        if not best:
            return None
        st = int(best["start"])
        ed = int(best["end"])
        # Provide an exact snippet from hay that the model can copy as anchor.
        snip = hay[st:ed]
        # Keep a slice in a reasonable anchor length range.
        if len(snip) > 220:
            snip = snip[:220]
        return {
            "ratio": float(best.get("ratio") or 0.0),
            "suggested_anchor": snip,
            "preview": _short_preview(snip, 180),
        }

    items = []
    if isinstance(patches, list):
        for p in patches[: max(1, int(max_items))]:
            if not isinstance(p, dict):
                continue
            op = str(p.get("op") or "").strip()
            entry = {"op": op}
            key = None
            if op == "replace":
                key = "find"
            elif op in ("insert_after", "insert_before"):
                key = "anchor"
            elif op == "delete_between":
                key = "start"
            if key and isinstance(p.get(key), str):
                needle = p.get(key)
                entry["needle_key"] = key
                entry["needle_preview"] = _short_preview(needle, 160)
                entry["occurrences_in_current"] = _count_occurrences(
                    current_text, needle
                )
                if entry["occurrences_in_current"] == 0:
                    hint = _best_anchor_hint(current_text, needle)
                    if hint:
                        entry["closest_hint"] = hint
            items.append(entry)

    fb = {
        "notes": (notes or "")[:600],
        "applied_logs": list(applied_logs or [])[-12:],
        "skipped_logs": list(skipped_logs or [])[-12:],
        "patch_items": items,
    }
    return fb


# ----------------------------
# OpenAI response helpers
# ----------------------------
def _normalize_gcc_response(res: Any) -> Dict[str, Any]:
    """
    generate_chat_completion sometimes returns a dict, sometimes a Starlette JSONResponse-like object
    (version dependent). We normalize to a dict.
    """
    if isinstance(res, dict):
        return res

    # Starlette JSONResponse-like
    body = None
    try:
        body = getattr(res, "body", None)
    except Exception:
        body = None

    if body is not None:
        txt = _norm_content(body)
        obj = _safe_json_loads(txt)
        if isinstance(obj, dict):
            return obj
        # Sometimes body is bytes but not valid JSON -> wrap as assistant text
        return {"choices": [{"message": {"role": "assistant", "content": txt}}]}

    # str/bytes fallback
    txt = _norm_content(res)
    obj = _safe_json_loads(txt)
    if isinstance(obj, dict):
        return obj
    return {"choices": [{"message": {"role": "assistant", "content": txt}}]}


def _extract_choice_message(res: Dict[str, Any]) -> Dict[str, Any]:
    choices = res.get("choices") or []
    if not isinstance(choices, list) or not choices:
        return {}
    msg = (choices[0] or {}).get("message") or {}
    return msg if isinstance(msg, dict) else {}


def _extract_text(res: Dict[str, Any]) -> str:
    msg = _extract_choice_message(res)
    return _norm_content(msg.get("content")).strip()


def _extract_text_from_assistant_response(res: Any) -> str:
    """
    Backward-compatible helper used across follow-up/edit-mode code paths.
    Accepts dict/JSONResponse-like/str and returns the assistant text content.
    """
    try:
        obj = _normalize_gcc_response(res)
        return _extract_text(obj)
    except Exception:
        try:
            return _norm_content(res).strip()
        except Exception:
            return str(res or "").strip()


def _extract_finish_reason(res: Dict[str, Any]) -> str:
    choices = res.get("choices") or []
    if not isinstance(choices, list) or not choices:
        return ""
    fr = (choices[0] or {}).get("finish_reason") or ""
    return str(fr)


def _extract_tool_calls(res: Dict[str, Any]) -> List[dict]:
    msg = _extract_choice_message(res)
    tc = msg.get("tool_calls")
    if isinstance(tc, list):
        return [x for x in tc if isinstance(x, dict)]

    # legacy function_call
    fc = msg.get("function_call")
    if isinstance(fc, dict) and fc.get("name"):
        return [
            {
                "id": "fc_1",
                "type": "function",
                "function": {
                    "name": str(fc.get("name")),
                    "arguments": fc.get("arguments", "{}"),
                },
            }
        ]
    return []


def _extract_action_and_clean_text(text: str) -> Tuple[str, str]:
    if not text:
        return "", ""
    m = re.search(r"<dr_action>\s*([\s\S]*?)\s*</dr_action>", text, re.IGNORECASE)
    if m:
        action = (m.group(1) or "").strip()
        cleaned = (text[: m.start()] + text[m.end() :]).strip()
        return action, cleaned
    # allow first line prefix
    lines = text.splitlines()
    if lines:
        head = lines[0].strip()
        if head.startswith("【本轮计划】"):
            action = head.replace("【本轮计划】", "", 1).strip()
            cleaned = "\n".join(lines[1:]).strip()
            return action, cleaned
    return "", text.strip()


def _code_fence_unbalanced(text: str) -> bool:
    if not text:
        return False
    return (text.count("```") % 2) == 1


# ----------------------------
# Router tag + metadata parse
# ----------------------------
def _parse_router_tag(messages: List[dict]) -> Dict[str, Any]:
    """
    Find <dr_router>{json}</dr_router> in any system message.
    """
    for m in messages or []:
        if not isinstance(m, dict) or m.get("role") != "system":
            continue
        c = m.get("content")
        if not isinstance(c, str):
            continue
        if "<dr_router>" in c and "</dr_router>" in c:
            mm = re.search(r"<dr_router>\s*([\s\S]*?)\s*</dr_router>", c)
            if mm:
                obj = _safe_json_loads(mm.group(1))
                if isinstance(obj, dict):
                    return obj
    return {}


def _safe_get_md(body: dict, __metadata__: Optional[dict]) -> Dict[str, Any]:
    md: Dict[str, Any] = {}
    if isinstance(__metadata__, dict):
        md.update(deepcopy(__metadata__))
    bmd = body.get("metadata")
    if isinstance(bmd, dict):
        md.update(deepcopy(bmd))
    # sometimes metadata is a json string
    if isinstance(md.get("metadata"), str) and md.get("metadata").strip().startswith(
        "{"
    ):
        try:
            md2 = json.loads(md.get("metadata"))
            if isinstance(md2, dict):
                md.update(md2)
        except Exception:
            pass
    return md


def _get_dr_dict(metadata: Dict[str, Any]) -> Dict[str, Any]:
    dr = metadata.get("deep_research")
    return dr if isinstance(dr, dict) else {}


def _resolve_base_model(
    metadata: Dict[str, Any], router: Dict[str, Any]
) -> Tuple[str, str]:
    """
    Returns (base_model, source).
    """
    dr = _get_dr_dict(metadata)
    base = str(dr.get("base_model") or "").strip()
    if base:
        return base, "metadata.deep_research.base_model"

    base = str(metadata.get("dr_original_model") or "").strip()
    if base:
        return base, "metadata.dr_original_model"

    base = str(router.get("base_model") or "").strip()
    if base:
        return base, "router_tag.base_model"

    # fallback config
    cfg = router.get("config") if isinstance(router.get("config"), dict) else {}
    base = str(cfg.get("fallback_base_model") or "").strip()
    if base:
        return base, "router_tag.config.fallback_base_model"

    cfg2 = dr.get("config") if isinstance(dr.get("config"), dict) else {}
    base = str(cfg2.get("fallback_base_model") or "").strip()
    if base:
        return base, "metadata.deep_research.config.fallback_base_model"

    return "", "missing"


def _resolve_session_id(
    metadata: Dict[str, Any],
    router: Dict[str, Any],
    *,
    body: Optional[Dict[str, Any]] = None,
    messages: Optional[List[dict]] = None,
    user: Optional[Dict[str, Any]] = None,
    request=None,
) -> str:
    """Resolve a stable session id.

    Priority order:
    1) metadata.deep_research.session_id (explicit)
    2) <dr_router> tag session_id
    3) chat/conversation ids from metadata / body / router / request headers
    4) deterministic fallback from (user_id + first user message id/content)

    This fixes "no context" issues when the upstream system does not provide
    deep_research.session_id or a stable chat_id.
    """

    def pick(v: Any) -> str:
        if v is None:
            return ""
        if isinstance(v, str) and v.strip():
            return v.strip()
        if isinstance(v, (int, float)):
            try:
                iv = int(v)
                if iv != 0:
                    return str(iv)
            except Exception:
                pass
        return ""

    dr = _get_dr_dict(metadata)
    sid = pick(dr.get("session_id"))
    if sid:
        return sid

    sid = pick(router.get("session_id"))
    if sid:
        return sid

    # 3) stable chat ids (prefer explicit chat_id / conversation_id; DO NOT default to generic metadata['id'] which may be a message id)
    for k in ("chat_id", "conversation_id", "thread_id"):
        v = pick(metadata.get(k))
        if v:
            return f"chat:{v}"

    if isinstance(body, dict):
        for k in ("chat_id", "conversation_id", "thread_id"):
            v = pick(body.get(k))
            if v:
                return f"chat:{v}"
        bmd = body.get("metadata")
        if isinstance(bmd, dict):
            for k in ("chat_id", "conversation_id", "thread_id"):
                v = pick(bmd.get(k))
                if v:
                    return f"chat:{v}"

    v = pick(router.get("chat_id"))
    if v:
        return f"chat:{v}"

    # request headers (best-effort)
    try:
        if request is not None:
            headers = getattr(request, "headers", None)
            if headers:
                for hk in (
                    "x-openwebui-chat-id",
                    "x-chat-id",
                    "x-conversation-id",
                    "x-session-id",
                ):
                    hv = pick(headers.get(hk))
                    if hv:
                        return f"chat:{hv}"
    except Exception:
        pass

    # 4) deterministic fallback
    user_id = ""
    if isinstance(user, dict):
        user_id = (
            pick(user.get("id")) or pick(user.get("email")) or pick(user.get("name"))
        )

    anchor = ""
    if isinstance(messages, list) and messages:
        first_user = None
        for m in messages:
            if isinstance(m, dict) and m.get("role") == "user":
                first_user = m
                break
        if not first_user:
            for m in messages:
                if isinstance(m, dict):
                    first_user = m
                    break

        if isinstance(first_user, dict):
            for kk in ("chat_id", "conversation_id", "thread_id"):
                vv = pick(first_user.get(kk))
                if vv:
                    anchor = f"{kk}:{vv}"
                    break
            if not anchor:
                for kk in ("id", "_id", "message_id", "uuid"):
                    vv = pick(first_user.get(kk))
                    if vv:
                        anchor = f"msg:{vv}"
                        break
            if not anchor:
                for kk in ("created_at", "create_at", "timestamp", "ts", "time"):
                    vv = first_user.get(kk)
                    if vv is not None and str(vv).strip():
                        anchor = f"{kk}:{vv}"
                        break
            if not anchor:
                c = _norm_content(first_user.get("content")).strip()
                if c:
                    anchor = f"first_user:{c[:200]}"

    seed = f"{user_id}|{anchor}".strip("|")
    if seed:
        try:
            h = hashlib.sha1(seed.encode("utf-8", "ignore")).hexdigest()[:16]
        except Exception:
            h = str(abs(hash(seed)))[:16]
        return f"fb:{h}"

    # last resort: per-request random id (won't leak across chats)
    return f"req:{int(_now()*1000)}"


def _resolve_config(
    metadata: Dict[str, Any], router: Dict[str, Any]
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    dr = _get_dr_dict(metadata)
    cfg = router.get("config") if isinstance(router.get("config"), dict) else {}
    ui = router.get("ui") if isinstance(router.get("ui"), dict) else {}
    if not cfg:
        cfg = dr.get("config") if isinstance(dr.get("config"), dict) else {}
    if not ui:
        ui = dr.get("ui") if isinstance(dr.get("ui"), dict) else {}
    return cfg if isinstance(cfg, dict) else {}, ui if isinstance(ui, dict) else {}


def _clamp_min(v: Any, min_v: int, default: int) -> int:
    x = _as_int(v, default)
    return x if x >= min_v else min_v


# ----------------------------
# Tools conversion + execution
# ----------------------------
def _build_tool_registry(__tools__: Optional[dict]) -> Dict[str, Any]:
    reg: Dict[str, Any] = {}
    if not isinstance(__tools__, dict):
        return reg
    for tool in __tools__.values():
        if not isinstance(tool, dict):
            continue
        spec = tool.get("spec")
        if not isinstance(spec, dict):
            continue
        name = spec.get("name")
        if isinstance(name, str) and name.strip():
            reg[name.strip()] = tool
    return reg


def _tools_to_openai(__tools__: Optional[dict]) -> List[dict]:
    if not isinstance(__tools__, dict):
        return []
    out: List[dict] = []
    for tool in __tools__.values():
        if not isinstance(tool, dict):
            continue
        spec = tool.get("spec")
        if not isinstance(spec, dict):
            continue
        name = spec.get("name")
        if not isinstance(name, str) or not name.strip():
            continue
        desc = (
            spec.get("description") if isinstance(spec.get("description"), str) else ""
        )
        params = (
            spec.get("parameters") if isinstance(spec.get("parameters"), dict) else None
        )
        if params is None and isinstance(spec.get("input_schema"), dict):
            params = spec.get("input_schema")
        if params is None:
            params = {"type": "object", "properties": {}, "required": []}
        out.append(
            {
                "type": "function",
                "function": {"name": name, "description": desc, "parameters": params},
            }
        )
    return out


def _tool_name_of(tc: dict) -> str:
    fn = tc.get("function") or {}
    return str(fn.get("name") or tc.get("name") or "unknown")


def _tool_args_of(tc: dict) -> Any:
    fn = tc.get("function") or {}
    arg_raw = fn.get("arguments")
    if isinstance(arg_raw, str):
        j = _safe_json_loads(arg_raw)
        return j if j is not None else {"_raw": arg_raw}
    return arg_raw


async def _execute_one_tool(
    tool_registry: Dict[str, Any], name: str, args: Any, injection: Dict[str, Any]
) -> str:
    tool_obj = tool_registry.get(name)
    if tool_obj is None:
        return f"TOOL_NOT_FOUND: {name}"

    fn = None
    if isinstance(tool_obj, dict):
        fn = (
            tool_obj.get("callable") or tool_obj.get("func") or tool_obj.get("function")
        )
    if fn is None and callable(tool_obj):
        fn = tool_obj

    if not callable(fn):
        return f"TOOL_INVALID: {name}"

    args_dict: Dict[str, Any]
    if args is None:
        args_dict = {}
    elif isinstance(args, dict):
        args_dict = dict(args)
    else:
        args_dict = {"_args": args}

    # best-effort signature match
    kwargs: Dict[str, Any] = {}
    try:
        sig = inspect.signature(fn)
        params = sig.parameters
        has_varkw = any(
            p.kind == inspect.Parameter.VAR_KEYWORD for p in params.values()
        )
        if has_varkw:
            kwargs = {**injection, **args_dict}
        else:
            for pname in params.keys():
                if pname in args_dict:
                    kwargs[pname] = args_dict[pname]
                elif pname in injection:
                    kwargs[pname] = injection[pname]
    except Exception:
        kwargs = {**injection, **args_dict}

    try:
        r = fn(**kwargs)
        if inspect.isawaitable(r):
            r = await r
    except Exception as e:
        return f"TOOL_ERROR: {type(e).__name__}: {e}"

    if r is None:
        return ""
    if isinstance(r, str):
        return r
    try:
        return json.dumps(r, ensure_ascii=False)
    except Exception:
        return str(r)


def _pretty_json(obj: Any, max_len: int = 200) -> str:
    try:
        s = json.dumps(obj, ensure_ascii=False)
    except Exception:
        s = str(obj)
    s = s.replace("\n", " ")
    if len(s) > max_len:
        s = s[:max_len] + "…"
    return s


# ----------------------------
# Chunking helpers
# ----------------------------
def _chunk_text_safely(text: str, max_chars: int) -> List[str]:
    if not text:
        return [""]
    if max_chars <= 0 or len(text) <= max_chars:
        return [text]

    chunks: List[str] = []
    i = 0
    n = len(text)

    while i < n:
        if n - i <= max_chars:
            chunks.append(text[i:])
            break

        window_end = i + max_chars
        slice_ = text[i:window_end]

        # locate fences within window
        fence_positions = []
        idx = slice_.find("```")
        while idx != -1:
            fence_positions.append(idx)
            idx = slice_.find("```", idx + 3)

        def is_outside_code(pos: int) -> bool:
            cnt = 0
            for fp in fence_positions:
                if fp < pos:
                    cnt += 1
            return (cnt % 2) == 0

        cut = None
        search_from = len(slice_) - 1
        search_to = max(0, len(slice_) - 1200)
        for pos in range(search_from, search_to - 1, -1):
            if slice_[pos] == "\n" and is_outside_code(pos):
                cut = pos + 1
                break
        if cut is None:
            cut = len(slice_)

        part = text[i : i + cut].rstrip()
        if part.strip():
            chunks.append(part)
        i += cut

    return chunks or [text]


# ----------------------------
# User object
# ----------------------------
def _get_user_obj(__user__: Optional[dict]):
    if Users and __user__ and __user__.get("id"):
        try:
            return Users.get_user_by_id(__user__["id"])
        except Exception:
            pass
    d = __user__ or {}
    return SimpleNamespace(
        id=d.get("id"),
        email=d.get("email"),
        name=d.get("name"),
        role=d.get("role", "user"),
    )


# ----------------------------
# Pipe implementation
# ----------------------------
class Pipe:
    class Valves(BaseModel):
        INTERNAL_SKIP_FLAG_KEY: str = Field(
            default="dr_internal_call",
            description="Internal calls set this metadata flag True.",
        )
        DEFAULT_TOOL_CHOICE_AUTO: bool = Field(default=True)
        FORCE_PARALLEL_TOOL_CALLS_FALSE: bool = Field(default=True)

        # defaults if filter didn't provide config
        DEFAULT_MAX_ROUNDS: int = Field(default=50)
        DEFAULT_MIN_ROUNDS: int = Field(default=1)
        DEFAULT_MAX_TOOL_TURNS_PER_ROUND: int = Field(default=24)
        DEFAULT_MAX_TOOL_CALLS_PER_TURN: int = Field(default=12)
        DEFAULT_MAX_OUTPUT_CONTINUE_TURNS: int = Field(default=80)
        DEFAULT_COMPILE_MAX_OUTPUT_CONTINUE_TURNS: int = Field(default=60)
        DEFAULT_COMPILE_MAX_OUTPUT_CONTINUE_TURNS_CODE: int = Field(default=120)
        # Ultra-long continuation: patch-continue to reduce drift/loss
        ULTRA_LONG_PATCH_CONTINUE_ENABLED: bool = Field(
            default=True,
            description="超长续写：用补丁追加/修补替代纯‘继续’，降低漂移与丢失。",
        )
        ULTRA_LONG_PATCH_CONTINUE_MIN_CHARS: int = Field(
            default=30000,
            description="当累计输出字符数达到该阈值后，允许切换到补丁续写。",
        )
        ULTRA_LONG_PATCH_CONTINUE_SWITCH_AFTER_N: int = Field(
            default=2,
            description="前 N 段仍用普通续写；从第 N+1 段开始允许使用补丁续写。",
        )
        ULTRA_LONG_PATCH_CONTINUE_TARGET_CHUNK_CHARS: int = Field(
            default=12000,
            description="补丁续写每段目标追加字符数（尽量接近即可）。",
        )
        ULTRA_LONG_PATCH_CONTINUE_RETRY_PER_TURN: int = Field(
            default=3,
            description="补丁应用不完整/锚点未命中时，每段最多重试次数。",
        )
        ULTRA_LONG_PATCH_CONTINUE_MODEL: str = Field(
            default="",
            description="补丁续写专用模型（留空=优先 judge_model，其次 base_model）。",
        )

        DEFAULT_FINAL_MIN_CHARS: int = Field(default=8000)
        DEFAULT_CHUNK_MAX_CHARS: int = Field(default=40000)
        DEFAULT_CHUNK_POLICY: str = Field(default="auto")

        STATUS_TOOL_PREVIEW_CHARS: int = Field(default=180)
        ACTION_MAX_LEN: int = Field(default=80)

        DEFAULT_CODE_EOF_MARKER: str = Field(default="DR_EOF")

        # --- Artifact cache (persist last deliverable to disk) ---
        ARTIFACT_CACHE_ENABLED: bool = Field(
            default=True,
            description="将最近一次交付物持久化到磁盘缓存，便于在聊天上下文被截断时仍能恢复基线交付物（尤其用于交付物修改模式）。",
        )
        ARTIFACT_CACHE_DIR: str = Field(
            default="",
            description="交付物缓存目录（留空=自动选择 OpenWebUI 数据目录或临时目录）",
        )
        ARTIFACT_CACHE_MAX_CHARS: int = Field(
            default=400000,
            description="单份交付物缓存的最大字符数（超出会截断并标记 DR_TRUNCATED_BY_CACHE）",
        )
        ARTIFACT_CACHE_HISTORY_MAX: int = Field(
            default=20,
            description="每个 session 最多保留多少条历史缓存索引",
        )

        # --- Context stitcher (sticky project snapshot for routing/intent) ---
        CONTEXT_STITCH_ENABLED: bool = Field(
            default=True,
            description="启用“字段提取 + 上下文拼接”快照，用于在夹杂闲聊后仍能正确识别‘修改上一版交付物’Context。",
        )
        CONTEXT_STITCH_SHOW_STATUS: bool = Field(
            default=True,
            description="在状态输出中显示一次简短的‘上下文拼接摘要’（包含活跃交付物类型/语言/标题/字段数量等）。",
        )
        CONTEXT_STITCH_OUTLINE_LINES: int = Field(
            default=60,
            description="拼接上下文时最多包含多少行交付物结构字段（baseline_outline）。",
        )
        CONTEXT_STITCH_ANCHOR_LINES: int = Field(
            default=18,
            description="拼接上下文时最多包含多少条锚点候选（anchor_candidates）。",
        )
        CONTEXT_STITCH_RECENT_CHARS: int = Field(
            default=2400,
            description="拼接上下文时附带的最近对话节选最大字符数。",
        )

        CONTEXT_STITCH_HISTORY_STORE_ENABLED: bool = Field(
            default=True,
            description="是否启用历史上下文缓存（会把每次请求中的 messages 追加到会话历史，以便‘全看’）。",
        )
        CONTEXT_STITCH_HISTORY_STORE_MAX_MESSAGES: int = Field(
            default=240,
            description="历史上下文缓存最多保留多少条消息（滚动）。",
        )
        CONTEXT_STITCH_HISTORY_STORE_MAX_CHARS: int = Field(
            default=240000,
            description="历史上下文缓存最多保留多少字符（滚动）。",
        )
        CONTEXT_STITCH_HISTORY_STORE_PER_MESSAGE_MAX_CHARS: int = Field(
            default=12000,
            description="单条消息写入历史缓存时的最大字符数（超出会做头尾截断）。",
        )
        CONTEXT_STITCH_RETRIEVAL_ENABLED: bool = Field(
            default=True,
            description="是否从历史缓存中按相关性检索早期片段，拼接给路由/规划模型（避免被中间闲聊误导）。",
        )
        CONTEXT_STITCH_RETRIEVAL_TOP_K: int = Field(
            default=8,
            description="相关性检索最多选取多少条历史消息片段。",
        )
        CONTEXT_STITCH_RETRIEVAL_MAX_CHARS: int = Field(
            default=7000,
            description="相关性检索拼接出来的最大字符数。",
        )
        CONTEXT_STITCH_ARTIFACT_SLICES_ENABLED: bool = Field(
            default=True,
            description="是否从活跃交付物中按关键词抽取若干代码片段（用于‘我有代码但模型没看到’场景）。",
        )
        CONTEXT_STITCH_ARTIFACT_SLICES_MAX: int = Field(
            default=8,
            description="从交付物中最多抽取多少个相关代码片段。",
        )
        CONTEXT_STITCH_ARTIFACT_SLICE_WINDOW_LINES: int = Field(
            default=34,
            description="每个代码片段上下文窗口的行数（含命中行附近）。",
        )
        CONTEXT_STITCH_STATE_PERSIST_ENABLED: bool = Field(
            default=True,
            description="是否把历史缓存落盘（与 artifact_cache 同目录），以便进程重启/网关截断时仍能‘全看’）。",
        )

        # --- Ultra-long code stitch review & patch (no full regeneration) ---
        ENABLE_CODE_STITCH_REVIEW: bool = Field(
            default=True,
            description="对超长代码拼接结果做复查，并用补丁(替换/删除/插入)修复；不允许整份重写",
        )
        CODE_STITCH_REVIEW_MAX_PASSES: int = Field(
            default=3, description="最多补丁修复迭代次数"
        )
        CODE_STITCH_REVIEW_MODEL: str = Field(
            default="", description="补丁修复使用的模型(空=跟随base_model)"
        )
        CODE_STITCH_REVIEW_PROMPT_MAX_CHARS: int = Field(
            default=45000,
            description="补丁修复提示中最多携带多少字符的代码(超出将截取重点片段)",
        )
        CODE_STITCH_REVIEW_TRIGGER_CHARS: int = Field(
            default=25000,
            description="代码长度超过该阈值时，即便未检测到明显结构性问题也可触发复查（需配合 CODE_STITCH_REVIEW_ONLY_WHEN_ISSUES=false）",
        )
        CODE_STITCH_REVIEW_ONLY_WHEN_ISSUES: bool = Field(
            default=True, description="仅当检测到结构性问题时才触发补丁修复"
        )

        # --- Final audit gate (code + ultra-long reports) ---
        ENABLE_FINAL_AUDIT: bool = Field(
            default=True,
            description="对最终交付物进行终审审计：模型自查并输出补丁，直到审计放行或达到最大轮次",
        )
        FINAL_AUDIT_TRIGGER_CHARS: int = Field(
            default=12000,
            description="最终输出长度超过该阈值时触发终审审计（代码交付通常也会触发）",
        )
        FINAL_AUDIT_MAX_PASSES: int = Field(
            default=5, description="终审审计最多迭代次数（每轮可产出补丁并重新审计）"
        )
        FINAL_AUDIT_MODEL: str = Field(
            default="", description="终审审计使用的模型(空=跟随base_model)"
        )
        FINAL_AUDIT_PROMPT_MAX_CHARS: int = Field(
            default=60000,
            description="终审审计提示中最多携带多少字符的内容（超出将截取头尾摘要）",
        )
        FINAL_AUDIT_ONLY_WHEN_ISSUES: bool = Field(
            default=True,
            description="仅当启发式检测到问题才触发终审审计；默认对超长内容也执行一次审计",
        )
        FINAL_AUDIT_SYSTEM_PROMPT: str = Field(
            default=(
                "你是最终交付物的‘终审审计器 + 补丁生成器’。\n"
                "你会收到：交付物类型（code/report/other）、语言、EOF 标记（如有）、启发式问题列表、需求摘要 requirements_summary、必须实现/必须包含 must_have（可能为空）、复杂度 complexity、生成工作流 build_workflow、以及交付物内容（可能是头/尾摘要；摘要中会包含 [DR_EXCERPT_CUT] 标记，注意这些标记不在原文中，禁止用作补丁锚点）。\n\n"
                "目标：如果可以放行，返回 pass=true；否则返回 pass=false，并给出最小必要 patches 来修复可用性与完整性问题。\n"
                "审计重点：\n"
                "- 任何拼接/截断/重复/残留提示（例如：‘回复继续’、分段脚注、重复的 DOCTYPE/</html>、多余代码块）\n"
                "- 结构性错误（代码：围栏/括号/标签明显不平衡；报告：标题/章节重复、结论缺失、关键清单缺失）\n"
                "- 可用性增强（代码：修复明显语法/引用错误、补齐缺失的关键函数/变量；报告：补齐行动项/风险/限制条件/下一步）\n\n"
                "严格只输出 JSON（不要 markdown，不要解释），格式：\n"
                "{\n"
                '  "pass": boolean,\n'
                '  "issues": string[],\n'
                '  "patches": [\n'
                '    {"op":"replace", "find":string, "replace":string, "occurrence":int|"all"},\n'
                '    {"op":"delete_between", "start":string, "end":string, "include_end":boolean, "occurrence":int},\n'
                '    {"op":"insert_after", "anchor":string, "insert":string, "occurrence":int},\n'
                '    {"op":"insert_before", "anchor":string, "insert":string, "occurrence":int},\n'
                '    {"op":"append", "insert":string},\n'
                '    {"op":"prepend", "insert":string}\n'
                "  ],\n"
                '  "notes": string\n'
                "}\n\n"
                "补丁约束：\n"
                "1) find/start/end/anchor 必须是‘原文中连续出现的精确子串’，不要使用省略号或你虚构的文本。\n"
                "2) 尽量让锚点足够长且唯一（建议 30-120 字符），避免误伤。\n"
                "3) 不允许输出整份重写；除非确实无法用补丁修复，才用少量 append/prepend 补齐缺失段落。\n"
            )
        )

        # alignment / confirmation
        ALIGNMENT_STATUS_MAX_CHARS: int = Field(default=260, ge=80, le=800)

        CHUNK_FOOTER: str = Field(
            default="\n\n——\n（第 {i}/{n} 段。回复“继续/接着/ok”➡️ 输出下一段）"
        )

        # prompts
        SCOPE_SYSTEM_PROMPT: str = Field(
            default=(
                "你是 Deep Research 的“需求理解/范围界定”助手（主模型）。\n"
                "你的目标：先把用户需求复述清楚并对齐范围，然后再开始工具检索与深度分析。\n"
                "原则：默认不阻塞推进——只要能在合理默认假设下开始，就直接开始；只有遇到“完全绕不过去/会导致研究对象错误”的关键点才需要提问澄清。\n\n"
                "只输出严格 JSON（不要 markdown，不要解释），格式：\n"
                "{"
                '"target": string, '
                '"target_locked": boolean, '
                '"goal": string, '
                '"need_clarification": boolean, '
                '"question": string, '
                '"confirm_recommended": boolean, '
                '"confirm_question": string, '
                '"assumptions": string[], '
                '"focus": string[]'
                "}\n\n"
                "规则：\n"
                "1) target 是研究对象（公司/股票/产品/主题）。如果用户已明确给出，就写入并 target_locked=true。\n"
                "   允许对明显的错别字/简称做归一化（例如“金证服份”->“金证股份”）。\n"
                "2) need_clarification=true 仅在“完全无法继续”时使用（例如 target 缺失/冲突）。一次只问 1 个问题。\n"
                "3) 对偏好问题（周期/风险/格式）不要阻塞推进：放进 assumptions 和 focus。\n"
                "4) 用户说“随便/都可以/你决定/按你想的”表示接受你选默认范围，不表示更换 target。\n"
                "5) confirm_recommended 通常应为 false：因为系统会在状态栏给出“需求对齐+默认假设+可调整项”，并直接开始执行。\n"
                "   只有当用户明确要求“先确认再做”，或你必须让用户在互斥选项中做选择时，才设为 true。\n"
                "6) confirm_question 必须是“可选的、具体可调整点”的提问；严禁输出泛泛的“是否继续/要不要继续/是否开始”等 yes-no 话术。\n"
                "   且用户可以用“按默认/改成XXX”直接回答。若不存在必须确认的点，请留空。\n"
            )
        )

        PLANNER_SYSTEM_PROMPT: str = Field(
            default=(
                "你是 Deep Research 的任务拆分器（主模型）。\n"
                "只输出严格 JSON（不要 markdown，不要解释），格式：\n"
                "{"
                '"plan":[{"step":int,"title":string,"goal":string,"done":string}],'
                '"notes":string'
                "}\n"
                "要求：3-7 步；不要提出澄清问题；每一步都能独立推进。\n"
                "如果研究涉及“今天/最近/近X个月/最新/实时”等时间敏感描述，请在 notes 里注明“截至时间锚点”。\n"
                "如果会用到图表/图片工具，请在 notes 里注明“输出时需引用工具返回的 Markdown 片段（如 [CHART_MARKDOWN_BEGIN]...）”。\n"
            )
        )

        DELIVERABLE_SYSTEM_PROMPT: str = Field(
            default=(
                """你是交付物侦测器（deliverable detector）。
给定用户需求与规划概要，输出一个 JSON 对象，描述最终交付物的类型与生成策略。
必须只输出 JSON，不要输出多余文字。

JSON 字段（全部必须给出）：
{
  "deliverable_type": "code"|"report"|"analysis"|"mixed"|"other",
  "must_include_full_code": true|false,
  "preferred_code_language": "<e.g. html|javascript|python|bash|text>",
  "is_single_file": true|false,
  "need_eof_marker": true|false,
  "eof_marker": "<marker string, default DR_EOF>",
  "build_workflow": "direct"|"patch",
  "complexity": "simple"|"standard"|"complex",
  "requirements_summary": "<1-2 sentences in Chinese, summarize must-have requirements>",
  "must_have": ["<feature/section 1>", "..."],
  "notes": "<short notes for compiler/auditor>"
}

决策规则（让模型自行判断，避免使用固定字数阈值）：
- 若 deliverable_type 包含 code：need_eof_marker=true；eof_marker 默认用 "DR_EOF"（除非用户指定）。
- 对代码：当需求明显是“复杂/完整项目/单文件应用或游戏/多系统”，优先选择 build_workflow="patch"；否则 "direct"。
- 对长文本：仅当用户明确需要“超长/全量/手册/很多章节/不允许省略”等，才选择 build_workflow="patch"；一般用 "direct"。
- must_have：列出 6-12 个最关键的“必须实现/必须包含”点，供后续审计与补丁修复判定缺失。
"""
            )
        )

        GENERATE_SYSTEM_PROMPT: str = Field(
            default=(
                "你在执行 Deep Research（深度研究）。\n"
                "请用自然交流的语气，但输出要专业、结构化、可复用。\n\n"
                "硬性要求：\n"
                "1) 每次回复必须先输出一行：<dr_action>一句话说明你接下来要做什么（<=80字）</dr_action>\n"
                "2) 如需事实/数据/最新信息：优先调用 tools（如果提供）。\n"
                "3) 如果要调用 tools：也必须输出 <dr_action> 行（content 不能为空）。\n"
                "4) 工具结果返回后：必须输出可交付的正文结果（可直接作为最终交付物的一部分：代码/章节/清单/表格等），内容要尽量完整详尽；不要只给几行结论，更不要“等待用户补充”。\n"
                "5) 用户说“随便/都可以/你决定”：用合理默认假设推进，并在答案开头列出【默认假设】与【可调整项】。\n"
                "6) 如果系统消息里提供了【本轮改进重点】，你的 <dr_action> 与正文必须围绕这些改进点推进（不要复读上一轮同一句话）。\n"
                "7) 【图表/图片渲染】有些工具会返回图表 Markdown 片段（含 ![...](...)）。\n"
                "   你不要把这些片段原样贴到正文里；而是在正文中用占位符标出图表应出现的位置：[[CHART_1]]、[[CHART_2]] ...（每个仅一次）。\n"
                "   规则：不要把占位符放进```代码块```；不要伪造图片链接。pipe 会在最终交付阶段自动把占位符替换为真实 Markdown，从而让图片出现在正文里。\n"
            )
        )

        JUDGE_SYSTEM_PROMPT: str = Field(
            default=(
                "你是严格评审器（只做推荐，不做最终决定；工具不可用）。\n"
                "你可以做两件事让流程更智能：\n"
                "A) 给出缺口与改进建议；\n"
                "B) 如有必要，直接给出“下一轮应使用的改写版 user_text”，以及“更新后的研究计划”。\n\n"
                "只输出严格 JSON（不要 markdown，不要解释），格式：\n"
                "{"
                '"recommend_done":boolean,'
                '"need_clarification":boolean,'
                '"question":string,'
                '"missing":string,'
                '"suggestions":string[],'
                '"step_done":boolean,'
                '"next_user_text":string,'
                '"plan_update":{'
                '"plan":[{"step":int,"title":string,"goal":string,"done":string}],'
                '"notes":string'
                "},"
                '"next_step_idx":int,'
                '"ui_status":string'
                "}\n"
                "说明：\n"
                "- recommend_done=true 表示你认为已经可交付。\n"
                "- need_clarification=true 仅在完全绕不过去时使用（一次只问 1 个问题）。\n"
                "- next_user_text：如果你认为下一轮不应该只是“追加补全指令”，而应该改写成更清晰、更可执行的请求，请在这里给出完整下一轮 user_text（纯文本即可，避免超长、避免重复整段旧答案）。否则留空。\n"
                "- plan_update：如果你认为原 plan 不再贴合现有内容/需要调整顺序/新增合并步骤，请给出新的 plan_update；否则给空对象 {} 或给空 plan。\n"
                "- next_step_idx：0-based，表示更新 plan 后下一轮从哪一步开始；不需要则填 -1。\n"
                "- ui_status：给前端状态栏显示的一句话（<=40字）。\n"
            )
        )

        DECIDER_SYSTEM_PROMPT: str = Field(
            default=(
                "你是 Deep Research 的最终决策者（主模型）。你要基于用户需求、当前答案、以及评审建议做决定。\n"
                "只输出严格 JSON（不要 markdown，不要解释），格式：\n"
                "{"
                '"decision":"done"|"continue"|"ask",'
                '"question":string,'
                '"advance_step":boolean,'
                '"next_instructions":string,'
                '"why":string'
                "}\n\n"
                "决策规则：\n"
                "1) ask 仅在完全绕不过去的关键问题时使用，一次只问 1 个问题。\n"
                "2) 如果当前答案主要是在向用户索要信息（例如“请提供/等待您提供...”），通常不应 decision=done。\n"
                "3) 用户说“随便/都可以”表示允许你默认推进，不代表研究对象可以更换。\n"
                "4) 除非用户明确要求更换，否则研究对象（target）必须保持不变。\n"
            )
        )

        CONTINUE_SYSTEM_PROMPT: str = Field(
            default=(
                "你正在续写上一段未完成的输出。\n"
                "要求：\n"
                "- 必须“紧接着上一次输出结尾”继续，不要重复已输出内容。\n"
                "- 保持相同的结构与格式。\n"
                "- 如果在代码块中间被截断，必须先正确闭合/继续该代码块。\n"
                "- 如果上一次输出包含图表 Markdown 片段（例如 ![...](...) 这一行）或任何渲染引用片段，请原样保留与延续，不要改写。\n"
            )
        )

        TURN_ROUTER_SYSTEM_PROMPT: str = Field(
            default=r"""你是 Deep Research 会话的『单轮路由判定器』。

你会收到一个 JSON（字符串），字段通常包含：
- user_message：本轮用户输入
- has_deliverable：是否已有上一版交付物（可能是代码/报告）
- deliverable_type："code"|"report"|""（可能为空）
- pending_kind：当前会话是否处于暂停等待（例如 need_artifact/clarify/confirm/user_pause）
- pending_question：如果有，系统上一轮问的问题
- has_pending_chunks：是否存在分段输出等待“继续”
- conversation_context_excerpt：最近对话的压缩上下文（可能为空）
- context_pack：系统已从“当前活跃交付物/需求快照/最近对话”提取并拼接的关键字段（可能为空）
- 重要：如果 context_pack.relevant_history_excerpt 非空，表示系统从“更早的全量历史缓存”里按相关性检索出的片段（通常用于避免被中间闲聊误导）。
- 重要：如果 context_pack.artifact_slices 非空，表示从活跃交付物中按关键词抽取的代码片段（用于定位要改哪里）。

你的任务：判断本轮应当走哪条路线。

【route 四选一】
- "chunk_continue"：用户是在分段输出场景下请求继续输出下一段（典型："继续/next/continue"）
- "answer_pause"：用户是在回答 pending_question / 补充缺失信息（不是要修改上一版交付物）
- "code_modify"：用户希望基于上一版交付物继续修改/修复/增加内容（默认要求输出“完整可用版本”，而不是差分）
- "new_task"：用户开启了一个新目标/新项目，与上一版交付物无关

【should_restore_deliverable】
当 has_deliverable=false 且 route="code_modify" 时：
- 如果你认为应该尝试从聊天记录中恢复上一版交付物，则为 true；否则为 false。

【输出格式】
你必须只输出可解析 JSON（不要解释、不要 markdown、不要额外文本）：
{
  "route": "chunk_continue"|"answer_pause"|"code_modify"|"new_task",
  "should_restore_deliverable": boolean,
  "confidence": 0.0,
  "why": "一句话理由"
}

【判定规则（简要）】
- 若 context_pack 提供了 active_artifact/requirements_summary 等字段，请优先用它判断是否在改同一份交付物（避免被无关闲聊误导）。
- 若 has_pending_chunks=true 且 user_message 很像“继续”，优先 route="chunk_continue"。
- 若 pending_kind 存在且 user_message 明显在回答 pending_question，route="answer_pause"。
- 若用户提到“在上面/上一版/继续修改/修复/补充/增加内容/按原代码改”等，或粘贴了大量代码，route="code_modify"。
- 只有当用户明确切换到完全不同目标时才 route="new_task"。
""",
            description="System prompt for per-turn router classifier.",
        )

        FOLLOWUP_INTENT_SYSTEM_PROMPT: str = Field(
            default="""你是 Deep Research 会话的『追问路由器/意图分类器』。

你会收到一段 JSON（字符串），字段通常包含：
- user_message：用户最新追问/补充
- prev_target：上一次交付物的对象/产品/主题
- prev_spec：上一次交付物规格（deliverable_type、code_language、eof_marker 等）
- prev_scoped：上一次 scope 摘要
- prev_locked_target：是否锁定目标（以及锁定的目标）
- prev_deliverable_stats：上一次交付物规模（chars、lines、is_code 等）
- prev_deliverable_head / prev_deliverable_tail：上一次交付物的头尾片段（用于辅助判断）
- context_pack：系统拼接的关键上下文（可能包含需求摘要/交付物关键字段/最近对话节选）
- 注意：context_pack.relevant_history_excerpt 若存在，代表从全量历史缓存里检索出的更早但相关内容；不要只盯最近几句。

你的任务：判断这条追问属于哪一类，并给出后续执行策略。

【可选意图 intent】（四选一）：
1) patch：对『同一个交付物』做增量修改/修复/优化（保持主体不变，尽量最小改动）。
2) regenerate：仍然是『同一个交付物』，但改动面很大或重构更划算/更安全，建议直接重新生成完整交付物。
3) supplement：不改交付物本体，只补充说明/文档/测试步骤/使用指南/变更说明等。
4) new_task：新目标/新项目/不同交付物（与上次不属于同一条产品线或无法通过修改达成）。

【关键判定规则】
- 若 context_pack 显示存在活跃交付物且用户语句像“继续改/修复/增加”，即便最近对话夹杂闲聊，也应优先 patch/regenerate，而不是 new_task。
- 只要用户明显是在『基于上一版』提需求（例如：增加关卡、优化美术/剧情、修 bug、增强 UI、提升移动端体验、加保存、加音效等），优先判为 patch 或 regenerate，而不是 new_task。
- supplement 仅在用户明确要“说明/文档/教程/测试用例/注释/README/攻略”等且不要求改代码/改正文时使用。
- new_task 仅在用户明确切换到另一个完全不同的目标（例如换题材/换产品/换交付物类型）时使用。

【patch vs regenerate 的取舍（让你更智能）】
- patch 更适合：改动点局部、可定位；或上次交付物很大/很复杂，保持可用更重要；或用户强调“在现有版本上改”。
- regenerate 更适合：用户要大幅重构/整体风格重做/玩法大改；改动会波及很多文件区域；或上次交付物规模较小（重写更省）/已出现多处拼接残缺（重生成更稳）。

【pipeline 选择】
- patch / regenerate / supplement：默认 pipeline="fast"（不要重新跑完整 Deep Research 多轮流程；最多做一个很短的变更清单，然后直接改/生成）。
- new_task：pipeline="full"（需要重新 scope + 规划 + 多轮生成/审计）。

【context_injection 选择】
- full：需要看全量上次交付物才能正确修改。
- excerpt：只需要相关片段即可（或上次交付物超长，建议只用头尾+关键片段）。
- summary：只需高层摘要，不需要代码细节。

【输出格式】
你必须只输出 JSON（不要解释、不要提问、不要额外文本）。
JSON 字段：
- intent: "patch" | "regenerate" | "supplement" | "new_task"
- delta_scope: "tiny" | "small" | "medium" | "large"（你预计改动覆盖面）
- pipeline: "fast" | "full"
- context_injection: "full" | "excerpt" | "summary"
- patch_focus: string[]（列出你认为要动的模块/方向，便于后续执行）
- confidence: 0.0~1.0
- why: string（简短理由，1-2 句）

请确保 JSON 可被严格解析。
""",
            description="System prompt for follow-up intent classifier.",
        )

        # 追问快速生成：更新/补丁
        FOLLOWUP_UPDATE_SYSTEM_PROMPT: str = Field(
            default="""
你是『交付物追问更新器』。

你会收到一个 JSON（字符串），包含：
- mode: patch | regenerate（patch=尽量在原交付物基础上做最小必要改动；regenerate=允许重写/重排，但必须保留同一目标与交付类型）
- user_message: 用户最新追问/修改点
- prev_target: 上一次交付物的目标/对象
- prev_spec: 上一次交付物的规格（deliverable_type/code_language/eof_marker/must_have 等）
- prev_scoped: 上一次的范围信息（目标/约束/假设等，可能为空）
- prev_deliverable_context: 上一次交付物内容（可能是全文/节选/摘要）
- patch_focus: 建议优先处理的点（可能为空）

你的任务：
1) 结合 user_message 与 prev_* 信息，直接产出『更新后的完整交付物』。
2) 不要反问用户要哪个项目/是哪款游戏；默认就是 prev_target 对应的交付物。
3) 不要输出差分/解释；只输出最终交付物本体。

强制输出规则：
- 如果 prev_spec.deliverable_type == "code"：
  - 必须只输出一个代码块（fenced code block），语言使用 prev_spec.code_language（如 html/js/python 等）。
  - 代码块内必须是可直接保存运行的完整文件/项目主文件。
  - 如果 prev_spec.need_eof_marker 为 true：代码块内必须在最后包含 eof_marker（原样）作为终止标记。
- 如果 prev_spec.deliverable_type != "code"：
  - 输出最终文本交付物即可（不要输出 JSON）。

质量要求：
- patch 模式：尽量保持原有结构与可用逻辑，只做与追问相关的最小必要改动；不要无故重命名/重排大量代码。
- regenerate 模式：可以重构以更好满足追问，但仍需保持与原需求一致（同类型、同目标、可直接使用）。
- 任何情况下都要确保语法完整、可运行/可阅读，不要留下 TODO 或半成品。
""",
            description="System prompt for fast follow-up patch/regenerate generator.",
        )

        FOLLOWUP_SUPPLEMENT_SYSTEM_PROMPT: str = Field(
            default="""
你是『交付物追问补充生成器』。

你会收到一个 JSON（字符串），包含：
- user_message: 用户追问/补充需求（偏文档/说明/测试/玩法介绍等）
- prev_target: 上一次交付物目标
- prev_spec: 上一次交付物规格
- prev_scoped: 上一次范围信息
- prev_deliverable_context: 上一次交付物内容（可能是节选）

任务：生成用户需要的补充内容（文档/说明/测试计划/变更说明等）。
要求：
- 直接给出可用的补充内容，不要反问。
- 不要修改代码本体；除非用户明确要求提供代码片段。
""",
            description="System prompt for follow-up supplement generator.",
        )

        # ----------------------------
        # Artifact Edit Mode (staged patch apply; avoid full rewrite)
        # ----------------------------
        ENABLE_ARTIFACT_EDIT_MODE: bool = Field(
            default=True,
            description="启用‘交付物修改模式’：读取上一版交付物 -> 生成分阶段修改意见 -> 多次生成补丁并自动应用到原文（不整份重写），最后输出完整可用版本。用于 patch/regenerate 追问，避免大内容重写丢东西。",
        )

        ARTIFACT_EDIT_MAX_STAGES: int = Field(
            default=8, description="修改模式：最多分多少个补丁阶段"
        )
        ARTIFACT_EDIT_MIN_STAGES: int = Field(
            default=3,
            description="修改模式：最少阶段数（阶段过少会导致一次补丁过大、容易丢内容）",
        )

        ARTIFACT_EDIT_FUZZY_MATCH: bool = Field(
            default=True,
            description="修改模式：补丁锚点匹配失败时启用高阈值模糊匹配（适用于差 1-3 个字符）",
        )
        ARTIFACT_EDIT_FUZZY_MIN_RATIO: float = Field(
            default=0.985, description="修改模式：模糊匹配最小相似度（0-1，越高越安全）"
        )
        ARTIFACT_EDIT_FUZZY_MAX_CANDIDATES: int = Field(
            default=120, description="修改模式：模糊匹配候选数量上限"
        )
        ARTIFACT_EDIT_FUZZY_SHORT_MIN_LEN: int = Field(
            default=24, description="修改模式：参与模糊匹配的最短锚点长度（太短不安全）"
        )
        ARTIFACT_EDIT_FUZZY_AMBIGUITY_MARGIN: float = Field(
            default=0.01,
            description="修改模式：若最佳/次佳候选相似度差小于该值则视为歧义，拒绝模糊替换",
        )
        ARTIFACT_EDIT_WS_REGEX_MATCH: bool = Field(
            default=True, description="修改模式：启用空白字符容错匹配（\s+）"
        )
        ARTIFACT_EDIT_MAX_PATCH_ATTEMPTS_PER_STAGE: int = Field(
            default=8, description="修改模式：每个阶段补丁生成失败时最多重试次数"
        )

        ARTIFACT_EDIT_STOP_ON_STAGE_FAIL: bool = Field(
            default=True,
            description="修改模式：若某阶段在重试后仍失败，是否停止并进入更强的补丁生成/定位（而不是跳过）。代码类交付物建议开启。",
        )

        ARTIFACT_EDIT_STAGE_CRITICAL_EXTRA_ATTEMPTS: int = Field(
            default=4,
            description="修改模式：当阶段标题/goal 命中‘修复/启动/报错’等关键字时，额外增加的重试次数（在每阶段基础重试次数之上）。",
        )

        ARTIFACT_EDIT_SHOW_ATTEMPT_STATUS: bool = Field(
            default=True,
            description="修改模式：是否在状态栏输出每次补丁尝试的简要结果（应用/跳过数量与失败原因）。",
        )

        ARTIFACT_EDIT_MAX_PATCHES_PER_ATTEMPT: int = Field(
            default=12,
            description="修改模式：每次补丁生成最多输出多少条 patches（太多会导致锚点失配/部分跳过）",
        )
        ARTIFACT_EDIT_REQUIRE_FULL_APPLY_FOR_CODE: bool = Field(
            default=True,
            description="修改模式：代码类交付物默认要求本轮 patches 全部成功应用；否则进入重试（更稳，避免半应用导致残缺）",
        )
        ARTIFACT_EDIT_ALLOWED_SKIP_RATIO: float = Field(
            default=0.0,
            description="修改模式：允许跳过补丁的比例（0-1）。代码类默认 0；文档类可适当放宽",
        )
        ARTIFACT_EDIT_RETRY_ON_PARTIAL_APPLY: bool = Field(
            default=True,
            description="修改模式：如果存在 skipped/部分应用，是否自动带失败信息重试补丁",
        )
        ARTIFACT_EDIT_COMMIT_PARTIAL_APPLY: bool = Field(
            default=True,
            description="修改模式：部分补丁已应用且无明显破坏时，先保留已应用内容，再继续重试剩余补丁",
        )
        ARTIFACT_EDIT_REQUIRE_PROGRESS: bool = Field(
            default=True,
            description="修改模式：补丁必须产生实际文本变化（new!=old），否则视为失败并重试",
        )

        ARTIFACT_EDIT_CONTEXT_FULL_MAX_CHARS: int = Field(
            default=180000,
            description="修改模式：基线交付物全文注入阈值（字符）。超过则改用头尾节选",
        )
        ARTIFACT_EDIT_CONTEXT_HEAD_CHARS: int = Field(
            default=45000, description="修改模式：节选头部字符数"
        )
        ARTIFACT_EDIT_CONTEXT_TAIL_CHARS: int = Field(
            default=45000, description="修改模式：节选尾部字符数"
        )

        # --- Artifact Detach (超大交付物分离) ---
        ARTIFACT_DETACH_ENABLED: bool = Field(
            default=True,
            description="超大交付物：启用‘交付物分离’（不把全文塞进模型上下文，仅注入大纲/头尾/聚焦片段），用于避免模型处理超长代码/长文时丢内容",
        )
        ARTIFACT_DETACH_MIN_CHARS: int = Field(
            default=60000,
            description="超大交付物：当基线交付物字符数 >= 该阈值时进入分离模式",
        )
        ARTIFACT_DETACH_CONTEXT_BUDGET_CHARS: int = Field(
            default=24000,
            description="分离模式：注入给模型的总字符预算（包含头尾节选 + 聚焦片段 + 大纲；越小越不容易丢内容）",
        )
        ARTIFACT_DETACH_HEAD_CHARS: int = Field(
            default=8000, description="分离模式：头部节选字符数"
        )
        ARTIFACT_DETACH_TAIL_CHARS: int = Field(
            default=8000, description="分离模式：尾部节选字符数"
        )
        ARTIFACT_DETACH_MAX_SNIPPETS: int = Field(
            default=10, description="分离模式：最多注入多少条聚焦片段"
        )
        ARTIFACT_DETACH_SNIPPET_WINDOW: int = Field(
            default=1200, description="分离模式：聚焦片段窗口大小（左右各多少字符）"
        )
        ARTIFACT_DETACH_OUTLINE_MAX_ITEMS: int = Field(
            default=60, description="分离模式：基线大纲最多保留多少条"
        )

        # --- Artifact Detach Threshold (智能判定) ---
        ARTIFACT_DETACH_THRESHOLD_MODE: str = Field(
            default="auto",
            description="超大交付物：分离阈值策略 fixed=使用MIN_CHARS；auto=启发式+必要时模型；smart=总是用模型判断是否分离。",
        )
        ARTIFACT_DETACH_SMART_MODEL: str = Field(
            default="",
            description="智能分离判定使用的模型（留空=judge_model；再留空=base_model）。",
        )
        ARTIFACT_DETACH_SMART_LOWER_BOUND: int = Field(
            default=20000,
            description="智能分离：交付物长度<=该值时不分离（允许注入全文以提升补丁精确度）。",
        )
        ARTIFACT_DETACH_SMART_UPPER_BOUND: int = Field(
            default=180000,
            description="智能分离：交付物长度>=该值时强制分离（避免模型处理超长交付物时丢内容）。",
        )
        ARTIFACT_DETACH_SMART_AMBIGUOUS_BAND: int = Field(
            default=15000,
            description="auto模式：仅当长度落在 MIN_CHARS±band 范围内才调用模型智能判定；否则仍按固定阈值规则。",
        )
        ARTIFACT_DETACH_SMART_SNIPPET_CHARS: int = Field(
            default=1200,
            description="智能分离：提供给判定模型的头部节选字符数（越小越快）。",
        )
        ARTIFACT_DETACH_SMART_MAX_TOKENS: int = Field(
            default=220,
            description="智能分离：判定模型最大输出 tokens（越小越快）。",
        )

        ARTIFACT_EDIT_STAGE_MIN_LEN_RATIO: float = Field(
            default=0.85,
            description="修改模式：单阶段补丁应用后内容长度不应低于上一版比例（防止误删大段）",
        )
        ARTIFACT_EDIT_OVERALL_MIN_LEN_RATIO: float = Field(
            default=0.8,
            description="修改模式：最终内容长度不应低于原始基线比例（防止丢内容）",
        )

        ARTIFACT_EDIT_ANCHOR_CANDIDATES: int = Field(
            default=12, description="修改模式：从基线提取多少条锚点候选用于防丢校验"
        )
        ARTIFACT_EDIT_ANCHOR_MIN_HITS: int = Field(
            default=3, description="修改模式：最终至少保留多少条锚点候选（防丢校验）"
        )

        ARTIFACT_EDIT_OUTPUT_PREFACE_MAX_LINES: int = Field(
            default=12,
            description="修改模式：最终输出正文前最多给几行简要说明/使用方式",
        )

        ARTIFACT_EDIT_ALIGNMENT_SYSTEM_PROMPT: str = Field(
            default="""
            你是‘交付物修改模式’的【需求对齐器】。

            输入：一个 JSON 字符串，包含 user_message、deliverable_type、code_language、baseline_len、baseline_head、baseline_tail、requirements_summary、must_have、intent、patch_focus 等。

            任务：
            1) 判断信息是否足够，能否直接进入“分阶段补丁修改”。
            2) 若信息不足或用户意图过于模糊，请提出【一个】关键澄清问题（只允许一个问题）。
            3) 若信息足够，请输出对齐后的目标、必改点、补丁重点关键词（patch_focus），并给出必要的假设与风险提示。

            输出必须是严格 JSON（不要 markdown、不要解释、不要多余文字），格式：
            {
              "ready": true/false,
              "clarify_question": "...",
              "aligned_summary": "...",
              "goals": ["..."],
              "assumptions": ["..."],
              "patch_focus": ["..."],
              "must_have": ["..."],
              "requirements_summary": "...",
              "risk_notes": "..."
            }

            规则：
            - 只有当 ready=false 时才输出 clarify_question；且 clarify_question 只能有一个。
            - 如果 baseline_len 很小，或 baseline_head/baseline_tail 无法判断结构（看不出函数/模块/关键信息），优先问用户粘贴/提供基线交付物。
            - patch_focus 尽量使用“可以在原文中直接搜索命中的关键词/锚点”，避免空泛。
            - requirements_summary 用 1-3 句话概括即可；must_have 以列表给出关键约束。
            """,
            description="System prompt for Artifact Edit Mode alignment step (outputs JSON).",
        )

        ARTIFACT_EDIT_STAGE_PLANNER_SYSTEM_PROMPT: str = Field(
            default="""
            你是‘交付物修改模式’的阶段规划器。
            输入：一个 JSON 字符串，包含 user_message、deliverable_type、code_language、requirements_summary、must_have、baseline_head、baseline_tail、baseline_len，以及 anchor_candidates、baseline_outline 等信息。
            
            任务：把用户的修改需求拆成 3-8 个‘阶段’，每个阶段都应该能通过‘补丁’在原交付物上完成，避免整份重写导致丢内容。
            优先顺序建议：先修复可运行/启动/报错 -> 再调参/手感/性能/兼容性 -> 再新增内容/功能 -> 最后清理一致性/补齐测试清单。
            
            每个阶段请尽量写清楚“改哪里”：
            - where：大概修改位置/模块（优先引用 anchor_candidates 或 baseline_outline 中出现的名字/片段，避免空泛）
            - keywords：用于定位的关键词/锚点（1-6 个字符串，最好能在原文中直接搜索到）
            
            输出必须是严格 JSON（不要 markdown、不要解释、不要提问），格式：
            {
              "stages": [
                {"id":1, "title":"...", "goal":"...", "where":"...", "keywords":["..."], "critical": true, "notes":"..."}
              ],
              "risk_notes": "..."
            }
            """,
            description="System prompt for Artifact Edit Mode stage planner (outputs JSON stages).",
        )

        ARTIFACT_EDIT_PATCH_SYSTEM_PROMPT: str = Field(
            default="""
            你是‘交付物修改模式’的补丁生成器。
            输入：一个 JSON 字符串，包含 stage（id/title/goal/where/keywords）、user_message、deliverable_type、code_language、eof_marker、requirements_summary、must_have，
            以及 baseline（可能是全文或节选；节选中可能含 [TRUNCATED]/[DR_EXCERPT_CUT] 等标记，这些标记不在原文中，禁止用作补丁锚点）。
            当 baseline 为节选且不包含目标区域时：请优先使用 focus_snippets / anchor_candidates 提供的原文片段作为 find/anchor（必须逐字复制）；不要臆造不存在的上下文。
            此外可能包含 focus_snippets（从全文中抽取的若干相关片段），以及 anchor_candidates。
            
            任务：只输出严格 JSON，给出可自动应用的补丁 patches；禁止输出修复后的完整交付物；禁止输出 markdown；不要解释。
            
            输出格式：
            {
              "patches": [
                {"op":"replace", "find":string, "replace":string, "occurrence":1|"all"},
                {"op":"delete_between", "start":string, "end":string, "include_end":true|false, "occurrence":1},
                {"op":"insert_after", "anchor":string, "insert":string, "occurrence":1},
                {"op":"insert_before", "anchor":string, "insert":string, "occurrence":1},
                {"op":"append", "insert":string},
                {"op":"prepend", "insert":string}
              ],
              "notes": "..."
            }
            
            补丁规则（非常重要）：
            1) find/start/end/anchor 必须来自 baseline 或 focus_snippets 中真实存在的连续精确子串；不要省略号；不要虚构。
            2) 锚点尽量长且唯一（建议 30-160 字符），避免误匹配；优先用 focus_snippets 里的片段做锚点（命中率更高）。
            3) 优先局部 replace/insert；新增内容尽量 insert_after/append；谨慎 delete_between。
            4) 尽量保持原结构与既有功能不丢失；除非 user_message 明确要求删除/移除。
            5) 如果 baseline 是节选、信息不足：补丁要更保守，宁可只加小段落/小函数；不要猜测不存在的代码。
            6) 如果输入 payload.feedback 提示 skipped/no_patch_applied/occurrence_not_found 等失败信息：必须优先修复这些失败项（换更长更唯一的锚点/调整 occurrence/拆小补丁）。
            7) patches 数量要克制：遵守 payload.max_patches；优先输出 1-6 条命中率高的补丁，避免一次输出太多导致部分跳过。
            8) 如果 payload.feedback.strict_anchor_mode == true：你必须只从 focus_snippets 或 anchor_candidates 中选取 find/start/end/anchor（原样复制），并尽量让每条补丁只改一个小点，以最大化命中率。
            9) 如果 payload.feedback.patch_items[*].closest_hint.suggested_anchor 存在：优先直接使用 suggested_anchor 作为新的锚点（它来自当前全文的最接近片段），并把锚点加长以保证唯一。
            """,
            description="System prompt for Artifact Edit Mode patch generator (outputs JSON patches).",
        )

        CODE_STITCH_REVIEW_SYSTEM_PROMPT: str = Field(
            default=(
                "你是“超长代码拼接复查器/补丁生成器”。\n"
                "你将收到一份【单文件代码】文本（可能是头/中/尾摘要，摘要里可能包含 [DR_EXCERPT_CUT] 边界标记；注意这些标记不在原始代码中，禁止用作补丁锚点）。它可能因为多段续写拼接而出现：重复片段、多个 ``` 代码块、残留的“第1/7段/继续”提示、字符串被断行、缺失闭合标签/括号等问题。\n\n"
                "任务：只输出一组可自动应用的补丁操作 patches 来修复代码，使其成为可直接运行的单文件。\n"
                "禁止重新生成整份代码；禁止输出修复后的完整代码；禁止输出除 JSON 以外的任何内容。\n\n"
                "输出必须是严格 JSON，结构：\n"
                "{\n"
                '  "patches": [\n'
                '    {"op":"replace","find":string,"replace":string,"occurrence":1|"all"},\n'
                '    {"op":"delete_between","start":string,"end":string,"include_end":true,"occurrence":1},\n'
                '    {"op":"insert_after","anchor":string,"insert":string,"occurrence":1},\n'
                '    {"op":"insert_before","anchor":string,"insert":string,"occurrence":1},\n'
                '    {"op":"append","insert":string},\n'
                '    {"op":"prepend","insert":string}\n'
                "  ],\n"
                '  "notes": string\n'
                "}\n\n"
                "规则：\n"
                "1) find/start/end/anchor 必须来自你看到的代码文本中的原样片段，并尽量足够长以保证唯一匹配。\n"
                "2) 只做最小必要修改，优先修复拼接导致的结构/语法问题；不要改动玩法/需求设定。\n"
                "3) 如果需要删除大段重复内容，优先使用 delete_between，通过 start/end 锚点精确定位。\n"
                "4) patches 要可被程序自动应用：不要使用正则，不要使用模糊描述。\n"
            )
        )

        COMPILE_SYSTEM_PROMPT: str = Field(
            default=(
                "你是 Deep Research 的“最终交付整合器”（主模型）。\n"
                "你会拿到：用户需求、研究计划、每个步骤的阶段性输出（可能不完整）、以及可能的图表占位符/附件信息。\n"
                "你的任务：产出一个**最终可交付物**，形式不固定——可以是完整代码/脚本/模板/表格/清单/报告/说明文档等；以最能解决用户问题为准。\n\n"
                "硬性要求：\n"
                "1) 只输出最终交付物本身：不要再向用户提问，不要解释你准备怎么做。\n"
                "2) 若用户明确要求交付形式（例如‘给我完整代码/给我报告/给我表格’），必须服从。若用户未限制，你要自行选择最合适的交付形式。\n"
                "3) 若交付物包含【代码】：必须可复制即用，默认给出完整文件而不是零散片段。\n"
                "   - 单文件需求：优先输出单文件完整内容。\n"
                "   - 多文件需求：用 ‘### FILE: 相对路径’ 分隔，每个文件用代码块包裹。\n"
                "   - 每个代码文件末尾必须包含一行注释作为结束标记：DR_EOF（按语言使用注释语法，例如 HTML 用 <!-- DR_EOF -->，JS 用 // DR_EOF，Python 用 # DR_EOF）。\n"
                "   - 允许在代码前给最多 10 行的运行/使用说明，但不要写冗长报告。\n"
                "4) 若提供了图表：不要把工具返回的 markers 原样输出；只在正文合适位置放占位符 [[CHART_1]] / [[CHART_2]] ...（每个仅一次、不要放进代码块），pipe 会自动替换为真实 Markdown。\n"
                "5) 覆盖用户核心需求与关键约束；不必为了‘看起来很全面’而扩展无关章节。\n"
            )
        )

    def __init__(self):
        self.valves = self.Valves()

    def pipes(self):
        return [{"id": "loop", "name": "Deep Research Loop (v12.6.21.0)"}]

    async def _emit(self, __event_emitter__, text: str, done: bool = False):
        if not __event_emitter__:
            return
        try:
            await __event_emitter__(
                {
                    "type": "status",
                    "data": {"description": text, "done": done, "hidden": False},
                }
            )
        except Exception:
            pass

    # ----------------------------
    # Status formatting
    # ----------------------------
    async def _status(
        self, __event_emitter__, ui_mode: str, sid: str, text: str, done: bool = False
    ):
        if ui_mode == "debug":
            await self._emit(__event_emitter__, f"[DR#{sid[:6]}] {text}", done=done)
        else:
            await self._emit(__event_emitter__, f"Deep Research：{text}", done=done)

    # ----------------------------
    # Backward-compatible status emitter
    # ----------------------------
    async def _emit_status(self, __event_emitter__, *args, **kwargs):
        """Emit a status event.

        This file historically called `_emit_status` with multiple signatures.
        To avoid breaking older call sites, we accept flexible arguments and
        forward to `_emit`.

        Supported patterns:
          - await self._emit_status(emitter, ui_mode, text, done=False)
          - await self._emit_status(emitter, text, level="info", include_timing=False, ui_mode=ui_mode, done=False)
        """
        if not __event_emitter__:
            return

        ui_mode = str(kwargs.get("ui_mode") or "").strip().lower()
        done = bool(kwargs.get("done", False))

        text = ""
        if len(args) >= 2 and str(args[0]).strip().lower() in ("simple", "debug"):
            ui_mode = str(args[0]).strip().lower()
            text = str(args[1] if args[1] is not None else "")
            # Some older calls pass `done` as 3rd positional
            if len(args) >= 3 and isinstance(args[2], bool):
                done = bool(args[2])
        elif len(args) >= 1:
            text = str(args[0] if args[0] is not None else "")

        if not ui_mode:
            ui_mode = "simple"

        t = (text or "").strip()
        if not t:
            return

        # If caller already formatted the prefix, keep it.
        if ui_mode == "debug":
            if not t.startswith("[DR#") and not t.startswith("[DR]"):
                t = "[DR] " + t
        else:
            if not t.startswith("Deep Research："):
                t = "Deep Research：" + t

        await self._emit(__event_emitter__, t, done=done)

    # ----------------------------
    # Status heartbeat (every N seconds)
    # ----------------------------
    def _install_status_heartbeat(
        self,
        __event_emitter__,
        sess: Dict[str, Any],
        *,
        ui_mode: str,
        sid: str,
        interval_sec: int = 120,
    ):
        """Install a heartbeat that re-emits the current status every `interval_sec` seconds
        when a single step runs long (i.e., no new status arrives).

        Returns: (wrapped_event_emitter, heartbeat_task_or_none)
        """
        if not __event_emitter__:
            return __event_emitter__, None
        try:
            interval_sec = int(interval_sec)
        except Exception:
            interval_sec = 120
        if interval_sec <= 0:
            return __event_emitter__, None

        orig_emitter = __event_emitter__

        def _is_heartbeat_desc(desc: str) -> bool:
            # Avoid letting heartbeat messages overwrite the "current status"
            return ("⏳ 仍在运行：" in desc) or ("⏳ Still running:" in desc)

        async def wrapped(event: Dict[str, Any]):
            try:
                if isinstance(event, dict) and event.get("type") == "status":
                    data = event.get("data") or {}
                    desc = data.get("description")
                    if isinstance(desc, str):
                        d = desc.strip()
                        if d and (not _is_heartbeat_desc(d)):
                            now = _now()
                            sess["hb_last_real_status_at"] = now
                            sess["hb_last_real_status_text"] = d
                            if d != sess.get("hb_current_status"):
                                sess["hb_current_status"] = d
                                sess["hb_current_status_started_at"] = now
            except Exception:
                pass

            try:
                return await orig_emitter(event)
            except Exception:
                return

        async def heartbeat_loop():
            try:
                now = _now()
                sess.setdefault("hb_last_real_status_at", now)
                sess.setdefault("hb_last_heartbeat_at", 0.0)
                sess.setdefault("hb_current_status", "")
                sess.setdefault("hb_current_status_started_at", now)

                tick = 5  # check frequently; emit only every interval_sec
                while True:
                    await asyncio.sleep(tick)
                    now = _now()

                    try:
                        last_real = float(sess.get("hb_last_real_status_at") or now)
                    except Exception:
                        last_real = now
                    try:
                        last_hb = float(sess.get("hb_last_heartbeat_at") or 0.0)
                    except Exception:
                        last_hb = 0.0

                    base = last_real if last_real > last_hb else last_hb
                    if now - base < float(interval_sec):
                        continue

                    status_raw = str(
                        sess.get("hb_current_status")
                        or sess.get("hb_last_real_status_text")
                        or "处理中"
                    ).strip()
                    # strip known prefixes so we don't duplicate
                    try:
                        status_raw = re.sub(r"^Deep Research：", "", status_raw)
                        status_raw = re.sub(r"^\[DR#[0-9a-fA-F]+\]\s*", "", status_raw)
                        status_raw = re.sub(r"^\[DR\]\s*", "", status_raw)
                    except Exception:
                        pass

                    try:
                        started_at = float(
                            sess.get("hb_current_status_started_at") or last_real
                        )
                    except Exception:
                        started_at = last_real

                    elapsed = int(max(0.0, now - started_at))
                    mins = elapsed // 60
                    secs = elapsed % 60

                    msg = f"⏳ 仍在运行：{self._shorten(status_raw, 120)}（已持续 {mins}分{secs:02d}秒）"
                    try:
                        await self._status(wrapped, ui_mode, sid, msg, done=False)
                    except Exception:
                        pass

                    sess["hb_last_heartbeat_at"] = _now()

            except asyncio.CancelledError:
                return
            except Exception:
                return

        try:
            task = asyncio.create_task(heartbeat_loop())
        except Exception:
            task = None

        return wrapped, task

    def _shorten(self, s: str, max_len: int) -> str:
        s = (s or "").strip().replace("\n", " ")
        s = re.sub(r"\s+", " ", s).strip()
        if max_len > 0 and len(s) > max_len:
            return s[: max_len - 1] + "…"
        return s

    def _make_alignment_status(self, scoped: Dict[str, Any], locked_target: str) -> str:
        target = (locked_target or str(scoped.get("target") or "")).strip()
        goal = str(scoped.get("goal") or "").strip() or "深度研究"
        assumptions = (
            scoped.get("assumptions")
            if isinstance(scoped.get("assumptions"), list)
            else []
        )
        focus = scoped.get("focus") if isinstance(scoped.get("focus"), list) else []

        ass_s = "；".join([str(x).strip() for x in assumptions if str(x).strip()][:3])
        foc_s = "；".join([str(x).strip() for x in focus if str(x).strip()][:3])

        parts = []
        if target:
            parts.append(f"对象=【{target}】")
        if goal:
            parts.append(f"目标={goal}")
        if ass_s:
            parts.append(f"默认假设={ass_s}")
        if foc_s:
            parts.append(f"侧重点={foc_s}")

        msg = "需求对齐：" + ("；".join(parts) if parts else "已按常规范围推进")
        msg += "。如需调整请直接回复（例如“更关注短期/风险/财报/估值”）；未说明则按以上默认推进。"
        return self._shorten(msg, int(self.valves.ALIGNMENT_STATUS_MAX_CHARS))

    def _default_confirm_question(
        self, scoped: Dict[str, Any], locked_target: str
    ) -> str:
        target = (locked_target or str(scoped.get("target") or "")).strip()
        goal = str(scoped.get("goal") or "").strip()
        if not goal:
            goal = "做一次深度研究并给出结构化结论"
        if target:
            return f"我理解你想对【{target}】{goal}。如果没问题回复“继续/接着/ok”开始；如果要改研究重点/周期/风险偏好，请直接说。"
        return (
            "我需要确认研究对象：你希望深度研究的标的是哪一个（公司/股票名称或代码）？"
        )

    def _is_generic_confirm_question(self, cq: str) -> bool:
        t = (cq or "").strip()
        if not t:
            return True
        generic_patterns = [
            r"是否继续",
            r"要不要继续",
            r"继续吗",
            r"是否开始",
            r"要不要开始",
            r"开始吗",
            r"是否进行",
            r"要不要进行",
            r"确认吗",
            r"是否确认",
            r"你确认",
            r"请确认是否",
            r"是否要继续",
            r"是否要开始",
            r"是否继续进行",
        ]
        for p in generic_patterns:
            try:
                if re.search(p, t):
                    return True
            except Exception:
                continue

        if (
            len(t) <= 14
            and ("继续" in t or "开始" in t)
            and ("吗" in t or "？" in t or "?" in t)
        ):
            return True

        if "已暂停" in t and "确认" in t:
            return True

        return False

    def _compose_confirm_message(
        self, scoped: Dict[str, Any], locked_target: str, extra_question: str = ""
    ) -> str:
        target = (locked_target or str(scoped.get("target") or "")).strip()
        goal = str(scoped.get("goal") or "").strip()

        assumptions = scoped.get("assumptions") if isinstance(scoped, dict) else None
        if not isinstance(assumptions, list):
            assumptions = []
        assumptions = [str(x).strip() for x in assumptions if str(x).strip()]

        focus = scoped.get("focus") if isinstance(scoped, dict) else None
        if not isinstance(focus, list):
            focus = []
        focus = [str(x).strip() for x in focus if str(x).strip()]

        if not target:
            return "我还缺一个关键信息：你想研究的标的是哪一个？（公司/股票名称或代码即可）"

        lines: List[str] = []
        lines.append("我先把方向对齐一下，然后就开始深度研究（会用工具检索/归纳）：")
        lines.append(f"- 研究对象：**{target}**（已锁定，不会换成别的标的）")
        if goal:
            lines.append(f"- 你想解决：{goal}")

        if assumptions:
            ass = "；".join(assumptions[:4])
            if len(assumptions) > 4:
                ass += "；…"
            lines.append(f"- 默认：{ass}")

        if focus:
            foc = "、".join(focus[:5])
            if len(focus) > 5:
                foc += "…"
            lines.append(f"- 重点：{foc}")

        eq = (extra_question or "").strip()
        if eq and (not self._is_generic_confirm_question(eq)):
            lines.append("")
            lines.append(f"可选小问题（不答也行，我会按默认推进）：{eq}")

        lines.append("")
        lines.append("✅ 没问题的话：回复 **继续** 我就开始检索与输出。")
        lines.append(
            "🛠️ 想改重点/周期/风险/输出格式：直接一句话说“更关注…/偏短期…/更保守…/先给结论再给依据…”。"
        )
        lines.append("🛑 想先不做：回复 **取消** 。")

        return "\n".join(lines).strip()

    # ----------------------------
    # LLM call wrapper (runs other filters)
    # ----------------------------
    async def _call_llm(
        self,
        __request__,
        user_obj,
        model_id: str,
        messages: List[dict],
        metadata: Dict[str, Any],
        forward_opts: Dict[str, Any],
        temperature: Optional[float] = None,
        tool_choice_override: Optional[Any] = None,
        tools_override: Optional[Any] = None,
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {
            "model": model_id,
            "messages": messages,
            "stream": False,
            "metadata": metadata or {},
        }
        if forward_opts:
            body.update(deepcopy(forward_opts))
        if temperature is not None:
            body["temperature"] = temperature
        if tool_choice_override is not None:
            body["tool_choice"] = tool_choice_override
        if tools_override is not None:
            body["tools"] = tools_override

        res = await generate_chat_completion(
            __request__, body, user_obj, bypass_filter=False
        )
        return _normalize_gcc_response(res)

    def _build_internal_metadata(
        self,
        metadata_in: Dict[str, Any],
        base_model: str,
        phase: str,
        round_idx: int,
        sid: str,
    ) -> Dict[str, Any]:
        md = deepcopy(metadata_in or {})
        md[self.valves.INTERNAL_SKIP_FLAG_KEY] = True
        dr = md.get("deep_research")
        if not isinstance(dr, dict):
            dr = {}
        dr = dict(dr)
        dr["_internal"] = True
        dr["base_model"] = base_model
        dr["phase"] = phase
        dr["round"] = round_idx
        dr["session_id"] = sid
        md["deep_research"] = dr
        return md

    def _extract_forward_opts(
        self, outer_body: dict, tools_payload: Optional[List[dict]]
    ) -> Dict[str, Any]:
        keys = [
            "tools",
            "tool_choice",
            "parallel_tool_calls",
            "functions",
            "function_call",
            "response_format",
            "temperature",
            "top_p",
            "max_tokens",
            "presence_penalty",
            "frequency_penalty",
            "stop",
            "seed",
            "n",
            "stream_options",
            "logprobs",
            "top_logprobs",
        ]
        out: Dict[str, Any] = {}
        for k in keys:
            if k in outer_body and outer_body[k] is not None:
                out[k] = deepcopy(outer_body[k])

        # Prefer our tools list derived from __tools__ so we can execute them
        if tools_payload is not None:
            out["tools"] = deepcopy(tools_payload)
            out.pop("functions", None)

        if (
            self.valves.DEFAULT_TOOL_CHOICE_AUTO
            and out.get("tools")
            and "tool_choice" not in out
        ):
            out["tool_choice"] = "auto"
        if self.valves.FORCE_PARALLEL_TOOL_CALLS_FALSE and out.get("tools"):
            out["parallel_tool_calls"] = False

        return out

    # ----------------------------
    # Scope / Plan / Judge / Decide
    # ----------------------------
    async def _scope(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        base_model: str,
        metadata: Dict[str, Any],
        user_text: str,
        locked_target: str,
        assumption_mode: bool,
        conversation_context: str = "",
        context_pack: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        await self._status(
            __event_emitter__, ui_mode, sid, "🧭 理解需求与范围…", done=False
        )

        hint = ""
        if locked_target:
            hint += f"\n【已锁定研究对象】{locked_target}\n（除非用户明确要求更换，否则不要更换。）\n"
        if assumption_mode:
            hint += "\n【用户偏好】用户表示“随便/都可以/你决定”，可使用默认假设推进。\n"

        ctx_block = ""
        if conversation_context:
            ctx_block = (
                "【对话上下文】\n" + (conversation_context or "").strip() + "\n\n"
            )

        msgs = [
            {"role": "system", "content": self.valves.SCOPE_SYSTEM_PROMPT},
            {
                "role": "user",
                "content": ctx_block
                + "【当前用户需求】\n"
                + (user_text or "")
                + "\n"
                + hint
                + "\n只输出 JSON。",
            },
        ]
        internal_md = self._build_internal_metadata(
            metadata, base_model, "scope", 0, sid
        )
        res = await self._call_llm(
            __request__,
            user_obj,
            base_model,
            msgs,
            internal_md,
            forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
            temperature=0.2,
            tool_choice_override="none",
            tools_override=[],
        )
        obj = _extract_json_object(_extract_text(res)) or {}
        return obj if isinstance(obj, dict) else {}

    async def _plan(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        planner_model: str,
        metadata: Dict[str, Any],
        scoped: Dict[str, Any],
        time_context: str = "",
    ) -> Dict[str, Any]:
        await self._status(
            __event_emitter__, ui_mode, sid, "🗺️ 规划研究步骤…", done=False
        )
        plan_input = {
            "target": str(scoped.get("target") or ""),
            "goal": str(scoped.get("goal") or ""),
            "focus": (
                scoped.get("focus") if isinstance(scoped.get("focus"), list) else []
            ),
            "assumptions": (
                scoped.get("assumptions")
                if isinstance(scoped.get("assumptions"), list)
                else []
            ),
            "time_context": (time_context or "").strip(),
        }
        msgs = [
            {"role": "system", "content": self.valves.PLANNER_SYSTEM_PROMPT},
        ]
        if time_context:
            msgs.append({"role": "system", "content": time_context.strip()})
        msgs.append(
            {"role": "user", "content": json.dumps(plan_input, ensure_ascii=False)}
        )

        internal_md = self._build_internal_metadata(
            metadata, planner_model, "plan", 0, sid
        )
        res = await self._call_llm(
            __request__,
            user_obj,
            planner_model,
            msgs,
            internal_md,
            forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
            temperature=0.2,
            tool_choice_override="none",
            tools_override=[],
        )
        obj = _extract_json_object(_extract_text(res)) or {}
        if (
            not isinstance(obj, dict)
            or not isinstance(obj.get("plan"), list)
            or not obj.get("plan")
        ):
            obj = {
                "plan": [
                    {
                        "step": 1,
                        "title": "基本信息与商业模式",
                        "goal": "厘清做什么、赚什么钱",
                        "done": "业务结构清晰",
                    },
                    {
                        "step": 2,
                        "title": "关键数据与事实收集",
                        "goal": "补齐公告/财务/行业信息",
                        "done": "关键事实齐备",
                    },
                    {
                        "step": 3,
                        "title": "风险与催化剂",
                        "goal": "识别主要风险与关键催化剂",
                        "done": "清单+观察指标",
                    },
                    {
                        "step": 4,
                        "title": "结论与策略",
                        "goal": "给出结论、区间与观察点",
                        "done": "可执行建议",
                    },
                ],
                "notes": "",
            }
        return obj

    async def _deliverable_spec(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        model_id: str,
        metadata: Dict[str, Any],
        user_text: str,
        scoped: Dict[str, Any],
        current_plan: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Internal: decide what the final deliverable should look like."""
        await self._status(
            __event_emitter__, ui_mode, sid, "🧾 确定最终交付形态…", done=False
        )

        payload = {
            "user_request": (user_text or "").strip(),
            "target": str(scoped.get("target") or "").strip(),
            "goal": str(scoped.get("goal") or "").strip(),
            "plan": current_plan or {},
        }
        msgs = [
            {"role": "system", "content": self.valves.DELIVERABLE_SYSTEM_PROMPT},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ]
        internal_md = self._build_internal_metadata(
            metadata, model_id, "deliverable", 0, sid
        )
        res = await self._call_llm(
            __request__,
            user_obj,
            model_id,
            msgs,
            internal_md,
            forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
            temperature=0.0,
            tool_choice_override="none",
            tools_override=[],
        )
        obj = _extract_json_object(_extract_text(res)) or {}
        if not isinstance(obj, dict):
            obj = {}

        def _bool(v: Any) -> bool:
            if isinstance(v, bool):
                return v
            if v is None:
                return False
            s = str(v).strip().lower()
            return s in {"1", "true", "yes", "y", "是", "需要", "必须"}

        deliverable_type = (
            str(obj.get("deliverable_type") or obj.get("type") or "").strip() or "auto"
        )

        # New schema (preferred) + legacy compatibility
        must_full_code = _bool(obj.get("must_include_full_code"))
        # Legacy aliases
        must_full_code = (
            must_full_code
            or _bool(obj.get("must_be_copy_paste"))
            or _bool(obj.get("must_copy_paste"))
        )

        must_copy_paste = (
            must_full_code
            or _bool(obj.get("must_be_copy_paste"))
            or _bool(obj.get("must_copy_paste"))
        )

        code_lang = str(
            obj.get("preferred_code_language")
            or obj.get("code_language")
            or obj.get("language")
            or ""
        ).strip()

        is_single_file = _bool(obj.get("is_single_file")) or _bool(
            obj.get("single_file")
        )

        need_eof = _bool(obj.get("need_eof_marker"))
        eof_marker = (
            str(obj.get("eof_marker") or "").strip()
            or self.valves.DEFAULT_CODE_EOF_MARKER
        )
        notes = str(
            obj.get("notes") or obj.get("brief") or obj.get("reason") or ""
        ).strip()

        build_workflow = (
            str(
                obj.get("build_workflow")
                or obj.get("workflow")
                or obj.get("build_mode")
                or ""
            )
            .strip()
            .lower()
        )
        if build_workflow in {"", "none", "null"}:
            build_workflow = "auto"
        if build_workflow not in {
            "auto",
            "direct",
            "patch",
            "staged_patch",
            "iterative_patch",
            "multi_patch",
        }:
            # tolerate Chinese labels
            if build_workflow in {"直接", "直出"}:
                build_workflow = "direct"
            elif build_workflow in {"补丁", "分段", "分块", "拼接"}:
                build_workflow = "patch"
            else:
                build_workflow = "auto"

        complexity = (
            str(obj.get("complexity") or obj.get("size") or obj.get("scale") or "")
            .strip()
            .lower()
        )
        if complexity in {"", "none", "null"}:
            complexity = "auto"
        if complexity not in {"auto", "simple", "standard", "complex"}:
            if complexity in {"low", "small", "tiny", "简单", "低"}:
                complexity = "simple"
            elif complexity in {"medium", "normal", "standard", "中", "普通", "标准"}:
                complexity = "standard"
            elif complexity in {"high", "large", "big", "复杂", "高", "大型"}:
                complexity = "complex"
            else:
                complexity = "auto"

        requirements_summary = str(
            obj.get("requirements_summary")
            or obj.get("req_summary")
            or obj.get("summary")
            or ""
        ).strip()
        must_have = (
            obj.get("must_have")
            or obj.get("must_have_features")
            or obj.get("sections")
            or []
        )
        if isinstance(must_have, str):
            must_have = [
                x.strip() for x in re.split(r"[\n,;，；]+", must_have) if x.strip()
            ]
        if not isinstance(must_have, list):
            must_have = []
        must_have = [str(x).strip() for x in must_have if str(x).strip()]

        # Fallback: if the model didn't decide, infer softly from user query.
        t = user_text
        if build_workflow == "auto":
            if deliverable_type.lower() in ("code", "mixed") or must_full_code:
                build_workflow = (
                    "patch"
                    if any(
                        k in t
                        for k in [
                            "复杂",
                            "完整",
                            "项目",
                            "大型",
                            "关卡",
                            "系统",
                            "小游戏",
                            "游戏",
                            "app",
                            "应用",
                            "单文件",
                        ]
                    )
                    else "direct"
                )
            else:
                build_workflow = (
                    "patch"
                    if any(
                        k in t
                        for k in [
                            "超长",
                            "长文",
                            "完整手册",
                            "全量",
                            "大量章节",
                            "百科",
                            "白皮书",
                        ]
                    )
                    else "direct"
                )

        if complexity == "auto":
            complexity = (
                "complex"
                if any(k in t for k in ["复杂", "大型", "全功能", "项目级", "完整版"])
                else "standard"
            )

        if not requirements_summary:
            requirements_summary = notes

        # Fallback heuristics (only when model gives empty/ambiguous signal)
        t = (user_text or "").lower()
        if deliverable_type == "auto":
            if any(
                k in t
                for k in [
                    "代码",
                    "code",
                    "html",
                    "javascript",
                    "js",
                    "python",
                    "脚本",
                    "程序",
                    "单文件",
                    "可运行",
                    "项目",
                    "源码",
                ]
            ):
                deliverable_type = "code"
                must_full_code = True
                must_copy_paste = True
                need_eof = True
            elif any(
                k in t for k in ["报告", "研报", "分析", "调研", "复盘", "策略", "总结"]
            ):
                deliverable_type = "report"

        if not is_single_file and any(
            k in t for k in ["单文件", "single file", "one file", "single-file"]
        ):
            is_single_file = True

        if ("code" in deliverable_type.lower()) and not must_full_code:
            # For code deliverable, default to full copy-paste code.
            must_full_code = True

        if (
            "code" in deliverable_type.lower() or must_full_code
        ) and not must_copy_paste:
            must_copy_paste = True

        if ("code" in deliverable_type.lower() or must_full_code) and not need_eof:
            need_eof = True

        if must_full_code and not eof_marker:
            eof_marker = self.valves.DEFAULT_CODE_EOF_MARKER

        return {
            "deliverable_type": deliverable_type,
            "must_include_full_code": bool(must_full_code),
            "must_be_copy_paste": bool(must_copy_paste),
            "code_language": code_lang,
            "is_single_file": bool(is_single_file),
            "need_eof_marker": bool(need_eof),
            "eof_marker": eof_marker,
            "build_workflow": build_workflow,
            "complexity": complexity,
            "requirements_summary": requirements_summary,
            "must_have": must_have,
            "notes": notes,
        }

    async def _classify_turn_route(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        model_id: str,
        metadata: Dict[str, Any],
        user_message: str,
        *,
        has_deliverable: bool,
        deliverable_type: str,
        pending_kind: str,
        pending_question: str,
        has_pending_chunks: bool,
        conversation_context: str = "",
        context_pack: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Model-based router for ambiguous turns.

        Used when:
          - Deep Research is enabled mid-chat and we may need to restore previous deliverable.
          - The session is paused but user is actually requesting to modify the previous deliverable.

        Returns keys:
          route: chunk_continue | answer_pause | code_modify | new_task
          should_restore_deliverable: bool
          confidence: 0..1
          why: str
        """
        await self._status(
            __event_emitter__, ui_mode, sid, "🧭 模型路由判定…", done=False
        )

        # Keep context short to avoid huge prompt growth.
        ctx = (conversation_context or "").strip()
        if len(ctx) > 9000:
            ctx = ctx[:4500] + "\n[DR_EXCERPT_CUT]\n" + ctx[-4500:]

        payload = {
            "user_message": (user_message or "").strip(),
            "has_deliverable": bool(has_deliverable),
            "deliverable_type": (deliverable_type or "").strip().lower(),
            "pending_kind": (pending_kind or "").strip(),
            "pending_question": (pending_question or "").strip(),
            "has_pending_chunks": bool(has_pending_chunks),
            "conversation_context_excerpt": ctx,
            "context_pack": context_pack or {},
        }

        msgs = [
            {"role": "system", "content": self.valves.TURN_ROUTER_SYSTEM_PROMPT},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ]

        internal_md = self._build_internal_metadata(
            metadata, model_id, "turn_router", 0, sid
        )
        res = await self._call_llm(
            __request__,
            user_obj,
            model_id,
            msgs,
            internal_md,
            forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
            temperature=0.0,
            tool_choice_override="none",
            tools_override=[],
        )

        obj = _extract_json_object(_extract_text(res)) or {}
        if not isinstance(obj, dict):
            obj = {}

        route = (
            str(obj.get("route") or obj.get("action") or obj.get("intent") or "")
            .strip()
            .lower()
        )
        # normalize
        if route in {"continue", "chunk", "chunk_continue", "next"}:
            route = "chunk_continue"
        elif route in {"pause", "answer", "answer_pause", "reply"}:
            route = "answer_pause"
        elif route in {"followup", "modify", "edit", "patch", "code_modify", "update"}:
            route = "code_modify"
        elif route in {"new", "new_task", "restart"}:
            route = "new_task"

        if route not in {"chunk_continue", "answer_pause", "code_modify", "new_task"}:
            # safe defaults
            if has_pending_chunks and _looks_like_continue(user_message):
                route = "chunk_continue"
            elif pending_kind and pending_question:
                route = "answer_pause"
            else:
                route = "code_modify" if has_deliverable else "new_task"

        def _bool(v: Any) -> bool:
            if isinstance(v, bool):
                return v
            if v is None:
                return False
            s = str(v).strip().lower()
            return s in {"1", "true", "yes", "y", "是", "需要", "要", "restore"}

        should_restore = _bool(
            obj.get("should_restore_deliverable")
            or obj.get("should_restore")
            or obj.get("restore")
        )

        try:
            confidence = float(obj.get("confidence") or 0.0)
        except Exception:
            confidence = 0.0
        confidence = max(0.0, min(1.0, confidence))

        why = str(obj.get("why") or obj.get("reason") or "").strip()

        return {
            "route": route,
            "should_restore_deliverable": bool(should_restore),
            "confidence": confidence,
            "why": why,
        }

    async def _classify_followup_intent(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        model_id: str,
        metadata: Dict[str, Any],
        user_message: str,
        prev_scoped: Optional[Dict[str, Any]] = None,
        prev_locked_target: str = "",
        prev_spec: Optional[Dict[str, Any]] = None,
        prev_deliverable: str = "",
        context_pack: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Decide whether the user's follow-up is a patch, a supplement, or a new task.

        This runs ONLY when we already have a completed deliverable in the session.
        """
        await self._status(
            __event_emitter__, ui_mode, sid, "🧭 识别追问意图…", done=False
        )

        prev_scoped = prev_scoped or {}
        prev_spec = prev_spec or {}

        prev_deliverable = prev_deliverable or ""
        head = prev_deliverable[:1200]
        tail = prev_deliverable[-1200:] if len(prev_deliverable) > 1200 else ""

        payload = {
            "prev_target": (
                prev_locked_target or str(prev_scoped.get("target") or "")
            ).strip(),
            "prev_goal": str(prev_scoped.get("goal") or "").strip(),
            "prev_deliverable_type": str(
                prev_spec.get("deliverable_type") or ""
            ).strip(),
            "prev_code_language": str(prev_spec.get("code_language") or "").strip(),
            "prev_requirements_summary": str(
                prev_spec.get("requirements_summary") or ""
            ).strip(),
            "prev_must_have": prev_spec.get("must_have") or [],
            "prev_build_workflow": str(prev_spec.get("build_workflow") or "").strip(),
            "prev_deliverable_len": len(prev_deliverable),
            "prev_deliverable_head": head,
            "prev_deliverable_tail": tail,
            "context_pack": context_pack or {},
            "new_user_message": (user_message or "").strip(),
        }

        msgs = [
            {"role": "system", "content": self.valves.FOLLOWUP_INTENT_SYSTEM_PROMPT},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ]
        internal_md = self._build_internal_metadata(
            metadata, model_id, "followup_intent", 0, sid
        )
        res = await self._call_llm(
            __request__,
            user_obj,
            model_id,
            msgs,
            internal_md,
            forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
            temperature=0.0,
            tool_choice_override="none",
            tools_override=[],
        )
        obj = _extract_json_object(_extract_text(res)) or {}
        if not isinstance(obj, dict):
            obj = {}

        intent = str(obj.get("intent") or obj.get("action") or "").strip().lower()
        # normalize intent synonyms
        if intent in {"patch", "modify", "edit", "fix", "update", "incremental"}:
            intent = "patch"
        elif intent in {
            "regen",
            "regenerate",
            "rewrite",
            "recreate",
            "rebuild",
            "from_scratch",
            "refactor_large",
        }:
            intent = "regenerate"
        elif intent in {
            "supplement",
            "append",
            "addendum",
            "extra",
            "doc",
            "docs",
            "explain",
        }:
            intent = "supplement"
        elif intent in {
            "new",
            "new_task",
            "fresh",
            "restart",
            "different",
            "unrelated",
        }:
            intent = "new_task"

        if intent not in {"patch", "regenerate", "supplement", "new_task"}:
            # Safer default: if user follows up without clearly asking for a brand-new object,
            # assume it's an update to the existing deliverable.
            intent = "patch"

        delta_scope = str(obj.get("delta_scope") or "").strip().lower()
        if delta_scope not in {"tiny", "small", "medium", "large"}:
            # reasonable default
            delta_scope = (
                "small"
                if intent in {"patch", "supplement"}
                else ("medium" if intent == "regenerate" else "large")
            )

        pipeline = str(obj.get("pipeline") or "").strip().lower()
        if pipeline not in {"fast", "full"}:
            pipeline = "full" if intent == "new_task" else "fast"

        try:
            confidence = float(obj.get("confidence") or 0.0)
        except Exception:
            confidence = 0.0
        confidence = max(0.0, min(1.0, confidence))

        why = str(
            obj.get("why") or obj.get("reason") or obj.get("rationale") or ""
        ).strip()
        notes = str(obj.get("notes") or "").strip()

        carry_over = obj.get("carry_over")
        if not isinstance(carry_over, dict):
            carry_over = {}

        context_injection = str(obj.get("context_injection") or "").strip().lower()
        if context_injection not in ("full", "excerpt", "summary"):
            # defaults
            if intent in {"patch", "regenerate"}:
                context_injection = "full"
            elif intent == "supplement":
                context_injection = "summary"
            else:
                context_injection = "summary"

        patch_focus = obj.get("patch_focus") or obj.get("focus") or []
        if isinstance(patch_focus, str):
            patch_focus = [patch_focus]
        if not isinstance(patch_focus, list):
            patch_focus = []
        patch_focus = [str(x).strip() for x in patch_focus if str(x).strip()]

        def _bool(v: Any) -> bool:
            if isinstance(v, bool):
                return v
            if v is None:
                return False
            s = str(v).strip().lower()
            return s in {"1", "true", "yes", "y", "是", "需要", "保持"}

        return {
            "intent": intent,
            "delta_scope": delta_scope,
            "pipeline": pipeline,
            "confidence": confidence,
            "why": why,
            "notes": notes,
            "context_injection": context_injection,
            "patch_focus": patch_focus,
            "carry_over": {
                "target": _bool(carry_over.get("target")),
                "deliverable_type": _bool(carry_over.get("deliverable_type")),
            },
        }

    def _infer_followup_spec(
        self, prev_spec: Optional[Dict[str, Any]], prev_deliverable: str
    ) -> Dict[str, Any]:
        """Best-effort spec inference when prev_spec is missing/corrupt."""
        if isinstance(prev_spec, dict) and prev_spec.get("deliverable_type"):
            # Shallow copy to avoid accidental in-place mutations.
            return dict(prev_spec)

        text = (prev_deliverable or "").strip()
        blocks = _extract_fenced_code_blocks(text)
        code = ""
        lang = ""
        if blocks:
            best = max(blocks, key=lambda b: len(b.get("code", "") or ""))
            code = best.get("code", "") or ""
            lang = (best.get("lang") or "").strip().lower()
        else:
            code = text

        if not lang:
            low = code.lower()
            if "<!doctype html" in low or "<html" in low:
                lang = "html"
            elif "def " in code and "import " in code:
                lang = "python"
            elif "function " in code or "const " in code or "let " in code:
                # Could be JS/TS; default to javascript.
                lang = "javascript"

        # Try to infer EOF marker line from the content itself.
        eof_line = ""
        eof_marker = ""
        if "DR_EOF" in text:
            for line in reversed(text.splitlines()):
                if "DR_EOF" in line:
                    eof_line = line.strip()
                    eof_marker = eof_line
                    break
        else:
            if lang == "html":
                eof_marker = "<!-- DR_EOF -->"
                eof_line = eof_marker
            elif lang in {"python"}:
                eof_marker = "# DR_EOF"
                eof_line = eof_marker
            elif lang in {"javascript", "js", "typescript", "ts"}:
                eof_marker = "// DR_EOF"
                eof_line = eof_marker

        deliverable_type = "code" if lang else "report"
        return {
            "deliverable_type": deliverable_type,
            "code_language": lang,
            "need_eof_marker": bool(eof_marker),
            "eof_marker": eof_marker,
            "eof_marker_line": eof_line or eof_marker,
            "must_have": [],
            "requirements_summary": "",
            "complexity": "medium",
            "build_workflow": "patch",
        }

    def _select_followup_context(self, prev_deliverable: str, mode: str) -> str:
        """Select how much of the previous deliverable to inject into follow-up generation."""
        text = prev_deliverable or ""
        mode = (mode or "").strip().lower() or "full"

        full_max = int(getattr(self.valves, "FOLLOWUP_CONTEXT_FULL_MAX_CHARS", 60000))
        head_n = int(getattr(self.valves, "FOLLOWUP_CONTEXT_HEAD_CHARS", 12000))
        tail_n = int(getattr(self.valves, "FOLLOWUP_CONTEXT_TAIL_CHARS", 12000))

        if mode == "full":
            if len(text) <= full_max:
                return text
            # Safety fallback
            mode = "excerpt"

        if mode == "summary":
            # Cheap "summary": smaller excerpt. (We avoid extra LLM call here.)
            head_n = max(2000, min(head_n, 6000))
            tail_n = max(2000, min(tail_n, 6000))

        if len(text) <= head_n + tail_n + 500:
            return text

        head = text[:head_n].rstrip()
        tail = text[-tail_n:].lstrip()
        return head + "\n\n... [TRUNCATED] ...\n\n" + tail

    async def _handle_followup_fast(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        base_model: str,
        metadata: Dict[str, Any],
        intent_obj: Dict[str, Any],
        user_message: str,
        *,
        prev_target: str,
        prev_spec: Optional[Dict[str, Any]],
        prev_deliverable: str,
        prev_scoped: Optional[Dict[str, Any]] = None,
        max_output_continue_turns: int = 0,
    ) -> Dict[str, Any]:
        """Fast follow-up path: patch/regenerate/supplement using previous deliverable."""

        intent = str(intent_obj.get("intent") or "patch").strip().lower()
        context_mode = (
            str(
                intent_obj.get("context_injection")
                or intent_obj.get("context_mode")
                or "full"
            )
            .strip()
            .lower()
        )

        # Resolve per-chat config (optional)
        dr = _get_dr_dict(metadata)
        cfg = dr.get("config") if isinstance(dr.get("config"), dict) else {}
        prefer_full_ctx_for_code = _as_bool(
            cfg.get("followup_force_full_context", True), True
        )
        anti_regression = _as_bool(cfg.get("followup_anti_regression", True), True)
        min_ratio = 0.8
        try:
            mr = float(cfg.get("followup_anti_regression_min_ratio", 0.8))
            if 0.3 <= mr <= 1.0:
                min_ratio = mr
        except Exception:
            pass
        min_anchor_hits = 3
        try:
            mah = int(cfg.get("followup_anti_regression_min_anchor_hits", 3))
            if 0 <= mah <= 10:
                min_anchor_hits = mah
        except Exception:
            pass

        spec = self._infer_followup_spec(prev_spec, prev_deliverable)
        deliverable_type = (
            str(spec.get("deliverable_type") or "").strip().lower() or "report"
        )
        is_code = deliverable_type == "code"
        code_lang = str(spec.get("code_language") or "").strip().lower()

        if is_code and prefer_full_ctx_for_code:
            context_mode = "full"

        prev_context = self._select_followup_context(prev_deliverable, context_mode)

        # Build anchors from the baseline deliverable (for anti-regression guard)
        def _build_anchors(baseline: str) -> list:
            if not baseline:
                return []
            anchors = []
            try:
                m = re.search(
                    r"<title>\s*([^<]{2,120})\s*</title>", baseline, flags=re.I
                )
                if m:
                    t = m.group(1).strip()
                    if t and t not in anchors:
                        anchors.append(t)
            except Exception:
                pass
            for a in [
                "水晶守护者",
                "gameCanvas",
                "loadAssets",
                "dialogueData",
                "class Player",
                "class Enemy",
                "class Crystal",
                "class Platform",
                "class Door",
            ]:
                if a in baseline and a not in anchors:
                    anchors.append(a)
            return anchors[:8]

        baseline_code_lang = code_lang or ""
        baseline_code = prev_deliverable or ""
        # Extract raw code for length comparison
        try:
            bl_lang, bl_code = _extract_code_candidate(
                baseline_code, prefer_lang=baseline_code_lang or "html"
            )
            baseline_code_norm = _normalize_code_candidate(
                bl_code, bl_lang or baseline_code_lang or "text"
            )
        except Exception:
            baseline_code_norm = baseline_code

        anchors = _build_anchors(
            baseline_code_norm if baseline_code_norm else baseline_code
        )

        # Compose payload
        payload = {
            "intent": intent,
            "prev_target": prev_target,
            "prev_deliverable_spec": prev_spec or spec,
            "prev_deliverable_context": prev_context,
            "prev_scoped": prev_scoped or {},
            "user_message": user_message,
        }

        # Pick system prompt
        if intent == "supplement":
            sys_prompt = FOLLOWUP_SUPPLEMENT_SYSTEM_PROMPT
        elif intent == "regenerate":
            sys_prompt = FOLLOWUP_REGENERATE_SYSTEM_PROMPT
        else:
            sys_prompt = FOLLOWUP_UPDATE_SYSTEM_PROMPT

        # Add a hard constraint block for code to avoid accidental "new project" regeneration.
        hard_constraints = ""
        if is_code:
            hc_lines = [
                "【硬约束】你必须在 prev_deliverable_context 这份代码基础上做增量修改（打补丁），不得换题、不得输出另一份全新项目。",
                "- 默认保留原有玩法/结构/功能；除非用户明确要求删除，否则不要删减现有内容。",
            ]
            if anti_regression and baseline_code_norm:
                hc_lines.append(
                    f"- 输出长度不得小于原代码长度的 {int(min_ratio*100)}%（避免把内容越改越少）。"
                )
            if anchors:
                hc_lines.append(
                    "- 输出必须包含以下锚点字符串（原样出现，用于防止跑题/丢失上下文）："
                )
                for a in anchors:
                    hc_lines.append(f"  • {a}")
            hard_constraints = "\n".join(hc_lines)

        async def _gen_once(extra_hard: str = "", temperature: float = 0.15) -> str:
            sys = sys_prompt
            if extra_hard:
                sys = sys + "\n\n" + extra_hard
            elif hard_constraints:
                sys = sys + "\n\n" + hard_constraints

            msgs = [
                {"role": "system", "content": sys},
                {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
            ]

            internal_md = self._build_internal_metadata(
                metadata, base_model, "followup_fast", 0, sid
            )
            resp = await self._call_llm(
                __request__,
                user_obj,
                base_model,
                msgs,
                internal_md,
                forward_opts={},
                temperature=temperature,
                tool_choice_override="none",
                tools_override=[],
            )
            text0 = _extract_text_from_assistant_response(resp)

            # Auto-continue if truncated
            require_marker = None
            if is_code and spec.get("need_eof_marker"):
                require_marker = str(spec.get("eof_marker") or "DR_EOF")
            if max_output_continue_turns and int(max_output_continue_turns) > 0:
                max_turns = int(max_output_continue_turns)
            else:
                max_turns = int(
                    getattr(self.valves, "DEFAULT_MAX_OUTPUT_CONTINUE_TURNS", 6)
                )

            full_text, _ = await self._auto_continue(
                __request__,
                __event_emitter__,
                ui_mode,
                sid,
                user_obj,
                base_model,
                msgs,
                internal_md,
                forward_opts={},
                partial=text0,
                max_turns=max_turns,
                require_marker=require_marker,
            )
            return full_text

        def _passes_regression_guard(candidate_text: str) -> bool:
            if not (is_code and anti_regression and baseline_code_norm):
                return True
            try:
                c_lang, c_code = _extract_code_candidate(
                    candidate_text, prefer_lang=code_lang or "html"
                )
                cand_norm = _normalize_code_candidate(
                    c_code, c_lang or code_lang or "text"
                )
            except Exception:
                cand_norm = candidate_text or ""

            if not cand_norm:
                return False

            # Length ratio guard
            try:
                if len(baseline_code_norm) >= 2000:
                    if len(cand_norm) < int(len(baseline_code_norm) * min_ratio):
                        return False
            except Exception:
                pass

            # Anchor guard
            if anchors and min_anchor_hits > 0:
                hit = 0
                for a in anchors:
                    if a and a in cand_norm:
                        hit += 1
                if hit < min_anchor_hits:
                    return False

            return True

        # Generate
        if intent == "supplement":
            text = await _gen_once(temperature=0.2)
            return {"answer": text, "update_deliverable": False}

        # patch / regenerate
        if __event_emitter__ and ui_mode == "debug":
            await self._status(
                __event_emitter__,
                ui_mode,
                sid,
                f"⚡ follow-up fast: intent={intent}, context={context_mode}, anti_regression={anti_regression}",
                done=False,
            )

        text = await _gen_once(temperature=0.15)

        if not _passes_regression_guard(text):
            if __event_emitter__:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    "🧯 检测到输出疑似跑题/内容缩水：将自动再生成一次（强约束：必须在原代码上打补丁）",
                    done=False,
                )
            extra = "\n".join(
                [
                    "【二次生成-强约束】上一次输出不合格：你没有在原代码基础上做增量修改，或丢失了大量原内容。",
                    "你必须：1）从 prev_deliverable_context 复制并修改，而不是重写；2）保留原有结构与玩法；3）补齐缺失部分；4）仍然输出完整可运行的单文件。",
                ]
            )
            if anchors:
                extra += "\n必须保留锚点：\n" + "\n".join([f"- {a}" for a in anchors])
            text = await _gen_once(extra_hard=extra, temperature=0.05)

        # Finalize code formatting if needed
        final_text = text
        if is_code:
            final_text = await self._finalize_code_deliverable(
                __request__,
                user_obj,
                base_model,
                metadata,
                prev_scoped or {},
                __event_emitter__,
                ui_mode,
                sid,
                spec,
                final_text,
            )

            # Final audit (optional)
            final_text = await self._final_audit_deliverable(
                __request__,
                user_obj,
                base_model,
                metadata,
                prev_scoped or {},
                __event_emitter__,
                ui_mode,
                sid,
                spec,
                final_text,
            )

        return {
            "answer": final_text,
            "update_deliverable": True,
            "deliverable": final_text,
            "deliverable_spec": spec,
            "followup_intent": intent_obj,
        }

    # ----------------------------
    # Artifact Edit Mode (staged patch apply; avoid full rewrite)
    # ----------------------------
    def _extract_anchor_candidates(self, baseline: str, max_n: int = 12) -> List[str]:
        """Pick distinctive lines/snippets from baseline as anchors for anti-regression guards."""
        if not baseline:
            return []
        try:
            max_n = max(0, int(max_n or 0))
        except Exception:
            max_n = 0
        if max_n <= 0:
            return []
        lines = baseline.splitlines()
        freq: Dict[str, int] = {}
        for line in lines:
            s = (line or "").strip()
            if 18 <= len(s) <= 180:
                freq[s] = freq.get(s, 0) + 1

        def _score(s: str) -> float:
            low = s.lower()
            sc = 0.0
            if low.startswith("###") or low.startswith("##") or low.startswith("#"):
                sc += 3.0
            if "class " in low:
                sc += 3.0
            if "function " in low:
                sc += 3.0
            if low.startswith("<title") or "<title>" in low:
                sc += 2.5
            if "<canvas" in low or "gamecanvas" in low:
                sc += 2.0
            if "id=" in low:
                sc += 1.5
            if "const " in low or "let " in low or "var " in low:
                sc += 1.2
            if "=>" in s:
                sc += 0.4
            # Penalize generic/noisy lines
            if low in (
                "{",
                "}",
                ");",
                ")",
                "]",
                "[",
                "</script>",
                "</body>",
                "</html>",
            ):
                sc -= 5.0
            # Prefer moderate length
            L = len(s)
            sc += min(L, 120) / 120.0
            return sc

        scored: List[Tuple[float, str]] = []
        for s, n in freq.items():
            if n != 1:
                continue
            if s.startswith("```"):
                continue
            sc = _score(s)
            if sc <= 0:
                continue
            scored.append((sc, s))
        scored.sort(key=lambda x: x[0], reverse=True)
        out: List[str] = []
        for _, s in scored:
            if s not in out:
                out.append(s)
            if len(out) >= max_n:
                break
        # Fallback: try title text
        if not out:
            try:
                m = re.search(
                    r"<title>\s*([^<]{2,100})\s*</title>", baseline, flags=re.I
                )
                if m:
                    out.append(m.group(1).strip())
            except Exception:
                pass
        return out[:max_n]

    def _select_artifact_edit_context(self, baseline: str, mode: str) -> str:
        """选择用于“修改/补丁生成”的基线上下文片段。

        设计目标：
        - 小交付物：允许注入全文（便于补丁精确定位）
        - 大交付物：默认只注入“头+尾+剪裁标记”，其余靠 focus_snippets / anchor_candidates / outline 来定位
        - 超大交付物：进入“交付物分离”模式，强制字符预算，避免模型处理超长代码/长文时丢内容
        """
        if not baseline:
            return ""
        m = str(mode or "auto").strip().lower()
        if m not in ("auto", "full", "summary", "detached"):
            m = "auto"

        baseline_len = len(baseline)

        # --- Base thresholds ---
        try:
            max_full = int(
                getattr(self.valves, "ARTIFACT_EDIT_CONTEXT_FULL_MAX_CHARS", 180000)
                or 180000
            )
        except Exception:
            max_full = 180000
        try:
            head_n = int(
                getattr(self.valves, "ARTIFACT_EDIT_CONTEXT_HEAD_CHARS", 45000) or 45000
            )
        except Exception:
            head_n = 45000
        try:
            tail_n = int(
                getattr(self.valves, "ARTIFACT_EDIT_CONTEXT_TAIL_CHARS", 45000) or 45000
            )
        except Exception:
            tail_n = 45000

        # --- Detach mode (超大交付物分离) ---
        detach_enabled = bool(getattr(self.valves, "ARTIFACT_DETACH_ENABLED", True))
        try:
            detach_min = int(
                getattr(self.valves, "ARTIFACT_DETACH_MIN_CHARS", 60000) or 60000
            )
        except Exception:
            detach_min = 60000
        try:
            budget = int(
                getattr(self.valves, "ARTIFACT_DETACH_CONTEXT_BUDGET_CHARS", 24000)
                or 24000
            )
        except Exception:
            budget = 24000
        try:
            d_head = int(
                getattr(self.valves, "ARTIFACT_DETACH_HEAD_CHARS", 8000) or 8000
            )
        except Exception:
            d_head = 8000
        try:
            d_tail = int(
                getattr(self.valves, "ARTIFACT_DETACH_TAIL_CHARS", 8000) or 8000
            )
        except Exception:
            d_tail = 8000

        # If artifact is huge, force detached mode (even if caller passed 'full')
        if detach_enabled and baseline_len >= detach_min:
            m = "detached"

        # Small artifact: allow full injection (unless we are forced into detached)
        if (
            m in ("auto", "full")
            and baseline_len <= max_full
            and not (detach_enabled and baseline_len >= detach_min)
        ):
            return baseline

        # For summary/detached, prefer smaller head/tail excerpts
        if m in ("summary", "detached"):
            head_n = min(head_n, d_head)
            tail_n = min(tail_n, d_tail)

        head = baseline[: max(0, head_n)]
        tail = baseline[-max(0, tail_n) :] if baseline_len > tail_n else ""

        if head and tail and baseline_len > (head_n + tail_n):
            ctx = head + "\n\n[DR_EXCERPT_CUT]\n\n" + tail
        else:
            # Fallback: keep only the beginning part
            ctx = baseline[: min(baseline_len, max_full)]

        # Hard budget for detached mode: keep prompt small and stable
        if (
            m == "detached"
            and isinstance(budget, int)
            and budget > 0
            and len(ctx) > budget
        ):
            marker = "\n\n[DR_EXCERPT_CUT]\n\n"
            keep_each = max(1200, (budget - len(marker)) // 2)
            h = baseline[:keep_each]
            t = baseline[-keep_each:] if baseline_len > keep_each else ""
            if h and t and baseline_len > (keep_each * 2):
                ctx = h + marker + t
            else:
                ctx = baseline[:budget]

        return ctx

    def _guess_artifact_kind_for_detach(
        self, text: str, deliverable_type: str = "", code_lang: str = ""
    ) -> str:
        """粗略判断交付物类型（用于智能分离判定的特征输入）。"""
        dt = str(deliverable_type or "").strip().lower()
        if dt == "code":
            return "code"
        lang = str(code_lang or "").strip().lower()
        if lang in (
            "html",
            "js",
            "javascript",
            "ts",
            "typescript",
            "python",
            "py",
            "java",
            "c",
            "cpp",
            "c++",
            "go",
            "rust",
            "php",
            "ruby",
            "json",
            "yaml",
            "toml",
            "css",
        ):
            return "code"
        if not isinstance(text, str):
            return "text"
        t = text[:4000].lower()
        if (
            "<!doctype" in t
            or "<html" in t
            or "<script" in t
            or "function " in t
            or "class " in t
        ):
            return "code"
        return "text"

    async def _call_detach_decider_llm(
        self,
        __request__,
        user_obj,
        model_id: str,
        metadata: Dict[str, Any],
        base_model: str,
        sid: str,
        payload: Dict[str, Any],
        max_tokens: int = 220,
    ) -> Optional[Dict[str, Any]]:
        """调用模型进行“是否进入分离模式”的判定。返回解析后的 JSON 或 None。"""
        try:
            sys_prompt = (
                "你是一个“上下文预算/分离判定器”。你的任务：判断当前交付物是否应该进入『分离模式』。\\n"
                "\\n"
                "分离模式含义：不要把完整交付物全文塞进模型上下文；只注入大纲/头尾节选/聚焦片段，然后用补丁去修改原交付物。\\n"
                "\\n"
                "输出要求：只输出严格 JSON（不要额外解释，不要 Markdown）。\\n"
                "JSON 格式：{\\n"
                '  "detach": true/false,\\n'
                '  "confidence": 0.0-1.0,\\n'
                '  "reason": "一句话原因"\\n'
                "}\\n"
                "\\n"
                "判定原则（倾向 detach=true）：\\n"
                "- 交付物很长，全文注入容易超上下文或导致模型丢内容/重写漂移；\\n"
                "- 交付物是代码（HTML/JS/CSS/脚本等），且需要修改/扩展，分离+补丁更稳；\\n"
                "- 用户需求需要多轮修改/审计，长上下文更容易出错。\\n"
                "\\n"
                "只有在交付物较短且全文注入明显更利于精确定位时，才 detach=false。"
            )
            msgs = [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
            ]
            internal_md = self._build_internal_metadata(
                metadata, base_model, "detach_decider", 0, sid
            )
            # 超时保护
            timeout_s = 20
            try:
                timeout_s = int(
                    getattr(self.valves, "ARTIFACT_DETACH_SMART_TIMEOUT_SECS", 20) or 20
                )
            except Exception:
                timeout_s = 20

            async def _do():
                return await self._call_llm(
                    __request__,
                    user_obj,
                    model_id,
                    msgs,
                    internal_md,
                    forward_opts={
                        "tool_choice": "none",
                        "parallel_tool_calls": False,
                        "max_tokens": int(max_tokens),
                    },
                    temperature=0.0,
                )

            res = await asyncio.wait_for(_do(), timeout=timeout_s)
            # normalized response
            content = ""
            try:
                content = (
                    ((res or {}).get("choices") or [{}])[0].get("message") or {}
                ).get("content") or ""
            except Exception:
                content = ""
            obj = _extract_json_object(content)
            if isinstance(obj, dict):
                return obj
            return None
        except Exception:
            return None

    async def _smart_adjust_context_mode_for_detach(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        base_model: str,
        metadata: Dict[str, Any],
        *,
        intent: str,
        deliverable_type: str,
        code_lang: str,
        work: str,
        context_mode: str,
        cfg: Dict[str, Any],
    ) -> str:
        """智能决定是否强制进入 detached（避免超长交付物全文注入导致丢内容）。"""
        try:
            cm = str(context_mode or "full").strip().lower()
        except Exception:
            cm = "full"

        # 已经明确要求 summary/detached 就不改
        if cm in ("detached", "summary"):
            return cm

        detach_enabled = _as_bool(
            (cfg.get("artifact_detach_enabled") if isinstance(cfg, dict) else None),
            bool(getattr(self.valves, "ARTIFACT_DETACH_ENABLED", True)),
        )
        if not detach_enabled:
            return cm

        # 读取策略
        mode = (
            str(
                (
                    cfg.get("artifact_detach_threshold_mode")
                    if isinstance(cfg, dict)
                    else None
                )
                or getattr(self.valves, "ARTIFACT_DETACH_THRESHOLD_MODE", "fixed")
                or "fixed"
            )
            .strip()
            .lower()
        )
        if mode not in ("fixed", "auto", "smart", "llm"):
            mode = "fixed"
        if mode == "llm":
            mode = "smart"

        baseline_len = len(work or "")
        # fixed 基线阈值
        detach_min = _as_int(
            (cfg.get("artifact_detach_min_chars") if isinstance(cfg, dict) else None),
            int(getattr(self.valves, "ARTIFACT_DETACH_MIN_CHARS", 60000) or 60000),
        )
        fixed_should_detach = bool(baseline_len >= int(detach_min))

        # 如果是 fixed 策略：直接返回（_select_artifact_edit_context 内部仍会按 MIN_CHARS 强制 detached）
        if mode == "fixed":
            return cm

        lower = _as_int(
            (
                cfg.get("artifact_detach_smart_lower_bound")
                if isinstance(cfg, dict)
                else None
            ),
            int(
                getattr(self.valves, "ARTIFACT_DETACH_SMART_LOWER_BOUND", 20000)
                or 20000
            ),
        )
        upper = _as_int(
            (
                cfg.get("artifact_detach_smart_upper_bound")
                if isinstance(cfg, dict)
                else None
            ),
            int(
                getattr(self.valves, "ARTIFACT_DETACH_SMART_UPPER_BOUND", 180000)
                or 180000
            ),
        )
        band = _as_int(
            (
                cfg.get("artifact_detach_smart_ambiguous_band")
                if isinstance(cfg, dict)
                else None
            ),
            int(
                getattr(self.valves, "ARTIFACT_DETACH_SMART_AMBIGUOUS_BAND", 15000)
                or 15000
            ),
        )
        snippet_chars = _as_int(
            (
                cfg.get("artifact_detach_smart_snippet_chars")
                if isinstance(cfg, dict)
                else None
            ),
            int(
                getattr(self.valves, "ARTIFACT_DETACH_SMART_SNIPPET_CHARS", 1200)
                or 1200
            ),
        )
        max_tokens = _as_int(
            (
                cfg.get("artifact_detach_smart_max_tokens")
                if isinstance(cfg, dict)
                else None
            ),
            int(getattr(self.valves, "ARTIFACT_DETACH_SMART_MAX_TOKENS", 220) or 220),
        )

        # 强规则：极小/极大直接判定
        if baseline_len <= max(0, lower):
            return cm
        if baseline_len >= max(upper, lower + 1000):
            # 强制分离
            if __event_emitter__ and ui_mode == "debug":
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"🧠 智能分离判定：len={baseline_len}>=upper({upper})，强制 detached",
                    done=False,
                )
            return "detached"

        # auto：只有在“临界区”才调用 LLM
        need_llm = mode == "smart"
        if mode == "auto":
            try:
                if abs(int(baseline_len) - int(detach_min)) <= int(band):
                    need_llm = True
            except Exception:
                need_llm = False

            # 代码交付物在接近阈值时更倾向调用模型
            if not need_llm and str(deliverable_type).lower() == "code":
                # 代码更容易漂移丢失：放宽触发条件
                try:
                    if baseline_len >= int(detach_min * 0.7):
                        need_llm = True
                except Exception:
                    pass

        if not need_llm:
            # 不调用模型时按 fixed 规则
            return "detached" if fixed_should_detach else cm

        # 选择判定模型
        det_model = str(
            (cfg.get("artifact_detach_smart_model") if isinstance(cfg, dict) else None)
            or getattr(self.valves, "ARTIFACT_DETACH_SMART_MODEL", "")
            or ""
        ).strip()
        judge_model = str(
            (cfg.get("judge_model") if isinstance(cfg, dict) else None) or ""
        ).strip()
        model_id = det_model or judge_model or base_model

        kind = self._guess_artifact_kind_for_detach(
            work or "", deliverable_type=deliverable_type, code_lang=code_lang
        )
        # 轻量特征
        head = (work or "")[: max(0, int(snippet_chars))]
        head_low = head.lower()
        feat = {
            "script_tags": head_low.count("<script"),
            "style_tags": head_low.count("<style"),
            "function_kw": head_low.count("function "),
            "class_kw": head_low.count("class "),
            "doctype": 1 if "<!doctype" in head_low else 0,
            "dr_eof": 1 if "dr_eof" in (work or "").lower() else 0,
        }
        payload = {
            "intent": intent,
            "deliverable_type": deliverable_type,
            "code_language": code_lang,
            "artifact_kind_guess": kind,
            "baseline_chars": baseline_len,
            "baseline_lines": (work.count("\n") + 1) if isinstance(work, str) else None,
            "requested_context_mode": cm,
            "fixed_threshold_min_chars": detach_min,
            "fixed_should_detach": fixed_should_detach,
            "features": feat,
            "excerpt_head": head,
        }

        obj = await self._call_detach_decider_llm(
            __request__,
            user_obj,
            model_id,
            metadata,
            base_model,
            sid,
            payload,
            max_tokens=max_tokens,
        )

        decided = None
        conf = None
        reason = ""
        if isinstance(obj, dict):
            decided = _as_bool(obj.get("detach"), fixed_should_detach)
            conf = obj.get("confidence")
            try:
                reason = str(obj.get("reason") or "").strip()
            except Exception:
                reason = ""

        # fallback
        if decided is None:
            decided = fixed_should_detach

        # 状态展示：debug 一定显示；simple 仅在与 fixed 不同才显示
        try:
            msg = f"🧠 智能分离判定：len={baseline_len}，fixed={'detached' if fixed_should_detach else 'full'}，decided={'detached' if decided else 'full'}"
            if conf is not None:
                msg += f"，conf={conf}"
            if reason:
                msg += f"；{reason}"
            if __event_emitter__ and (
                ui_mode == "debug" or bool(decided) != bool(fixed_should_detach)
            ):
                await self._status(__event_emitter__, ui_mode, sid, msg, done=False)
        except Exception:
            pass

        return "detached" if decided else cm

    def _extract_html_title(self, text: str) -> str:
        try:
            m = re.search(
                r"<title[^>]*>(.*?)</title>", text, flags=re.IGNORECASE | re.DOTALL
            )
            if not m:
                return ""
            title = re.sub(r"\s+", " ", m.group(1) or "").strip()
            return title[:120]
        except Exception:
            return ""

    def _compute_artifact_meta(
        self, deliverable: str, spec: Optional[dict]
    ) -> Dict[str, Any]:
        spec = spec or {}
        d_type = str(spec.get("deliverable_type") or "").strip().lower() or "unknown"
        lang = str(spec.get("code_language") or "").strip().lower()
        if not lang:
            low = (deliverable or "").lower()
            if "<!doctype html" in low or "<html" in low:
                lang = "html"
            elif "def " in low and "import " in low:
                lang = "python"
            elif "function " in low or "=>" in low:
                lang = "javascript"
            else:
                lang = "text"

        title = ""
        if lang == "html":
            title = self._extract_html_title(deliverable)

        has_eof = "DR_EOF" in (deliverable or "")
        return {
            "deliverable_type": d_type,
            "code_language": lang,
            "title": title,
            "has_dr_eof": bool(has_eof),
            "chars": len(deliverable or ""),
            "lines": (deliverable or "").count("\n") + (1 if deliverable else 0),
        }

    def _ensure_artifact_fields_cache(
        self,
        sess: Dict[str, Any],
        *,
        deliverable: str,
        spec: Optional[dict],
        cfg: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Extract & cache stable fields from the active artifact.

        Purpose: after some unrelated chat, routing/intent should still know what
        the 'current artifact' is, without requiring the whole artifact to be in context.
        """
        cache: Dict[str, Any] = sess.setdefault("artifact_fields", {}) or {}
        try:
            h = hashlib.sha1((deliverable or "").encode("utf-8", "ignore")).hexdigest()
        except Exception:
            h = str(time.time())

        if cache.get("hash") != h:
            meta = self._compute_artifact_meta(deliverable, spec)
            outline_max = _as_int(
                (cfg or {}).get("context_stitch_outline_lines"),
                getattr(self.valves, "CONTEXT_STITCH_OUTLINE_LINES", 60),
            )
            outline_max = max(10, min(200, outline_max))
            anchors_max = _as_int(
                (cfg or {}).get("context_stitch_anchor_lines"),
                getattr(self.valves, "CONTEXT_STITCH_ANCHOR_LINES", 18),
            )
            anchors_max = max(6, min(80, anchors_max))

            baseline_outline = self._extract_baseline_outline(
                deliverable,
                code_language=meta.get("code_language", ""),
                max_items=max(outline_max, 30),
            )
            anchor_candidates = self._extract_anchor_candidates(
                deliverable, max_n=anchors_max
            )

            cache = {
                "hash": h,
                "meta": meta,
                "baseline_outline": list(baseline_outline[:outline_max]),
                "anchor_candidates": list(anchor_candidates[:anchors_max]),
                "updated_at": time.time(),
            }
            sess["artifact_fields"] = cache

        return cache

    def _build_context_pack(
        self,
        sess: Dict[str, Any],
        *,
        deliverable: str,
        spec: Optional[dict],
        scoped: Optional[dict],
        locked_target: Optional[dict],
        recent_context_excerpt: str,
        user_message: str,
        cfg: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Build a compact 'context pack' used for routing/intent/planning.

        This is NOT the artifact itself; it's a stitched, structured snapshot.
        """
        spec = spec or {}
        scoped = scoped or {}
        locked_target = locked_target or {}

        fields = self._ensure_artifact_fields_cache(
            sess, deliverable=deliverable, spec=spec, cfg=cfg
        )

        recent_chars = _as_int(
            (cfg or {}).get("context_stitch_recent_chars"),
            getattr(self.valves, "CONTEXT_STITCH_RECENT_CHARS", 2400),
        )
        recent_chars = max(400, min(8000, recent_chars))

        req_summary = (
            str(spec.get("requirements_summary") or "").strip()
            or str(scoped.get("requirements_summary") or "").strip()
            or str(scoped.get("scope") or "").strip()
        )

        pack = {
            "active_artifact": fields.get("meta", {}),
            "requirements_summary": req_summary[:1200],
            "must_have": list(spec.get("must_have") or [])[:30],
            "locked_target": locked_target,
            "baseline_outline": fields.get("baseline_outline", []),
            "anchor_candidates": fields.get("anchor_candidates", []),
            "recent_context_excerpt": (recent_context_excerpt or "")[:recent_chars],
            "user_message": (user_message or "")[:2000],
        }

        # ---- augment: history store stats / retrieval excerpt ----
        try:
            hs = sess.get("history_store")
            if isinstance(hs, list):
                pack["history_store"] = {
                    "count": len(hs),
                    "loaded_from_disk": bool(
                        (sess.get("history_store_meta") or {}).get("loaded_from_disk")
                    ),
                }
        except Exception:
            pass

        try:
            retrieval_enabled = _as_bool(
                cfg.get("context_stitch_retrieval_enabled"),
                getattr(self.valves, "CONTEXT_STITCH_RETRIEVAL_ENABLED", True),
            )
            if retrieval_enabled:
                top_k = _as_int(
                    cfg.get("context_stitch_retrieval_top_k"),
                    getattr(self.valves, "CONTEXT_STITCH_RETRIEVAL_TOP_K", 8),
                )
                max_chars = _as_int(
                    cfg.get("context_stitch_retrieval_max_chars"),
                    getattr(self.valves, "CONTEXT_STITCH_RETRIEVAL_MAX_CHARS", 7000),
                )
                q = (user_message or "").strip()
                excerpt = _select_relevant_history_excerpt(
                    sess.get("history_store"),
                    q,
                    top_k=top_k,
                    max_chars=max_chars,
                )
                if excerpt:
                    pack["relevant_history_excerpt"] = excerpt
        except Exception:
            pass

        # ---- augment: artifact slices (query-aware snippets from the active artifact) ----
        try:
            slices_enabled = _as_bool(
                cfg.get("context_stitch_artifact_slices_enabled"),
                getattr(self.valves, "CONTEXT_STITCH_ARTIFACT_SLICES_ENABLED", True),
            )
            if slices_enabled and deliverable:
                max_slices = _as_int(
                    cfg.get("context_stitch_artifact_slices_max"),
                    getattr(self.valves, "CONTEXT_STITCH_ARTIFACT_SLICES_MAX", 8),
                )
                window_lines = _as_int(
                    cfg.get("context_stitch_artifact_slice_window_lines"),
                    getattr(
                        self.valves, "CONTEXT_STITCH_ARTIFACT_SLICE_WINDOW_LINES", 34
                    ),
                )
                slices = _select_artifact_slices(
                    deliverable,
                    user_message or "",
                    max_slices=max_slices,
                    window_lines=window_lines,
                )
                # cap each excerpt to avoid token blowup
                for s in slices:
                    try:
                        ex = s.get("excerpt")
                        if isinstance(ex, str) and len(ex) > 2600:
                            s["excerpt"] = _smart_truncate_middle(ex, 2600)
                    except Exception:
                        pass
                if slices:
                    pack["artifact_slices"] = slices
        except Exception:
            pass

        sess["context_pack"] = pack
        return pack

    def _extract_baseline_outline(
        self, baseline: str, code_language: str = "", max_items: int = 40
    ) -> List[str]:
        """Build a lightweight outline (headings / function/class names / key IDs) to help the planner locate areas."""
        if not isinstance(baseline, str) or not baseline:
            return []
        try:
            max_items = int(max_items or 0)
        except Exception:
            max_items = 40
        max_items = max(0, min(max_items, 120))
        if max_items <= 0:
            return []
        lang = (code_language or "").strip().lower()
        out: List[str] = []
        seen = set()

        def _add(s: str):
            s = (s or "").strip()
            if not s:
                return
            if s in seen:
                return
            seen.add(s)
            out.append(s)

        lines = baseline.splitlines()
        # headings for docs
        for ln in lines:
            t = (ln or "").strip()
            if not t:
                continue
            if t.startswith("#") and len(t) <= 120:
                _add(t)
            if len(out) >= max_items:
                return out

        # common code structures
        import re as _re

        if lang in (
            "html",
            "htm",
            "javascript",
            "js",
            "typescript",
            "ts",
            "python",
            "py",
            "",
        ):
            for ln in lines:
                t = (ln or "").strip()
                if not t or len(t) > 220:
                    continue
                m = _re.match(r"^(class\s+\w+)", t)
                if m:
                    _add(m.group(1))
                m = _re.match(r"^(function\s+\w+)", t)
                if m:
                    _add(m.group(1))
                m = _re.match(r"^(async\s+function\s+\w+)", t)
                if m:
                    _add(m.group(1))
                m = _re.match(r"^(def\s+\w+\s*\()", t)
                if m:
                    _add(m.group(1).rstrip())
                # HTML id hints
                if "id=" in t and ("<" in t and ">" in t):
                    m = _re.search(r"id=[\"\x27]([^\"\x27]{1,50})[\"\x27]", t)
                    if m:
                        _add(f"id={m.group(1)}")
                if len(out) >= max_items:
                    break
        return out[:max_items]

    def _extract_focus_snippets(
        self,
        baseline: str,
        keywords: List[str],
        *,
        window: int = 700,
        max_snippets: int = 6,
        max_total_chars: int = 16000,
    ) -> List[str]:
        """Extract focused snippets around keywords to help patch generator hit anchors in the middle of huge artifacts."""
        if not isinstance(baseline, str) or not baseline:
            return []
        if not isinstance(keywords, list) or not keywords:
            return []
        try:
            window = int(window or 0)
        except Exception:
            window = 700
        window = max(120, min(window, 4000))
        try:
            max_snippets = int(max_snippets or 0)
        except Exception:
            max_snippets = 6
        max_snippets = max(1, min(max_snippets, 20))
        try:
            max_total_chars = int(max_total_chars or 0)
        except Exception:
            max_total_chars = 16000
        max_total_chars = max(2000, min(max_total_chars, 80000))

        # normalize keywords
        kws: List[str] = []
        seen_kw = set()
        for kw in keywords:
            if kw is None:
                continue
            s = str(kw).strip()
            if len(s) < 2:
                continue
            if s in seen_kw:
                continue
            seen_kw.add(s)
            kws.append(s)
        if not kws:
            return []

        out: List[str] = []
        seen_snip = set()
        total = 0
        text = baseline
        for kw in kws:
            start = 0
            hits = 0
            while hits < 2:
                idx = text.find(kw, start)
                if idx == -1:
                    break
                s = max(0, idx - window)
                e = min(len(text), idx + len(kw) + window)
                snip = text[s:e].strip()
                if snip and snip not in seen_snip:
                    seen_snip.add(snip)
                    out.append(snip)
                    total += len(snip)
                    if len(out) >= max_snippets or total >= max_total_chars:
                        return out
                hits += 1
                start = idx + max(1, len(kw))
        return out

    async def _artifact_edit_align_request(
        self,
        __request__,
        user_obj: dict,
        base_model: str,
        metadata_in: Dict[str, Any],
        sid: str,
        deliverable_type: str,
        code_language: str,
        spec: Dict[str, Any],
        intent_obj: Dict[str, Any],
        baseline_text: str,
        user_message: str,
    ) -> Dict[str, Any]:
        """Lightweight requirement alignment for Artifact Edit Mode.

        Returns a dict like:
          {"ready": bool, "clarify_question": str, "aligned_summary": str, "patch_focus": [...], ...}
        """
        sys_prompt = str(
            getattr(self.valves, "ARTIFACT_EDIT_ALIGNMENT_SYSTEM_PROMPT", "") or ""
        ).strip()
        if not sys_prompt:
            return {"ready": True}

        head = (baseline_text or "")[:4000]
        tail = (
            (baseline_text or "")[-4000:]
            if baseline_text and len(baseline_text) > 4000
            else (baseline_text or "")
        )
        payload = {
            "user_message": user_message,
            "deliverable_type": deliverable_type,
            "code_language": code_language,
            "requirements_summary": spec.get("requirements_summary") or "",
            "must_have": spec.get("must_have") or [],
            "baseline_len": len(baseline_text or ""),
            "baseline_head": head,
            "baseline_tail": tail,
            "baseline_outline": self._extract_baseline_outline(
                baseline_text, code_language=code_language, max_items=40
            ),
            "anchor_candidates": self._extract_anchor_candidates(
                baseline_text, max_n=12
            ),
            "intent": intent_obj.get("intent") or "",
            "patch_focus": intent_obj.get("patch_focus") or [],
            "delta_scope": intent_obj.get("delta_scope") or "",
        }

        msgs = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ]

        internal_md = self._build_internal_metadata(
            metadata_in, base_model, "artifact_edit_align", 0, sid
        )
        resp = await self._call_llm(
            __request__,
            user_obj,
            base_model,
            msgs,
            internal_md,
            forward_opts={},
            temperature=0.1,
            tool_choice_override="none",
            tools_override=[],
        )
        txt = _extract_text_from_assistant_response(resp)
        obj = _extract_json_object(txt) if isinstance(txt, str) else None
        if not isinstance(obj, dict):
            # Soft-fallback: proceed without blocking
            return {"ready": True}

        # Normalize fields
        ready = bool(obj.get("ready")) if obj.get("ready") is not None else True
        q = str(obj.get("clarify_question") or "").strip()
        aligned_summary = str(obj.get("aligned_summary") or "").strip()
        goals = obj.get("goals")
        assumptions = obj.get("assumptions")
        patch_focus = obj.get("patch_focus")
        must_have = obj.get("must_have")
        req_sum = str(obj.get("requirements_summary") or "").strip()
        risk_notes = str(obj.get("risk_notes") or "").strip()

        def _as_list(v):
            if isinstance(v, list):
                return [str(x).strip() for x in v if str(x).strip()]
            if isinstance(v, str) and v.strip():
                return [v.strip()]
            return []

        out: Dict[str, Any] = {
            "ready": bool(ready),
            "clarify_question": q,
            "aligned_summary": aligned_summary,
            "goals": _as_list(goals)[:12],
            "assumptions": _as_list(assumptions)[:12],
            "patch_focus": _as_list(patch_focus)[:16],
            "must_have": _as_list(must_have)[:16],
            "requirements_summary": req_sum[:1200],
            "risk_notes": risk_notes[:1200],
        }
        # If ready is False but question empty, don't block.
        if (not out["ready"]) and (not out["clarify_question"]):
            out["ready"] = True
        return out

    async def _artifact_edit_plan_stages(
        self,
        __request__,
        user_obj: dict,
        base_model: str,
        metadata_in: dict,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        *,
        deliverable_type: str,
        code_language: str,
        spec: Dict[str, Any],
        baseline_text: str,
        intent_obj: Dict[str, Any],
        user_message: str,
    ) -> List[Dict[str, Any]]:
        sys_prompt = str(
            getattr(self.valves, "ARTIFACT_EDIT_STAGE_PLANNER_SYSTEM_PROMPT", "") or ""
        ).strip()
        if not sys_prompt:
            return []
        try:
            max_stages = int(getattr(self.valves, "ARTIFACT_EDIT_MAX_STAGES", 8) or 8)
        except Exception:
            max_stages = 8
        max_stages = max(1, min(max_stages, 12))
        head = baseline_text[:4000]
        tail = baseline_text[-4000:] if len(baseline_text) > 4000 else baseline_text
        payload = {
            "user_message": user_message,
            "deliverable_type": deliverable_type,
            "code_language": code_language,
            "requirements_summary": spec.get("requirements_summary") or "",
            "must_have": spec.get("must_have") or [],
            "baseline_len": len(baseline_text or ""),
            "baseline_head": head,
            "baseline_tail": tail,
            "anchor_candidates": self._extract_anchor_candidates(
                baseline_text, max_n=12
            ),
            "baseline_outline": self._extract_baseline_outline(
                baseline_text, code_language=code_language, max_items=40
            ),
            "intent": intent_obj.get("intent") or "",
            "patch_focus": intent_obj.get("patch_focus") or [],
            "delta_scope": intent_obj.get("delta_scope") or "",
            "max_patches": spec.get("artifact_edit_max_patches") or 12,
            "apply_policy": spec.get("artifact_edit_apply_policy") or {},
        }
        msgs = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ]
        internal_md = self._build_internal_metadata(
            metadata_in, base_model, "artifact_edit_plan", 0, sid
        )
        resp = await self._call_llm(
            __request__,
            user_obj,
            base_model,
            msgs,
            internal_md,
            forward_opts={},
            temperature=0.15,
            tool_choice_override="none",
            tools_override=[],
        )
        txt = _extract_text_from_assistant_response(resp)
        obj = _extract_json_object(txt) if isinstance(txt, str) else None
        if not isinstance(obj, dict):
            return []
        stages = obj.get("stages")
        if not isinstance(stages, list):
            return []
        clean: List[Dict[str, Any]] = []
        for it in stages:
            if not isinstance(it, dict):
                continue
            title = str(it.get("title") or "").strip()
            goal = str(it.get("goal") or "").strip()
            if not title:
                continue

            sid_val = it.get("id")
            try:
                sid_int = (
                    int(str(sid_val).strip())
                    if sid_val is not None
                    else (len(clean) + 1)
                )
            except Exception:
                sid_int = len(clean) + 1

            where = str(
                it.get("where") or it.get("area") or it.get("location") or ""
            ).strip()
            kws = (
                it.get("keywords")
                or it.get("search_keywords")
                or it.get("locate_keywords")
                or []
            )
            if isinstance(kws, str):
                kws = [kws]
            if not isinstance(kws, list):
                kws = []
            kws = [str(x).strip() for x in kws if str(x).strip()]

            clean.append(
                {
                    "id": sid_int,
                    "title": title[:120],
                    "goal": goal[:400],
                    "where": where[:200],
                    "keywords": kws[:12],
                    "notes": str(it.get("notes") or "")[:400],
                }
            )
            if len(clean) >= max_stages:
                break
        # Enforce a minimum number of stages; if the planner returns too few, retry once with stricter instruction.
        try:
            min_stages = int(getattr(self.valves, "ARTIFACT_EDIT_MIN_STAGES", 3) or 3)
        except Exception:
            min_stages = 3
        if min_stages < 1:
            min_stages = 1
        if len(clean) < min_stages:
            try:
                sys_prompt2 = sys_prompt + (
                    "\n\n【强制要求】你上一次输出的 stages 数量不足。请把需求拆成至少 %d 个阶段（更细，且每阶段都写清 where/keywords）。"
                    % min_stages
                )
                msgs2 = [
                    {"role": "system", "content": sys_prompt2},
                    {
                        "role": "user",
                        "content": json.dumps(payload, ensure_ascii=False),
                    },
                ]
                resp2 = await self._call_llm(
                    __request__,
                    user_obj,
                    base_model,
                    msgs2,
                    internal_md,
                    forward_opts={},
                    temperature=0.1,
                    tool_choice_override="none",
                    tools_override=[],
                )
                txt2 = _extract_text_from_assistant_response(resp2)
                obj2 = _extract_json_object(txt2) if isinstance(txt2, str) else None
                stages2 = obj2.get("stages") if isinstance(obj2, dict) else None
                if isinstance(stages2, list):
                    clean2 = []
                    for it in stages2:
                        if not isinstance(it, dict):
                            continue
                        title = str(it.get("title") or "").strip()
                        goal = str(it.get("goal") or "").strip()
                        if not title:
                            continue
                        sid_val = it.get("id")
                        try:
                            sid_int = (
                                int(str(sid_val).strip())
                                if sid_val is not None
                                else (len(clean2) + 1)
                            )
                        except Exception:
                            sid_int = len(clean2) + 1
                        where = str(
                            it.get("where")
                            or it.get("area")
                            or it.get("location")
                            or ""
                        ).strip()
                        kws = (
                            it.get("keywords")
                            or it.get("search_keywords")
                            or it.get("locate_keywords")
                            or []
                        )
                        if isinstance(kws, str):
                            kws = [kws]
                        if not isinstance(kws, list):
                            kws = []
                        kws = [str(x).strip() for x in kws if str(x).strip()]
                        clean2.append(
                            {
                                "id": sid_int,
                                "title": title[:120],
                                "goal": goal[:400],
                                "where": where[:200],
                                "keywords": kws[:12],
                                "notes": str(it.get("notes") or "")[:400],
                            }
                        )
                        if len(clean2) >= max_stages:
                            break
                    if len(clean2) >= min_stages:
                        clean = clean2
            except Exception:
                pass

        if len(clean) < min_stages:
            # Final fallback: deterministic coarse-to-fine split
            clean = [
                {
                    "id": 1,
                    "title": "修复启动/报错/可运行性",
                    "goal": "先让交付物可运行/不报错",
                    "where": "初始化/入口/资源加载/关键报错处",
                    "keywords": payload.get("anchor_candidates", [])[:4],
                    "notes": "",
                },
                {
                    "id": 2,
                    "title": "参数与体验调整（如速度/兼容性）",
                    "goal": "调参并验证跨端手感与兼容性",
                    "where": "配置区/输入控制/帧率相关逻辑",
                    "keywords": (
                        payload.get("anchor_candidates", [])
                        + payload.get("baseline_outline", [])
                    )[:6],
                    "notes": "",
                },
                {
                    "id": 3,
                    "title": "新增内容并收尾自测",
                    "goal": "在不丢原功能基础上增加内容并做一致性检查",
                    "where": "关卡/剧情/玩法扩展区域 + 最终输出收尾",
                    "keywords": payload.get("anchor_candidates", [])[-4:],
                    "notes": "",
                },
            ][:max_stages]

        return clean

    async def _artifact_edit_generate_patches_for_stage(
        self,
        __request__,
        user_obj: dict,
        base_model: str,
        metadata_in: dict,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        *,
        stage: Dict[str, Any],
        deliverable_type: str,
        code_language: str,
        spec: Dict[str, Any],
        baseline_context: str,
        focus_snippets: List[str],
        anchor_candidates: List[str],
        intent_obj: Dict[str, Any],
        user_message: str,
        feedback: Optional[Dict[str, Any]] = None,
    ) -> Tuple[List[Dict[str, Any]], str]:
        sys_prompt = str(
            getattr(self.valves, "ARTIFACT_EDIT_PATCH_SYSTEM_PROMPT", "") or ""
        ).strip()
        if not sys_prompt:
            return [], ""
        payload = {
            "stage": stage,
            "user_message": user_message,
            "deliverable_type": deliverable_type,
            "code_language": code_language,
            "eof_marker": spec.get("eof_marker") or "DR_EOF",
            "requirements_summary": spec.get("requirements_summary") or "",
            "must_have": spec.get("must_have") or [],
            "baseline": baseline_context,
            "focus_snippets": focus_snippets or [],
            "anchor_candidates": anchor_candidates or [],
            "intent": intent_obj.get("intent") or "",
            "patch_focus": intent_obj.get("patch_focus") or [],
            "delta_scope": intent_obj.get("delta_scope") or "",
            "max_patches": spec.get("artifact_edit_max_patches") or 12,
            "apply_policy": spec.get("artifact_edit_apply_policy") or {},
        }
        if isinstance(feedback, dict) and feedback:
            payload["feedback"] = feedback
        msgs = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ]
        internal_md = self._build_internal_metadata(
            metadata_in,
            base_model,
            f"artifact_edit_patch_{stage.get('id') or 'x'}",
            0,
            sid,
        )
        resp = await self._call_llm(
            __request__,
            user_obj,
            base_model,
            msgs,
            internal_md,
            forward_opts={},
            temperature=0.12,
            tool_choice_override="none",
            tools_override=[],
        )
        txt = _extract_text_from_assistant_response(resp)
        obj = _extract_json_object(txt) if isinstance(txt, str) else None
        if not isinstance(obj, dict):
            return [], ""
        patches = obj.get("patches")
        if not isinstance(patches, list):
            patches = []
        notes = str(obj.get("notes") or "").strip()
        clean_patches: List[Dict[str, Any]] = []
        forbidden = ("[DR_EXCERPT_CUT]", "[TRUNCATED]")
        for p in patches:
            if not isinstance(p, dict):
                continue
            bad = False
            for k in ("find", "anchor", "start", "end"):
                v = str(p.get(k) or "")
                if v and any(f in v for f in forbidden):
                    bad = True
                    break
            if bad:
                continue
            clean_patches.append(p)
        return clean_patches, notes

    async def _handle_followup_edit_mode(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        base_model: str,
        metadata: Dict[str, Any],
        intent_obj: Dict[str, Any],
        user_message: str,
        *,
        prev_target: str,
        prev_spec: Optional[Dict[str, Any]],
        prev_deliverable: str,
        prev_scoped: Optional[Dict[str, Any]] = None,
        max_output_continue_turns: int = 0,
    ) -> Dict[str, Any]:
        """Independent Artifact Edit Mode: read baseline -> plan stages -> generate multiple patch sets -> apply -> output full updated version."""
        intent = str(intent_obj.get("intent") or "patch").strip().lower()
        context_mode = (
            str(
                intent_obj.get("context_injection")
                or intent_obj.get("context_mode")
                or "full"
            )
            .strip()
            .lower()
        )
        spec = self._infer_followup_spec(prev_spec, prev_deliverable)
        deliverable_type = (
            str(spec.get("deliverable_type") or "").strip().lower() or "report"
        )
        is_code = deliverable_type == "code"
        code_lang = str(spec.get("code_language") or "").strip().lower()

        # Normalize baseline (code: extract raw; text: keep)
        baseline_raw = prev_deliverable or ""
        work = baseline_raw
        if is_code:
            try:
                bl_lang, bl_code = _extract_code_candidate(
                    baseline_raw, prefer_lang=code_lang or "html"
                )
                work = _normalize_code_candidate(
                    bl_code, bl_lang or code_lang or "text"
                )
            except Exception:
                work = baseline_raw
        baseline_len = len(work or "")

        # Per-chat config (optional, from router filter)
        dr = _get_dr_dict(metadata)
        cfg = dr.get("config") if isinstance(dr.get("config"), dict) else {}

        # --- Smart Detach: let model decide whether to detach huge artifacts ---
        try:
            context_mode = await self._smart_adjust_context_mode_for_detach(
                __request__,
                __event_emitter__,
                ui_mode,
                sid,
                user_obj,
                base_model,
                metadata,
                intent=intent,
                deliverable_type=deliverable_type,
                code_lang=code_lang,
                work=work,
                context_mode=context_mode,
                cfg=cfg if isinstance(cfg, dict) else {},
            )
        except Exception:
            # keep original context_mode
            pass

        # --- Patch apply retry policy (cfg overrides supported) ---
        max_attempts = _as_int(
            (
                cfg.get("artifact_edit_max_patch_attempts_per_stage")
                if isinstance(cfg, dict)
                else None
            ),
            int(
                getattr(self.valves, "ARTIFACT_EDIT_MAX_PATCH_ATTEMPTS_PER_STAGE", 3)
                or 3
            ),
        )
        max_attempts = max(1, min(int(max_attempts), 30))

        stop_on_stage_fail = _as_bool(
            (
                cfg.get("artifact_edit_stop_on_stage_fail")
                if isinstance(cfg, dict)
                else None
            ),
            bool(getattr(self.valves, "ARTIFACT_EDIT_STOP_ON_STAGE_FAIL", True)),
        )

        critical_extra_attempts = _as_int(
            (
                cfg.get("artifact_edit_stage_critical_extra_attempts")
                if isinstance(cfg, dict)
                else None
            ),
            int(
                getattr(self.valves, "ARTIFACT_EDIT_STAGE_CRITICAL_EXTRA_ATTEMPTS", 4)
                or 4
            ),
        )
        if critical_extra_attempts < 0:
            critical_extra_attempts = 0
        if critical_extra_attempts > 20:
            critical_extra_attempts = 20

        show_attempt_status = _as_bool(
            (
                cfg.get("artifact_edit_show_attempt_status")
                if isinstance(cfg, dict)
                else None
            ),
            bool(getattr(self.valves, "ARTIFACT_EDIT_SHOW_ATTEMPT_STATUS", True)),
        )

        stage_min_ratio = _as_float(
            (
                cfg.get("artifact_edit_stage_min_len_ratio")
                if isinstance(cfg, dict)
                else None
            ),
            float(
                getattr(self.valves, "ARTIFACT_EDIT_STAGE_MIN_LEN_RATIO", 0.85) or 0.85
            ),
        )
        overall_min_ratio = _as_float(
            (
                cfg.get("artifact_edit_overall_min_len_ratio")
                if isinstance(cfg, dict)
                else None
            ),
            float(
                getattr(self.valves, "ARTIFACT_EDIT_OVERALL_MIN_LEN_RATIO", 0.8) or 0.8
            ),
        )
        n_anchors = _as_int(
            (
                cfg.get("artifact_edit_anchor_candidates")
                if isinstance(cfg, dict)
                else None
            ),
            int(getattr(self.valves, "ARTIFACT_EDIT_ANCHOR_CANDIDATES", 12) or 12),
        )
        min_anchor_hits = _as_int(
            (
                cfg.get("artifact_edit_anchor_min_hits")
                if isinstance(cfg, dict)
                else None
            ),
            int(getattr(self.valves, "ARTIFACT_EDIT_ANCHOR_MIN_HITS", 3) or 3),
        )
        preface_max = _as_int(
            (
                cfg.get("artifact_edit_output_preface_max_lines")
                if isinstance(cfg, dict)
                else None
            ),
            int(
                getattr(self.valves, "ARTIFACT_EDIT_OUTPUT_PREFACE_MAX_LINES", 12) or 12
            ),
        )

        max_patches_per_attempt = _as_int(
            (
                cfg.get("artifact_edit_max_patches_per_attempt")
                if isinstance(cfg, dict)
                else None
            ),
            int(
                getattr(self.valves, "ARTIFACT_EDIT_MAX_PATCHES_PER_ATTEMPT", 12) or 12
            ),
        )
        max_patches_per_attempt = max(1, min(int(max_patches_per_attempt), 30))

        require_full_apply_for_code = _as_bool(
            (
                cfg.get("artifact_edit_require_full_apply_for_code")
                if isinstance(cfg, dict)
                else None
            ),
            bool(
                getattr(self.valves, "ARTIFACT_EDIT_REQUIRE_FULL_APPLY_FOR_CODE", True)
            ),
        )
        allowed_skip_ratio = _as_float(
            (
                cfg.get("artifact_edit_allowed_skip_ratio")
                if isinstance(cfg, dict)
                else None
            ),
            float(getattr(self.valves, "ARTIFACT_EDIT_ALLOWED_SKIP_RATIO", 0.0) or 0.0),
        )
        if allowed_skip_ratio < 0.0:
            allowed_skip_ratio = 0.0
        if allowed_skip_ratio > 0.8:
            allowed_skip_ratio = 0.8

        retry_on_partial_apply = _as_bool(
            (
                cfg.get("artifact_edit_retry_on_partial_apply")
                if isinstance(cfg, dict)
                else None
            ),
            bool(getattr(self.valves, "ARTIFACT_EDIT_RETRY_ON_PARTIAL_APPLY", True)),
        )
        commit_partial_apply = _as_bool(
            (
                cfg.get("artifact_edit_commit_partial_apply")
                if isinstance(cfg, dict)
                else None
            ),
            bool(getattr(self.valves, "ARTIFACT_EDIT_COMMIT_PARTIAL_APPLY", True)),
        )
        require_progress = _as_bool(
            (
                cfg.get("artifact_edit_require_progress")
                if isinstance(cfg, dict)
                else None
            ),
            bool(getattr(self.valves, "ARTIFACT_EDIT_REQUIRE_PROGRESS", True)),
        )

        # Make policy available to patch generator (via spec payload)
        try:
            spec = dict(spec or {})
            spec["artifact_edit_max_patches"] = max_patches_per_attempt
            spec["artifact_edit_apply_policy"] = {
                "require_full_apply_for_code": bool(require_full_apply_for_code),
                "allowed_skip_ratio": float(allowed_skip_ratio),
                "retry_on_partial_apply": bool(retry_on_partial_apply),
                "commit_partial_apply": bool(commit_partial_apply),
                "require_progress": bool(require_progress),
                "enable_ws_regex": bool(
                    _as_bool(
                        (
                            cfg.get("artifact_edit_ws_regex_match")
                            if isinstance(cfg, dict)
                            else None
                        ),
                        bool(
                            getattr(self.valves, "ARTIFACT_EDIT_WS_REGEX_MATCH", True)
                        ),
                    )
                ),
                "enable_fuzzy": bool(
                    _as_bool(
                        (
                            cfg.get("artifact_edit_fuzzy_match")
                            if isinstance(cfg, dict)
                            else None
                        ),
                        bool(getattr(self.valves, "ARTIFACT_EDIT_FUZZY_MATCH", True)),
                    )
                ),
                "fuzzy_min_ratio": float(
                    _as_float(
                        (
                            cfg.get("artifact_edit_fuzzy_min_ratio")
                            if isinstance(cfg, dict)
                            else None
                        ),
                        float(
                            getattr(self.valves, "ARTIFACT_EDIT_FUZZY_MIN_RATIO", 0.985)
                            or 0.985
                        ),
                    )
                ),
                "fuzzy_max_candidates": int(
                    _as_int(
                        (
                            cfg.get("artifact_edit_fuzzy_max_candidates")
                            if isinstance(cfg, dict)
                            else None
                        ),
                        int(
                            getattr(
                                self.valves, "ARTIFACT_EDIT_FUZZY_MAX_CANDIDATES", 120
                            )
                            or 120
                        ),
                    )
                ),
                "fuzzy_short_min_len": int(
                    _as_int(
                        (
                            cfg.get("artifact_edit_fuzzy_short_min_len")
                            if isinstance(cfg, dict)
                            else None
                        ),
                        int(
                            getattr(
                                self.valves, "ARTIFACT_EDIT_FUZZY_SHORT_MIN_LEN", 24
                            )
                            or 24
                        ),
                    )
                ),
                "fuzzy_ambiguity_margin": float(
                    _as_float(
                        (
                            cfg.get("artifact_edit_fuzzy_ambiguity_margin")
                            if isinstance(cfg, dict)
                            else None
                        ),
                        float(
                            getattr(
                                self.valves,
                                "ARTIFACT_EDIT_FUZZY_AMBIGUITY_MARGIN",
                                0.01,
                            )
                            or 0.01
                        ),
                    )
                ),
                "fuzzy_hint_min_ratio": float(
                    _as_float(
                        (
                            cfg.get("artifact_edit_fuzzy_hint_min_ratio")
                            if isinstance(cfg, dict)
                            else None
                        ),
                        0.90,
                    )
                ),
            }
        except Exception:
            pass

        guard_anchors = self._extract_anchor_candidates(work, max_n=n_anchors)

        if __event_emitter__:
            await self._status(
                __event_emitter__,
                ui_mode,
                sid,
                "🧩 进入交付物修改模式：读取基线交付物并生成修改意见…",
                done=False,
            )

        # --- (Edit Mode) Requirement alignment (model-based) ---
        enable_clarify = _as_bool(
            (cfg.get("enable_clarify") if isinstance(cfg, dict) else None), True
        )
        # In edit mode, even if assumption_mode is enabled, we still allow the aligner to request ONE critical question.
        align_obj: Dict[str, Any] = {}
        try:
            if __event_emitter__:
                await self._status(
                    __event_emitter__, ui_mode, sid, "🧭 对齐追问需求…", done=False
                )
            align_obj = await self._artifact_edit_align_request(
                __request__,
                user_obj,
                base_model,
                metadata,
                sid,
                deliverable_type=deliverable_type,
                code_language=code_lang,
                spec=spec,
                intent_obj=intent_obj,
                baseline_text=work,
                user_message=user_message,
            )
        except Exception as _e:
            align_obj = {"ready": True}
            if __event_emitter__ and ui_mode == "debug":
                try:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"⚠️ 需求对齐器异常：{type(_e).__name__}: {_e}",
                        done=False,
                    )
                except Exception:
                    pass

        if isinstance(align_obj, dict) and align_obj:
            try:
                # Merge optional refined requirements into spec
                _rs = str(align_obj.get("requirements_summary") or "").strip()
                if _rs:
                    spec["requirements_summary"] = _rs

                _mh = align_obj.get("must_have")
                if isinstance(_mh, list) and _mh:
                    mh0 = (
                        spec.get("must_have")
                        if isinstance(spec.get("must_have"), list)
                        else []
                    )
                    mh1 = [str(x).strip() for x in _mh if str(x).strip()]
                    # de-dup, keep order
                    out_mh = []
                    seen = set()
                    for it in list(mh0) + mh1:
                        if not it:
                            continue
                        if it in seen:
                            continue
                        seen.add(it)
                        out_mh.append(it)
                    spec["must_have"] = out_mh[:32]

                _pf = align_obj.get("patch_focus")
                if isinstance(_pf, list) and _pf:
                    pf0 = (
                        intent_obj.get("patch_focus")
                        if isinstance(intent_obj.get("patch_focus"), list)
                        else []
                    )
                    pf1 = [str(x).strip() for x in _pf if str(x).strip()]
                    out_pf = []
                    seen = set()
                    for it in list(pf0) + pf1:
                        if not it:
                            continue
                        if it in seen:
                            continue
                        seen.add(it)
                        out_pf.append(it)
                    intent_obj["patch_focus"] = out_pf[:40]

                _sum = str(align_obj.get("aligned_summary") or "").strip()
                if _sum and __event_emitter__:
                    _one = " ".join(_sum.split())
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"✅ 需求对齐：{_one}",
                        done=False,
                    )
            except Exception:
                pass

            # If aligner says not ready, ask ONE critical question and pause (unless clarify disabled).
            try:
                _ready = align_obj.get("ready")
                _q = str(align_obj.get("clarify_question") or "").strip()
                if (_ready is False) and _q and enable_clarify:
                    return {
                        "answer": _q,
                        "update_deliverable": False,
                        "pause": True,
                        "pause_kind": "clarify",
                    }
            except Exception:
                pass

        try:
            stages = await self._artifact_edit_plan_stages(
                __request__,
                user_obj,
                base_model,
                metadata,
                __event_emitter__,
                ui_mode,
                sid,
                deliverable_type=deliverable_type,
                code_language=code_lang,
                spec=spec,
                baseline_text=work,
                intent_obj=intent_obj,
                user_message=user_message,
            )
        except Exception as e:
            stages = []
            if __event_emitter__ and ui_mode == "debug":
                try:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"⚠️ 修改阶段规划器异常：{type(e).__name__}: {e}",
                        done=False,
                    )
                except Exception:
                    pass

        if not stages:
            stages = [
                {
                    "id": 1,
                    "title": "按追问要求修改并补充内容",
                    "goal": str(user_message)[:300],
                    "notes": "",
                }
            ]

        # Show a concise plan so the user knows what will be modified and roughly where
        if __event_emitter__:
            try:
                titles = []
                for i, st in enumerate(stages, start=1):
                    t = str((st or {}).get("title") or f"阶段{i}").strip()
                    w = str((st or {}).get("where") or "").strip()
                    if ui_mode == "debug" and w:
                        titles.append(f"{i}){t}（{w}）")
                    else:
                        titles.append(f"{i}){t}")
                plan_line = "；".join(titles[:8]) + (" …" if len(titles) > 8 else "")
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"🗺️ 修改计划：{plan_line}",
                    done=False,
                )
            except Exception:
                pass

        applied_logs: List[str] = []
        applied_stage_titles: List[str] = []
        failed_stage_titles: List[str] = []

        def _stage_is_critical(st: Dict[str, Any]) -> bool:
            try:
                if isinstance(st, dict) and bool(st.get("critical")):
                    return True
            except Exception:
                pass
            t = str((st or {}).get("title") or "").strip().lower()
            g = str((st or {}).get("goal") or "").strip().lower()
            txt = t + " " + g
            for kw in (
                "修复",
                "fix",
                "bug",
                "启动",
                "初始化",
                "init",
                "报错",
                "error",
                "exception",
                "崩溃",
                "crash",
                "语法",
                "syntax",
                "无法",
                "打不开",
                "白屏",
                "无法启动",
                "无法运行",
                "兼容",
                "compat",
            ):
                if kw in txt:
                    return True
            return False

        def _auto_keywords_for_stage(title: str) -> List[str]:
            tl = str(title or "")
            kws: List[str] = []
            if is_code and (
                code_lang in ("html", "htm", "js", "javascript", "text", "")
                or deliverable_type == "code"
            ):
                if any(
                    x in tl for x in ("启动", "初始化", "报错", "白屏", "无法", "兼容")
                ):
                    kws += [
                        "loadAssets",
                        "DOMContentLoaded",
                        "initLevel",
                        "gameLoop",
                        "requestAnimationFrame",
                        "canvas",
                        "getContext",
                    ]
                if any(
                    x in tl
                    for x in ("速度", "移动", "走路", "手感", "playerSpeed", "控制")
                ):
                    kws += [
                        "playerSpeed",
                        "playerSpeed:",
                        "vx",
                        "vy",
                        "touchControls",
                        "keys",
                        "deltaTime",
                        "speed",
                    ]
                if any(x in tl for x in ("关卡", "level", "剧情", "新增", "扩展")):
                    kws += [
                        "const levels",
                        "levels =",
                        "initLevel",
                        "gameState.level",
                        "dialogueData",
                        "showDialogue",
                    ]
                if any(
                    x in tl
                    for x in ("UI", "界面", "按钮", "手机", "移动端", "触摸", "控制")
                ):
                    kws += [
                        "mobileControls",
                        "touchstart",
                        "touchend",
                        "controlBtn",
                        "uiElements",
                        "getElementById",
                    ]
            # de-dup
            out: List[str] = []
            for k in kws:
                ks = str(k).strip()
                if ks and ks not in out:
                    out.append(ks)
            return out[:12]

        def _merge_stage_keywords(stage: Dict[str, Any]) -> List[str]:
            kws: List[str] = []
            try:
                if isinstance(stage.get("keywords"), list):
                    kws.extend(
                        [str(x) for x in stage.get("keywords") if str(x).strip()]
                    )
            except Exception:
                pass
            try:
                wh = str(stage.get("where") or "").strip()
                if wh:
                    kws.append(wh)
            except Exception:
                pass
            try:
                pf = intent_obj.get("patch_focus")
                if isinstance(pf, list):
                    kws.extend([str(x) for x in pf if str(x).strip()])
            except Exception:
                pass
            try:
                stt = str(stage.get("title") or "").strip()
                if stt:
                    kws.append(stt)
            except Exception:
                pass
            # auto keywords based on title
            try:
                kws.extend(_auto_keywords_for_stage(str(stage.get("title") or "")))
            except Exception:
                pass
            # de-dup
            out: List[str] = []
            for k in kws:
                ks = str(k).strip()
                if ks and ks not in out:
                    out.append(ks)
            return out[:24]

        # Iterate stages; stage failure will trigger stronger retry (no silent skip)
        stopped_due_to_failure = False
        for idx_stage, stage in enumerate(stages, start=1):
            st_title = str(stage.get("title") or f"阶段{idx_stage}").strip()
            st_where = str(stage.get("where") or "").strip()

            stage_critical = _stage_is_critical(stage)
            stage_max_attempts = int(max_attempts) + (
                int(critical_extra_attempts) if stage_critical else 0
            )
            if stage_max_attempts < 1:
                stage_max_attempts = 1
            if stage_max_attempts > 40:
                stage_max_attempts = 40

            if __event_emitter__:
                if st_where and ui_mode == "debug":
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"🔧 修改阶段 {idx_stage}/{len(stages)}：{st_title}（{st_where}）",
                        done=False,
                    )
                else:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"🔧 修改阶段 {idx_stage}/{len(stages)}：{st_title}",
                        done=False,
                    )

            success = False
            feedback: Optional[dict] = None
            last_failure_reason = ""

            # Normal retry loop
            for attempt in range(1, stage_max_attempts + 1):

                # If we are retrying, explicitly tell the user we will regenerate patches based on the last failure feedback.
                if (
                    show_attempt_status
                    and __event_emitter__
                    and attempt > 1
                    and feedback
                ):
                    try:
                        _r = str(
                            (feedback or {}).get("reason") or last_failure_reason or ""
                        ).strip()
                        _comm = bool((feedback or {}).get("committed_partial"))
                        _sklogs = (feedback or {}).get("skipped_logs")
                        _sk = ""
                        if isinstance(_sklogs, list) and _sklogs:
                            _sk = "；跳过示例：" + " | ".join(
                                [str(x) for x in _sklogs[:2]]
                            )
                        _note = "（已保留部分修改）" if _comm else ""
                        _msg = f'🧠 阶段{idx_stage}/{len(stages)}：根据上次失败原因重新生成补丁{_note}（{_r or "retry"}{_sk}）'
                        await self._status(
                            __event_emitter__, ui_mode, sid, _msg, done=False
                        )
                    except Exception:
                        pass

                ctx = self._select_artifact_edit_context(work, context_mode)
                anchors_now = self._extract_anchor_candidates(
                    work, max_n=min(12, n_anchors)
                )
                stage_keywords = _merge_stage_keywords(stage)

                detach_active = False
                try:
                    if bool(getattr(self.valves, "ARTIFACT_DETACH_ENABLED", True)):
                        detach_min = int(
                            getattr(self.valves, "ARTIFACT_DETACH_MIN_CHARS", 60000)
                            or 60000
                        )
                        detach_active = len(work) >= detach_min
                except Exception:
                    detach_active = False

                snip_window = 1000
                snip_max = 6
                snip_total = 22000
                if detach_active:
                    snip_window = int(
                        getattr(self.valves, "ARTIFACT_DETACH_SNIPPET_WINDOW", 1200)
                        or 1200
                    )
                    snip_max = int(
                        getattr(self.valves, "ARTIFACT_DETACH_MAX_SNIPPETS", 10) or 10
                    )
                    budget_hint = int(
                        getattr(
                            self.valves, "ARTIFACT_DETACH_CONTEXT_BUDGET_CHARS", 24000
                        )
                        or 24000
                    )
                    # 聚焦片段不要太大：否则“baseline节选 + 片段”会把模型上下文塞满，反而更容易丢内容
                    snip_total = max(6000, min(16000, int(budget_hint * 0.6)))

                focus_snippets = self._extract_focus_snippets(
                    work,
                    stage_keywords,
                    window=snip_window,
                    max_snippets=snip_max,
                    max_total_chars=snip_total,
                )

                patches, notes = await self._artifact_edit_generate_patches_for_stage(
                    __request__,
                    user_obj,
                    base_model,
                    metadata,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    stage=stage,
                    deliverable_type=deliverable_type,
                    code_language=code_lang,
                    spec=spec,
                    baseline_context=ctx,
                    focus_snippets=focus_snippets,
                    anchor_candidates=anchors_now,
                    intent_obj=intent_obj,
                    user_message=user_message,
                    feedback=feedback,
                )

                if not patches:
                    last_failure_reason = "no_patches_returned"
                    feedback = (
                        {"reason": "no_patches_returned", "notes": notes}
                        if notes
                        else {"reason": "no_patches_returned"}
                    )
                    if show_attempt_status and __event_emitter__:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🔁 阶段{idx_stage}/{len(stages)} 尝试 {attempt}/{stage_max_attempts}：未生成可用补丁（将重试）",
                            done=False,
                        )
                    continue

                if (
                    max_patches_per_attempt
                    and isinstance(patches, list)
                    and len(patches) > int(max_patches_per_attempt)
                ):
                    patches = patches[: int(max_patches_per_attempt)]
                    if notes:
                        notes = (
                            notes
                            + f" | trimmed patches -> {int(max_patches_per_attempt)}"
                        )[:800]
                    else:
                        notes = f"trimmed patches -> {int(max_patches_per_attempt)}"

                # Apply patches
                new_work, applied, skipped = _apply_code_patches(
                    work,
                    patches,
                    policy=(
                        spec.get("artifact_edit_apply_policy")
                        if isinstance(spec, dict)
                        else None
                    ),
                )

                # IMPORTANT: keep/restore EOF marker *after* patch application.
                # Many patch ops (especially large replace) may accidentally drop the marker.
                # This should NOT cause endless retries; we can safely normalize it back.
                try:
                    if is_code and bool(spec.get("need_eof_marker")):
                        _eof_tok = str(spec.get("eof_marker") or "DR_EOF").strip()
                        if _eof_tok:
                            _eof_line = _format_eof_marker_line(
                                _eof_tok, (code_lang or "text")
                            ).strip()
                            new_work = _ensure_eof_marker(
                                new_work, _eof_tok, eof_line=_eof_line
                            )
                except Exception:
                    pass

                total_patches = len(patches)
                applied_n = len(applied)
                skipped_n = len(skipped)
                changed = new_work != work
                skip_ratio = skipped_n / float(total_patches or 1)

                # Guard: avoid accidental big deletion on large artifacts
                stage_len_guard_failed = False
                if changed and len(work) >= 5000:
                    try:
                        if len(new_work) < int(len(work) * stage_min_ratio):
                            stage_len_guard_failed = True
                    except Exception:
                        stage_len_guard_failed = False

                # Basic integrity guard for code deliverables (heuristic)
                integrity_worsened = False
                eof_missing = False
                try:
                    if is_code:
                        eof_marker = str(spec.get("eof_marker") or "DR_EOF").strip()
                        before_issues = _code_integrity_issues(
                            work, code_lang or "text", eof_marker
                        )
                        after_issues = _code_integrity_issues(
                            new_work, code_lang or "text", eof_marker
                        )
                        if len(after_issues) > len(before_issues) + 2:
                            integrity_worsened = True
                        if (
                            bool(spec.get("need_eof_marker"))
                            and eof_marker
                            and ("missing_eof_marker" in after_issues)
                        ):
                            eof_missing = True
                except Exception:
                    pass

                # Decide success / retry
                require_full = bool(is_code and require_full_apply_for_code)
                allow_skip = float(allowed_skip_ratio or 0.0)

                if not retry_on_partial_apply:
                    # Legacy behavior: any applied patch marks stage success
                    if (
                        applied_n > 0
                        and (not stage_len_guard_failed)
                        and (not eof_missing)
                    ):
                        work = new_work
                        applied_logs.extend(applied)
                        applied_stage_titles.append(st_title)
                        success = True
                        if show_attempt_status and __event_emitter__:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                f"✅ 阶段{idx_stage} 完成：应用 {applied_n}/{total_patches} 条补丁",
                                done=False,
                            )
                        break
                    last_failure_reason = "no_patch_applied"
                    feedback = {
                        "reason": "no_patch_applied",
                        "skipped": skipped[:10],
                        "notes": notes,
                        "anchor_candidates": anchors_now,
                    }
                    if show_attempt_status and __event_emitter__:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🔁 阶段{idx_stage} 尝试 {attempt}/{stage_max_attempts}：未命中补丁（将重试）",
                            done=False,
                        )
                    continue

                # Retry-aware mode
                failure_reason = None
                if applied_n <= 0:
                    failure_reason = "no_patch_applied"
                elif require_progress and (not changed):
                    failure_reason = "no_text_change"
                elif stage_len_guard_failed:
                    failure_reason = "stage_len_ratio_too_low"
                elif eof_missing:
                    failure_reason = "eof_marker_missing"
                elif integrity_worsened:
                    failure_reason = "integrity_worsened"
                elif require_full and skipped_n > 0:
                    failure_reason = "partial_apply_skipped"
                elif skip_ratio > allow_skip:
                    failure_reason = "skip_ratio_too_high"

                # Commit partial progress if allowed & safe (so retries build on it)
                committed = False
                if (
                    applied_n > 0
                    and changed
                    and (not stage_len_guard_failed)
                    and (not eof_missing)
                    and (not integrity_worsened)
                    and commit_partial_apply
                ):
                    work = new_work
                    applied_logs.extend(applied)
                    committed = True

                if failure_reason is None:
                    # Stage success
                    if not committed:
                        work = new_work
                        applied_logs.extend(applied)
                    applied_stage_titles.append(st_title)
                    success = True
                    if show_attempt_status and __event_emitter__:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"✅ 阶段{idx_stage} 完成：应用 {applied_n}/{total_patches} 条补丁",
                            done=False,
                        )
                    break

                last_failure_reason = str(failure_reason)

                # Build rich feedback for the next retry
                fb = _build_patch_apply_feedback(
                    work,
                    patches,
                    applied,
                    skipped,
                    notes=notes,
                    max_items=8,
                    policy=(
                        spec.get("artifact_edit_apply_policy")
                        if isinstance(spec, dict)
                        else None
                    ),
                )
                fb["reason"] = failure_reason
                fb["attempt"] = attempt
                fb["max_attempts"] = stage_max_attempts
                fb["require_full_apply_for_code"] = bool(require_full_apply_for_code)
                fb["allowed_skip_ratio"] = float(allowed_skip_ratio or 0.0)
                fb["changed"] = bool(changed)
                fb["committed_partial"] = bool(committed)
                fb["stage_title"] = st_title
                fb["anchor_candidates"] = anchors_now

                feedback = fb

                if show_attempt_status and __event_emitter__:
                    # keep it short in simple mode
                    extra_note = "；已保留已应用补丁" if committed else ""
                    msg = f"🔁 阶段{idx_stage}/{len(stages)} 尝试 {attempt}/{stage_max_attempts}：应用{applied_n} 跳过{skipped_n}（原因：{failure_reason}{extra_note}，将重试并重新生成补丁）"
                    await self._status(__event_emitter__, ui_mode, sid, msg, done=False)

                continue

            # Escalation: strict-anchor mode (avoid drifting anchors)
            if (not success) and stop_on_stage_fail:
                strict_rounds = 4 if stage_critical else 3
                strict_rounds = max(2, min(int(strict_rounds), 8))
                if __event_emitter__:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"🧯 阶段{idx_stage} 未完成：进入更稳健的锚点模式重试（strict-anchor x{strict_rounds}）",
                        done=False,
                    )

                strict_feedback = dict(feedback or {})
                strict_feedback["strict_anchor_mode"] = True

                for s_try in range(1, strict_rounds + 1):
                    ctx = self._select_artifact_edit_context(work, context_mode)
                    anchors_now = self._extract_anchor_candidates(
                        work, max_n=min(16, n_anchors)
                    )
                    stage_keywords = _merge_stage_keywords(stage)
                    focus_snippets = self._extract_focus_snippets(
                        work,
                        stage_keywords,
                        window=1200,
                        max_snippets=6,
                        max_total_chars=26000,
                    )

                    patches, notes = (
                        await self._artifact_edit_generate_patches_for_stage(
                            __request__,
                            user_obj,
                            base_model,
                            metadata,
                            __event_emitter__,
                            ui_mode,
                            sid,
                            stage=stage,
                            deliverable_type=deliverable_type,
                            code_language=code_lang,
                            spec=spec,
                            baseline_context=ctx,
                            focus_snippets=focus_snippets,
                            anchor_candidates=anchors_now,
                            intent_obj=intent_obj,
                            user_message=user_message,
                            feedback=strict_feedback,
                        )
                    )

                    if not patches:
                        last_failure_reason = "no_patches_returned"
                        strict_feedback = {
                            "reason": "no_patches_returned",
                            "strict_anchor_mode": True,
                            "notes": notes,
                        }
                        if show_attempt_status and __event_emitter__:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                f"🔁 strict-anchor {s_try}/{strict_rounds}：未生成补丁（继续）",
                                done=False,
                            )
                        continue

                    if (
                        max_patches_per_attempt
                        and isinstance(patches, list)
                        and len(patches) > int(max_patches_per_attempt)
                    ):
                        patches = patches[: int(max_patches_per_attempt)]

                    new_work, applied, skipped = _apply_code_patches(
                        work,
                        patches,
                        policy=(
                            spec.get("artifact_edit_apply_policy")
                            if isinstance(spec, dict)
                            else None
                        ),
                    )

                    # Keep/restore EOF marker after patch application (see normal retry loop).
                    try:
                        if is_code and bool(spec.get("need_eof_marker")):
                            _eof_tok = str(spec.get("eof_marker") or "DR_EOF").strip()
                            if _eof_tok:
                                _eof_line = _format_eof_marker_line(
                                    _eof_tok, (code_lang or "text")
                                ).strip()
                                new_work = _ensure_eof_marker(
                                    new_work, _eof_tok, eof_line=_eof_line
                                )
                    except Exception:
                        pass
                    total_patches = len(patches)
                    applied_n = len(applied)
                    skipped_n = len(skipped)
                    changed = new_work != work

                    stage_len_guard_failed = False
                    if changed and len(work) >= 5000:
                        try:
                            if len(new_work) < int(len(work) * stage_min_ratio):
                                stage_len_guard_failed = True
                        except Exception:
                            stage_len_guard_failed = False

                    integrity_worsened = False
                    eof_missing = False
                    try:
                        if is_code:
                            eof_marker = str(spec.get("eof_marker") or "DR_EOF").strip()
                            before_issues = _code_integrity_issues(
                                work, code_lang or "text", eof_marker
                            )
                            after_issues = _code_integrity_issues(
                                new_work, code_lang or "text", eof_marker
                            )
                            if len(after_issues) > len(before_issues) + 2:
                                integrity_worsened = True
                            if (
                                bool(spec.get("need_eof_marker"))
                                and eof_marker
                                and ("missing_eof_marker" in after_issues)
                            ):
                                eof_missing = True
                    except Exception:
                        pass

                    failure_reason = None
                    if applied_n <= 0:
                        failure_reason = "no_patch_applied"
                    elif require_progress and (not changed):
                        failure_reason = "no_text_change"
                    elif stage_len_guard_failed:
                        failure_reason = "stage_len_ratio_too_low"
                    elif eof_missing:
                        failure_reason = "eof_marker_missing"
                    elif integrity_worsened:
                        failure_reason = "integrity_worsened"
                    elif (
                        bool(is_code and require_full_apply_for_code) and skipped_n > 0
                    ):
                        failure_reason = "partial_apply_skipped"

                    if failure_reason is None:
                        work = new_work
                        applied_logs.extend(applied)
                        applied_stage_titles.append(st_title)
                        success = True
                        if __event_emitter__:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                f"✅ strict-anchor 成功：阶段{idx_stage} 已完成（应用 {applied_n}/{total_patches}）",
                                done=False,
                            )
                        break

                    last_failure_reason = str(failure_reason)
                    # commit partial if safe
                    committed = False
                    if (
                        applied_n > 0
                        and changed
                        and (not stage_len_guard_failed)
                        and (not eof_missing)
                        and (not integrity_worsened)
                        and commit_partial_apply
                    ):
                        work = new_work
                        applied_logs.extend(applied)
                        committed = True

                    fb = _build_patch_apply_feedback(
                        work,
                        patches,
                        applied,
                        skipped,
                        notes=notes,
                        max_items=8,
                        policy=(
                            spec.get("artifact_edit_apply_policy")
                            if isinstance(spec, dict)
                            else None
                        ),
                    )
                    fb["reason"] = failure_reason
                    fb["strict_anchor_mode"] = True
                    fb["changed"] = bool(changed)
                    fb["committed_partial"] = bool(committed)
                    fb["stage_title"] = st_title
                    fb["anchor_candidates"] = anchors_now
                    strict_feedback = fb

                    if show_attempt_status and __event_emitter__:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🔁 strict-anchor {s_try}/{strict_rounds}：应用{applied_n} 跳过{skipped_n}（原因：{failure_reason}）",
                            done=False,
                        )

                # End strict loop

            if not success:
                failed_stage_titles.append(st_title)
                if __event_emitter__:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"❌ 阶段{idx_stage}/{len(stages)} 未能完成：{st_title}（最后原因：{last_failure_reason or 'unknown'}）。",
                        done=False,
                    )
                # 对代码类/关键修复阶段：不再‘跳过’，直接停止后续阶段，进入兜底整合
                if stop_on_stage_fail or stage_critical:
                    stopped_due_to_failure = True
                    if __event_emitter__:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "🧩 为避免后续阶段在错误基线上叠加导致更糟结果：已停止继续阶段，准备进入兜底整合/保守更新…",
                            done=False,
                        )
                    break
                # 如果允许继续（例如文档类宽容模式），则带着失败记录继续后续阶段
                continue

        # If we stopped due to a failed stage, fall back to a safe full-file update (still tries to preserve anchors/length).
        if (
            stopped_due_to_failure
            and failed_stage_titles
            and is_code
            and work
            and baseline_len >= 2000
        ):
            if __event_emitter__:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    "🧯 自动补丁多次未能完成关键阶段：将回退到‘保守整文件更新’兜底（仍基于原代码，不换题、不丢内容；会做防缩水校验）…",
                    done=False,
                )
            try:
                # reuse fast follow-up patch generator with strong anti-regression guard
                fast = await self._handle_followup_fast(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    base_model,
                    metadata,
                    dict(intent_obj or {}, intent="patch", context_injection="full"),
                    user_message,
                    prev_target=prev_target,
                    prev_spec=prev_spec,
                    prev_deliverable=work,
                    prev_scoped=prev_scoped,
                    max_output_continue_turns=max_output_continue_turns,
                )
                if isinstance(fast, dict) and fast.get("answer"):
                    return fast
            except Exception as _e:
                if __event_emitter__ and ui_mode == "debug":
                    try:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"⚠️ 兜底整文件更新失败：{type(_e).__name__}: {_e}",
                            done=False,
                        )
                    except Exception:
                        pass

        # Overall anti-regression guard (length + anchors)
        if baseline_len >= 5000:
            try:
                if len(work) < int(baseline_len * overall_min_ratio):
                    # If we ever hit this, prefer keeping baseline and only append changes via a final safe stage.
                    work = work  # keep current, but we'll rely on final audit to catch issues
            except Exception:
                pass
        if guard_anchors and min_anchor_hits > 0:
            hit = sum(1 for a in guard_anchors if a and a in work)
            if hit < min_anchor_hits and __event_emitter__:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"⚠️ 防丢校验：锚点命中 {hit}/{len(guard_anchors)}（低于阈值 {min_anchor_hits}），将通过终审审计尽量修复…",
                    done=False,
                )

        # Finalize formatting + audit
        final_text = work
        if is_code:
            final_text = await self._finalize_code_deliverable(
                __request__,
                user_obj,
                base_model,
                metadata,
                prev_scoped or {},
                __event_emitter__,
                ui_mode,
                sid,
                spec,
                final_text,
            )
            final_text = await self._final_audit_deliverable(
                __request__,
                user_obj,
                base_model,
                metadata,
                prev_scoped or {},
                __event_emitter__,
                ui_mode,
                sid,
                spec,
                final_text,
            )
            # Store raw code (without any preface text)
            try:
                fl, fc = _extract_code_candidate(
                    final_text, prefer_lang=code_lang or "html"
                )
                deliverable_out = _normalize_code_candidate(
                    fc, fl or code_lang or "text"
                )
            except Exception:
                deliverable_out = final_text
        else:
            final_text = await self._final_audit_deliverable(
                __request__,
                user_obj,
                base_model,
                metadata,
                prev_scoped or {},
                __event_emitter__,
                ui_mode,
                sid,
                spec,
                final_text,
            )
            deliverable_out = final_text

        # Build a short preface so big-code outputs are not abrupt
        preface: List[str] = []
        preface.append(
            "已进入【交付物修改模式】：在上一版基础上分阶段打补丁修改（不会整份重写，避免丢内容）。"
        )
        if applied_stage_titles:
            preface.append(
                "本次修改阶段："
                + " / ".join(applied_stage_titles[:6])
                + (" …" if len(applied_stage_titles) > 6 else "")
            )
        if is_code:
            if code_lang in ("html", "htm"):
                preface.append(
                    "运行：保存为 .html 文件，用浏览器打开即可（手机端用浏览器打开同样可玩）。"
                )
            elif code_lang in ("python", "py"):
                preface.append("运行：保存为 .py 文件，然后执行：python your_file.py")
        # Trim preface lines
        preface = [ln for ln in preface if ln][: max(0, preface_max)]
        answer = (
            ("\n".join(preface) + "\n\n" + final_text).strip()
            if preface
            else final_text
        )

        return {
            "answer": answer,
            "update_deliverable": True,
            "deliverable": deliverable_out,
            "deliverable_spec": spec,
            "followup_intent": intent_obj,
            "artifact_edit_stages": stages,
            "artifact_edit_applied": applied_logs,
        }

    async def _judge(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        judge_model: str,
        metadata: Dict[str, Any],
        scoped: Dict[str, Any],
        step_info: str,
        candidate: str,
        current_plan: Optional[Dict[str, Any]] = None,
        step_idx: int = 0,
    ) -> Dict[str, Any]:
        await self._status(
            __event_emitter__, ui_mode, sid, "🔎 评审建议生成…", done=False
        )
        payload = {
            "target": str(scoped.get("target") or ""),
            "goal": str(scoped.get("goal") or ""),
            "step": step_info,
            "step_idx": int(step_idx),
            "plan": current_plan or {},
            "candidate": candidate,
        }
        msgs = [
            {"role": "system", "content": self.valves.JUDGE_SYSTEM_PROMPT},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ]
        internal_md = self._build_internal_metadata(
            metadata, judge_model, "judge", 0, sid
        )
        res = await self._call_llm(
            __request__,
            user_obj,
            judge_model,
            msgs,
            internal_md,
            forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
            temperature=0.0,
            tool_choice_override="none",
            tools_override=[],
        )
        obj = _extract_json_object(_extract_text(res)) or {}
        return obj if isinstance(obj, dict) else {}

    async def _decide(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        base_model: str,
        metadata: Dict[str, Any],
        scoped: Dict[str, Any],
        step_info: str,
        candidate: str,
        judge_obj: Dict[str, Any],
        assumption_mode: bool,
    ) -> Dict[str, Any]:
        await self._status(
            __event_emitter__, ui_mode, sid, "🧠 主模型决策…", done=False
        )
        payload = {
            "target": str(scoped.get("target") or ""),
            "goal": str(scoped.get("goal") or ""),
            "assumption_mode": bool(assumption_mode),
            "step": step_info,
            "candidate": candidate,
            "judge": judge_obj,
        }
        msgs = [
            {"role": "system", "content": self.valves.DECIDER_SYSTEM_PROMPT},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
        ]
        internal_md = self._build_internal_metadata(
            metadata, base_model, "decide", 0, sid
        )
        res = await self._call_llm(
            __request__,
            user_obj,
            base_model,
            msgs,
            internal_md,
            forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
            temperature=0.0,
            tool_choice_override="none",
            tools_override=[],
        )
        obj = _extract_json_object(_extract_text(res)) or {}
        return obj if isinstance(obj, dict) else {}

    # ----------------------------
    # Tool-loop generation + auto-continue
    # ----------------------------
    async def _generate_with_tool_loop(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        base_model: str,
        messages: List[dict],
        metadata: Dict[str, Any],
        forward_opts: Dict[str, Any],
        tool_registry: Dict[str, Any],
        injection: Dict[str, Any],
        sess: Dict[str, Any],
        max_tool_turns: int,
        max_tool_calls_per_turn: int,
        max_output_continue_turns: int,
        show_tool_detail: bool,
    ) -> Tuple[str, Dict[str, Any]]:
        cur_messages = deepcopy(messages)
        stats = {"llm_calls": 0, "tool_calls": 0, "llm_secs": 0.0, "tool_secs": 0.0}
        chart_blocks: List[str] = []

        for turn in range(1, max_tool_turns + 1):
            t0 = _now()
            res = await self._call_llm(
                __request__,
                user_obj,
                base_model,
                cur_messages,
                metadata,
                forward_opts,
                temperature=0.7,
            )
            stats["llm_secs"] += _now() - t0
            stats["llm_calls"] += 1

            raw_text = _extract_text(res)
            tool_calls = _extract_tool_calls(res)
            fr = _extract_finish_reason(res)

            action, cleaned = _extract_action_and_clean_text(raw_text)

            # status
            if action:
                a = action.replace("\n", " ").strip()
                if len(a) > self.valves.ACTION_MAX_LEN:
                    a = a[: self.valves.ACTION_MAX_LEN] + "…"
                await self._status(
                    __event_emitter__, ui_mode, sid, f"➡️ 本轮动作：{a}", done=False
                )

            if ui_mode == "debug":
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"gen_turn {turn}: finish_reason={fr or 'n/a'} text_len={len(cleaned)} tool_calls={len(tool_calls)}",
                    done=False,
                )

            # tool calls
            if tool_calls and max_tool_calls_per_turn > 0 and tool_registry:
                tool_calls = tool_calls[:max_tool_calls_per_turn]
                cur_messages.append(
                    {
                        "role": "assistant",
                        "content": raw_text or "",
                        "tool_calls": tool_calls,
                    }
                )

                for tc in tool_calls:
                    name = _tool_name_of(tc)
                    args = _tool_args_of(tc)
                    if ui_mode == "debug" and show_tool_detail:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🛠️ tool_exec: {name} args={_pretty_json(args, 180)}",
                            done=False,
                        )
                    else:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🛠️ 调用工具：{name}",
                            done=False,
                        )

                    t1 = _now()
                    content = await _execute_one_tool(
                        tool_registry, name, args, injection
                    )
                    stats["tool_secs"] += _now() - t1
                    stats["tool_calls"] += 1

                    # chart snippet extraction
                    new_blocks = _extract_chart_markdown_blocks(content or "")
                    if new_blocks:
                        chart_blocks.extend(new_blocks)

                        # Persist charts across rounds so final output won't lose them.
                        try:
                            if isinstance(sess, dict):
                                allb = sess.get("chart_blocks")
                                if not isinstance(allb, list):
                                    allb = []
                                allb.extend(new_blocks)
                                sess["chart_blocks"] = _dedup_str_list(
                                    [
                                        x
                                        for x in allb
                                        if isinstance(x, str) and x.strip()
                                    ]
                                )
                        except Exception:
                            pass

                        hint = _build_chart_render_system_hint(new_blocks)
                        if hint:
                            cur_messages.append({"role": "system", "content": hint})
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"📊 已识别图表片段：{len(new_blocks)} 个",
                            done=False,
                        )

                    if ui_mode == "debug" and show_tool_detail:
                        prev = (content or "")[
                            : self.valves.STATUS_TOOL_PREVIEW_CHARS
                        ].replace("\n", " ")
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"tool_result: {name} len={len(content or '')} preview={prev}",
                            done=False,
                        )

                    cur_messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": str(tc.get("id") or ""),
                            "content": content or "",
                        }
                    )

                continue

            # If tool_calls exist but tools disabled/unavailable -> ask model to produce final without tools
            if tool_calls and (not tool_registry or max_tool_calls_per_turn <= 0):
                cur_messages.append(
                    {
                        "role": "system",
                        "content": "当前环境无法执行工具调用。请直接基于已有上下文给出可交付的最终答案（不要再请求工具）。",
                    }
                )
                continue

            # got output
            if cleaned:
                # auto-continue if model truncated or code fences unbalanced
                if fr == "length" or _code_fence_unbalanced(cleaned):
                    cleaned2, more_stats = await self._auto_continue(
                        __request__,
                        __event_emitter__,
                        ui_mode,
                        sid,
                        user_obj,
                        base_model,
                        cur_messages,
                        metadata,
                        forward_opts,
                        cleaned,
                        max_output_continue_turns,
                    )
                    stats["llm_calls"] += int((more_stats or {}).get("llm_calls", 0))
                    stats["llm_secs"] += float((more_stats or {}).get("llm_secs", 0.0))
                    cleaned = cleaned2

                # final safety: inject chart blocks if model forgot
                if chart_blocks:
                    cleaned = _inject_chart_blocks(cleaned, chart_blocks)

                return cleaned, stats

            # empty output: retry with nudge
            cur_messages.append(
                {
                    "role": "system",
                    "content": "你刚才没有输出可交付正文。请直接输出最终可交付答案（不能为空）。",
                }
            )

        return "", stats

    async def _auto_continue(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        base_model: str,
        cur_messages: List[dict],
        metadata: Dict[str, Any],
        forward_opts: Dict[str, Any],
        partial: str,
        max_turns: int,
        require_marker: Optional[str] = None,
    ) -> Tuple[str, Dict[str, Any]]:
        """Auto-continue when the model output is truncated (finish_reason=length).

        v12.6.16.2 enhancement:
        - Medium outputs: keep classic "continue" stitching (append new chunk).
        - Ultra-long outputs: optionally switch to "patch-continue" where the model emits
          patch ops (append/replace/insert) and we apply them to the current text. This
          reduces drift / missing parts compared to full regeneration.

        Other features (best-effort; must never block delivery):
        - Emits per-chunk progress to the status bar.
        - Optional per-chunk summary using a dedicated model (or judge_model).
        """

        stats: Dict[str, Any] = {
            "llm_calls": 0,
            "llm_secs": 0.0,
            "continued_chunks": 0,
            "mode": "continue",
        }

        text = partial or ""
        if max_turns <= 0:
            return text, stats

        # already complete
        if require_marker and _marker_literal_is_terminal(text, require_marker):
            return text, stats

        # ---- per-chat config (written by router filter) ----
        dr = _get_dr_dict(metadata) if isinstance(metadata, dict) else {}
        cfg = dr.get("config") if isinstance(dr.get("config"), dict) else {}

        # ---- progress summary settings ----
        try:
            prog_enabled = _as_bool(cfg.get("progress_summary_enabled"), False)
        except Exception:
            prog_enabled = False

        try:
            prog_every_n = max(
                1, _as_int(cfg.get("progress_summary_every_n_continues"), 1)
            )
        except Exception:
            prog_every_n = 1

        try:
            prog_max_chars = max(
                400, _as_int(cfg.get("progress_summary_prompt_max_chars"), 2500)
            )
        except Exception:
            prog_max_chars = 2500

        try:
            prog_max_tokens = max(
                64, _as_int(cfg.get("progress_summary_max_tokens"), 220)
            )
        except Exception:
            prog_max_tokens = 220

        prog_model = str(cfg.get("progress_summary_model") or "").strip()
        if not prog_model:
            prog_model = str(cfg.get("judge_model") or "").strip() or base_model

        # ---- continue context trimming ----
        try:
            ctx_max_chars = _as_int(cfg.get("continue_context_max_chars"), 0)
        except Exception:
            ctx_max_chars = 0
        if ctx_max_chars and ctx_max_chars < 2000:
            ctx_max_chars = 2000

        # ---- ultra-long patch-continue settings ----
        try:
            ultra_enabled = _as_bool(
                cfg.get("ultra_long_patch_continue_enabled"),
                bool(getattr(self.valves, "ULTRA_LONG_PATCH_CONTINUE_ENABLED", True)),
            )
        except Exception:
            ultra_enabled = bool(
                getattr(self.valves, "ULTRA_LONG_PATCH_CONTINUE_ENABLED", True)
            )

        try:
            ultra_min_chars = max(
                0,
                _as_int(
                    cfg.get("ultra_long_patch_continue_min_chars"),
                    int(
                        getattr(
                            self.valves, "ULTRA_LONG_PATCH_CONTINUE_MIN_CHARS", 30000
                        )
                    ),
                ),
            )
        except Exception:
            ultra_min_chars = int(
                getattr(self.valves, "ULTRA_LONG_PATCH_CONTINUE_MIN_CHARS", 30000)
            )

        try:
            ultra_switch_after_n = max(
                0,
                _as_int(
                    cfg.get("ultra_long_patch_continue_switch_after_n"),
                    int(
                        getattr(
                            self.valves, "ULTRA_LONG_PATCH_CONTINUE_SWITCH_AFTER_N", 2
                        )
                    ),
                ),
            )
        except Exception:
            ultra_switch_after_n = int(
                getattr(self.valves, "ULTRA_LONG_PATCH_CONTINUE_SWITCH_AFTER_N", 2)
            )

        try:
            ultra_target_chars = max(
                1000,
                _as_int(
                    cfg.get("ultra_long_patch_continue_target_chunk_chars"),
                    int(
                        getattr(
                            self.valves,
                            "ULTRA_LONG_PATCH_CONTINUE_TARGET_CHUNK_CHARS",
                            12000,
                        )
                    ),
                ),
            )
        except Exception:
            ultra_target_chars = int(
                getattr(
                    self.valves, "ULTRA_LONG_PATCH_CONTINUE_TARGET_CHUNK_CHARS", 12000
                )
            )

        try:
            ultra_retry = max(
                1,
                _as_int(
                    cfg.get("ultra_long_patch_continue_retry_per_turn"),
                    int(
                        getattr(
                            self.valves, "ULTRA_LONG_PATCH_CONTINUE_RETRY_PER_TURN", 3
                        )
                    ),
                ),
            )
        except Exception:
            ultra_retry = int(
                getattr(self.valves, "ULTRA_LONG_PATCH_CONTINUE_RETRY_PER_TURN", 3)
            )

        ultra_model = str(cfg.get("ultra_long_patch_continue_model") or "").strip()
        if not ultra_model:
            ultra_model = str(
                getattr(self.valves, "ULTRA_LONG_PATCH_CONTINUE_MODEL", "") or ""
            ).strip()
        if not ultra_model:
            ultra_model = str(cfg.get("judge_model") or "").strip() or base_model

        if __event_emitter__:
            await self._status(
                __event_emitter__, ui_mode, sid, "✍️ 输出较长，自动续写拼接…", done=False
            )

        # Summary helper (model-based, short JSON)
        summary_sys = (
            "你是 Deep Research 的‘续写进度摘要器’。用户只能看到状态栏滚动信息。\n"
            "请根据输入的 chunk_excerpt，用中文给出：\n"
            "1) summary：本段新增内容的极短摘要（<=80字）\n"
            "2) progress：进度句（<=80字，包含第几段/总段数、累计字符、EOF/代码围栏状态等）\n"
            '严格输出 JSON：{"summary":"...","progress":"..."}，不要输出多余文本。'
        )

        # Patch-continue system prompt (JSON only)
        patch_sys = (
            "你是 Deep Research 的‘超长交付物补丁续写器’。\n"
            "目标：在不重写已有内容的前提下，把交付物继续写完整。\n"
            '你必须只输出 JSON：{"patches":[...],"done":true/false,"notes":""}。\n'
            "patches 是一组操作（尽量少、尽量稳）：\n"
            '- {"op":"append","insert":"..."}  # 追加（最常用）\n'
            '- {"op":"replace","find":"...","replace":"...","occurrence":1}  # 仅用于修补末尾截断/闭合标签/闭合代码围栏\n'
            "- 也允许 insert_after/insert_before/delete/prepend，但尽量不用。\n"
            "规则：\n"
            "1) 主要使用 append，目标追加约 target_chunk_chars 字符（可略多/略少）。\n"
            "2) 不要重复 tail_excerpt 中已经出现的内容，不要整篇重写。\n"
            "3) 只有当你确信交付物已完整时，才把 require_marker（如果提供）追加在最后。\n"
            "4) 不要输出三引号代码围栏，不要输出解释文字，只输出 JSON。"
        )

        last_fr = ""
        for i in range(max_turns):
            chunk_no = i + 1
            stats["continued_chunks"] = int(chunk_no)

            # decide mode for this chunk
            use_patch = bool(
                ultra_enabled
                and (ultra_min_chars > 0)
                and (len(text) >= ultra_min_chars)
                and (chunk_no > int(ultra_switch_after_n))
            )

            if use_patch:
                stats["mode"] = "patch_continue"

                if __event_emitter__:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"🧩 补丁续写 {chunk_no}/{max_turns}：生成补丁并应用（累计 {len(text)} 字符）",
                        done=False,
                    )

                feedback_obj = None
                success = False
                inserted_text = ""

                # For patch-continue we only provide head/tail excerpts to keep prompt bounded.
                head_excerpt = text[:800] if len(text) > 800 else text
                tail_excerpt = text[-2200:] if len(text) > 2200 else text

                for attempt in range(1, int(ultra_retry) + 1):
                    assistant_ctx = text
                    ctx_note = ""
                    if ctx_max_chars and len(assistant_ctx) > ctx_max_chars:
                        assistant_ctx = _smart_truncate_middle(
                            assistant_ctx, max_chars=ctx_max_chars
                        )
                        ctx_note = (
                            "注意：为控制上下文长度，已输出内容仅提供了节选（包含开头+结尾，中间省略）。"
                            "补丁续写时不要重复节选中的内容；请在此基础上继续把交付物写完整。"
                        )

                    payload = {
                        "kind": "patch_continue",
                        "chunk_no": int(chunk_no),
                        "max_turns": int(max_turns),
                        "current_total_chars": int(len(text)),
                        "target_chunk_chars": int(ultra_target_chars),
                        "require_marker": str(require_marker or ""),
                        "code_fence_unbalanced": bool(_code_fence_unbalanced(text)),
                        "head_excerpt": head_excerpt,
                        "tail_excerpt": tail_excerpt,
                        "instructions": (
                            "请输出 patches（以 append 为主）把交付物继续写下去。"
                            "如果需要修补末尾截断，可用 replace。"
                        ),
                    }
                    if feedback_obj:
                        payload["last_apply_feedback"] = feedback_obj

                    msgs = deepcopy(cur_messages)
                    # Provide current output as assistant context (possibly truncated)
                    msgs.append({"role": "assistant", "content": assistant_ctx})
                    msgs.append({"role": "system", "content": patch_sys})
                    if ctx_note:
                        msgs.append({"role": "system", "content": ctx_note})
                    if require_marker:
                        msgs.append(
                            {
                                "role": "system",
                                "content": (
                                    f"完成判定：只有当交付内容已经完整时，才把标记 {require_marker} 追加在最后；"
                                    "未完成时不要输出该标记。"
                                ),
                            }
                        )
                    msgs.append(
                        {
                            "role": "user",
                            "content": json.dumps(payload, ensure_ascii=False),
                        }
                    )

                    t0 = _now()
                    res = await self._call_llm(
                        __request__,
                        user_obj,
                        ultra_model,
                        msgs,
                        metadata,
                        forward_opts={
                            "tool_choice": "none",
                            "parallel_tool_calls": False,
                        },
                        temperature=0.2,
                        tool_choice_override="none",
                        tools_override=[],
                    )
                    stats["llm_secs"] += _now() - t0
                    stats["llm_calls"] += 1

                    resp_txt = str(_extract_text(res) or "").strip()
                    obj = _extract_json_object(resp_txt)
                    if not isinstance(obj, dict):
                        feedback_obj = {
                            "error": "invalid_json",
                            "raw_excerpt": _smart_truncate_middle(
                                resp_txt, max_chars=800
                            ),
                        }
                        if __event_emitter__:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                f"🧩 补丁续写 {chunk_no}/{max_turns}：补丁JSON无效，重试 {attempt}/{ultra_retry}",
                                done=False,
                            )
                        continue

                    patches = obj.get("patches")
                    if not isinstance(patches, list) or not patches:
                        # accept some alternative keys (just in case)
                        alt = obj.get("ops") or obj.get("edits") or obj.get("patch")
                        if isinstance(alt, list) and alt:
                            patches = alt
                        else:
                            feedback_obj = {
                                "error": "missing_patches",
                                "obj_keys": list(obj.keys())[:30],
                            }
                            if __event_emitter__:
                                await self._status(
                                    __event_emitter__,
                                    ui_mode,
                                    sid,
                                    f"🧩 补丁续写 {chunk_no}/{max_turns}：未返回 patches，重试 {attempt}/{ultra_retry}",
                                    done=False,
                                )
                            continue

                    policy = {
                        "enable_ws_regex": True,
                        "enable_fuzzy": True,
                        "fuzzy_min_ratio": 0.985,
                    }
                    new_text2, applied_logs, skipped_logs = _apply_code_patches(
                        text, patches, policy=policy
                    )

                    if not applied_logs:
                        feedback_obj = _build_patch_apply_feedback(
                            text,
                            patches,
                            applied_logs,
                            skipped_logs,
                            notes="no_patch_applied",
                            policy=policy,
                        )
                        if __event_emitter__:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                f"🧩 补丁续写 {chunk_no}/{max_turns}：补丁未命中锚点/未应用，重试 {attempt}/{ultra_retry}",
                                done=False,
                            )
                        continue

                    # success
                    inserted_parts = []
                    try:
                        for p in patches:
                            if not isinstance(p, dict):
                                continue
                            op = str(p.get("op") or "").strip().lower()
                            if op in (
                                "append",
                                "prepend",
                                "insert_after",
                                "insert_before",
                            ):
                                ins = p.get("insert")
                                if isinstance(ins, str) and ins:
                                    inserted_parts.append(ins)
                    except Exception:
                        pass
                    inserted_text = "".join(inserted_parts)

                    text = new_text2
                    success = True

                    done_flag = _as_bool(obj.get("done"), False) or _as_bool(
                        obj.get("complete"), False
                    )
                    last_fr = "patch_done" if done_flag else "patch"

                    if __event_emitter__:
                        marker_note = ""
                        if require_marker:
                            marker_note = (
                                "；EOF=✅"
                                if _marker_literal_is_terminal(text, require_marker)
                                else "；EOF=⏳"
                            )
                        fence_note = (
                            "；代码围栏=⚠️" if _code_fence_unbalanced(text) else ""
                        )
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            (
                                f"✅ 补丁续写 {chunk_no}/{max_turns} 应用成功："
                                f"应用 {len(applied_logs)} 条，跳过 {len(skipped_logs)} 条；"
                                f"新增≈{len(inserted_text)} 字符，累计 {len(text)} 字符（finish={last_fr}{marker_note}{fence_note}）"
                            ),
                            done=False,
                        )
                    break

                if not success:
                    if __event_emitter__:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"⚠️ 补丁续写 {chunk_no}/{max_turns}：多次重试仍失败。你可以回复“继续”让我再试，或要求改为普通续写。",
                            done=False,
                        )
                    break

                # optional summary for front-end status (use inserted_text)
                if (
                    prog_enabled
                    and __event_emitter__
                    and prog_model
                    and (prog_every_n > 0)
                    and ((chunk_no % prog_every_n) == 0)
                    and inserted_text
                ):
                    try:
                        payload = {
                            "kind": "patch_continue",
                            "chunk_no": int(chunk_no),
                            "max_turns": int(max_turns),
                            "new_chars": int(len(inserted_text)),
                            "total_chars": int(len(text)),
                            "finish_reason": str(last_fr or ""),
                            "has_eof": (
                                bool(_marker_literal_is_terminal(text, require_marker))
                                if require_marker
                                else None
                            ),
                            "code_fence_unbalanced": bool(_code_fence_unbalanced(text)),
                            "chunk_excerpt": _smart_truncate_middle(
                                inserted_text, max_chars=prog_max_chars
                            ),
                        }
                        internal_md_sum = self._build_internal_metadata(
                            metadata, base_model, "continue_progress_summary", 0, sid
                        )
                        t1 = _now()
                        sum_res = await self._call_llm(
                            __request__,
                            user_obj,
                            prog_model,
                            [
                                {"role": "system", "content": summary_sys},
                                {
                                    "role": "user",
                                    "content": json.dumps(payload, ensure_ascii=False),
                                },
                            ],
                            internal_md_sum,
                            forward_opts={
                                "tool_choice": "none",
                                "parallel_tool_calls": False,
                                "max_tokens": prog_max_tokens,
                            },
                            temperature=0.2,
                            tool_choice_override="none",
                            tools_override=[],
                        )
                        stats["llm_secs"] += _now() - t1
                        stats["llm_calls"] += 1

                        sum_txt = str(_extract_text(sum_res) or "").strip()
                        j = _safe_json_loads(sum_txt) if sum_txt else None
                        if isinstance(j, dict):
                            s1 = str(j.get("summary") or "").strip()
                            s2 = str(j.get("progress") or "").strip()
                            if s1:
                                await self._status(
                                    __event_emitter__,
                                    ui_mode,
                                    sid,
                                    f"🧾 续写摘要：{s1}",
                                    done=False,
                                )
                            if s2:
                                await self._status(
                                    __event_emitter__,
                                    ui_mode,
                                    sid,
                                    f"📈 {s2}",
                                    done=False,
                                )
                    except Exception:
                        pass

                # stop condition for patch-continue: done flag + heuristics
                if (
                    (last_fr == "patch_done")
                    and (not _code_fence_unbalanced(text))
                    and (
                        not require_marker
                        or _marker_literal_is_terminal(text, require_marker)
                    )
                ):
                    break

                # continue next chunk
                continue

            # ----------------------------
            # Classic continue (append new chunk)
            # ----------------------------
            if __event_emitter__:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"✍️ 续写 {chunk_no}/{max_turns}：正在生成下一段（累计 {len(text)} 字符）",
                    done=False,
                )

            tail = text[-2000:] if len(text) > 2000 else text
            msgs = deepcopy(cur_messages)

            assistant_ctx = text
            ctx_note = ""
            if ctx_max_chars and len(assistant_ctx) > ctx_max_chars:
                assistant_ctx = _smart_truncate_middle(
                    assistant_ctx, max_chars=ctx_max_chars
                )
                ctx_note = (
                    "注意：为控制上下文长度，已输出内容仅提供了节选（包含开头+结尾，中间省略）。"
                    "续写时不要重复节选中的内容；请在此基础上继续把交付物写完整。"
                )

            msgs.append({"role": "assistant", "content": assistant_ctx})
            msgs.append(
                {"role": "system", "content": self.valves.CONTINUE_SYSTEM_PROMPT}
            )
            if ctx_note:
                msgs.append({"role": "system", "content": ctx_note})

            if require_marker:
                msgs.append(
                    {
                        "role": "system",
                        "content": (
                            f"续写时的完成判定：当且仅当你已把交付内容输出完整，"
                            f"请在合适位置（通常是最后）加入标记 {require_marker}（作为注释/安全标记）。"
                            "如果还没写完，不要输出该标记。"
                        ),
                    }
                )

            msgs.append(
                {
                    "role": "user",
                    "content": "请从这里继续（仅供定位，不要重复输出这段）：\n" + tail,
                }
            )

            t0 = _now()
            res = await self._call_llm(
                __request__,
                user_obj,
                base_model,
                msgs,
                metadata,
                forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
                temperature=0.3,
                tool_choice_override="none",
                tools_override=[],
            )
            stats["llm_secs"] += _now() - t0
            stats["llm_calls"] += 1

            last_fr = _extract_finish_reason(res)
            add = _extract_text(res).strip()
            if not add:
                if __event_emitter__:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"⚠️ 续写 {chunk_no}/{max_turns}：模型未返回新增文本，已停止自动续写",
                        done=False,
                    )
                break

            # naive de-dup: if model repeats tail, remove overlap
            if tail and add.startswith(tail[:200]):
                add = add[len(tail[:200]) :].lstrip()

            text = (text.rstrip() + "\n" + add.lstrip()).strip()

            if __event_emitter__:
                marker_note = ""
                if require_marker:
                    marker_note = (
                        "；EOF=✅"
                        if _marker_literal_is_terminal(text, require_marker)
                        else "；EOF=⏳"
                    )
                fence_note = "；代码围栏=⚠️" if _code_fence_unbalanced(text) else ""
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    (
                        f"✅ 续写 {chunk_no}/{max_turns} 完成：新增 {len(add)} 字符，"
                        f"累计 {len(text)} 字符（finish={last_fr}{marker_note}{fence_note}）"
                    ),
                    done=False,
                )

            # optional summary for front-end status
            if (
                prog_enabled
                and __event_emitter__
                and prog_model
                and (prog_every_n > 0)
                and ((chunk_no % prog_every_n) == 0)
            ):
                try:
                    payload = {
                        "kind": "auto_continue",
                        "chunk_no": int(chunk_no),
                        "max_turns": int(max_turns),
                        "new_chars": len(add),
                        "total_chars": len(text),
                        "finish_reason": str(last_fr or ""),
                        "has_eof": (
                            bool(_marker_literal_is_terminal(text, require_marker))
                            if require_marker
                            else None
                        ),
                        "code_fence_unbalanced": bool(_code_fence_unbalanced(text)),
                        "chunk_excerpt": _smart_truncate_middle(
                            add, max_chars=prog_max_chars
                        ),
                    }

                    internal_md_sum = self._build_internal_metadata(
                        metadata, base_model, "continue_progress_summary", 0, sid
                    )
                    t1 = _now()
                    sum_res = await self._call_llm(
                        __request__,
                        user_obj,
                        prog_model,
                        [
                            {"role": "system", "content": summary_sys},
                            {
                                "role": "user",
                                "content": json.dumps(payload, ensure_ascii=False),
                            },
                        ],
                        internal_md_sum,
                        forward_opts={
                            "tool_choice": "none",
                            "parallel_tool_calls": False,
                            "max_tokens": prog_max_tokens,
                        },
                        temperature=0.2,
                        tool_choice_override="none",
                        tools_override=[],
                    )
                    # Count summary calls into timing stats (more truthful).
                    stats["llm_secs"] += _now() - t1
                    stats["llm_calls"] += 1

                    sum_txt = str(_extract_text(sum_res) or "").strip()
                    j = _safe_json_loads(sum_txt) if sum_txt else None
                    if isinstance(j, dict):
                        s1 = str(j.get("summary") or "").strip()
                        s2 = str(j.get("progress") or "").strip()
                        if s1:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                f"🧾 续写摘要：{s1}",
                                done=False,
                            )
                        if s2:
                            await self._status(
                                __event_emitter__, ui_mode, sid, f"📈 {s2}", done=False
                            )
                except Exception:
                    pass

            # stop condition for classic continue
            if (
                last_fr != "length"
                and not _code_fence_unbalanced(text)
                and (
                    not require_marker
                    or _marker_literal_is_terminal(text, require_marker)
                )
            ):
                break

        # If we exhausted max_turns but still looks truncated, warn.
        if __event_emitter__:
            if (
                last_fr == "length"
                or _code_fence_unbalanced(text)
                or (
                    require_marker
                    and (not _marker_literal_is_terminal(text, require_marker))
                )
            ):
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"⚠️ 已达到自动续写上限（{max_turns}）。如仍未完整，你可以回复“继续”让我再续写。",
                    done=False,
                )

        return text, stats

    # ----------------------------
    # Step outputs + final compile
    # ----------------------------

    def _get_step_output_draft(
        self,
        sess: Dict[str, Any],
        step_idx: int,
        max_chars: int = 6000,
    ) -> str:
        """Get stored draft content for the given step.

        Why:
        - When we iterate within the same step, we want the model to output a full improved
          version (not just a short patch), so we provide the current draft as context.
        - We DO NOT blindly feed "last answer" across different steps because that can make
          later rounds look like a restart / redo.
        """
        if not isinstance(sess, dict):
            return ""
        lst = sess.get("step_outputs")
        if not isinstance(lst, list):
            return ""
        try:
            sidx = int(step_idx)
        except Exception:
            sidx = step_idx

        for it in lst:
            if not isinstance(it, dict):
                continue
            try:
                if int(it.get("step_idx", -1)) != int(sidx):
                    continue
            except Exception:
                continue
            c = str(it.get("content") or "").strip()
            if not c:
                return ""
            if max_chars and max_chars > 0 and len(c) > int(max_chars):
                return c[-int(max_chars) :]
            return c
        return ""

    def _upsert_step_output(
        self,
        sess: Dict[str, Any],
        step_idx: int,
        step_head: str,
        step_title: str,
        content: str,
    ) -> None:
        """Store per-step outputs so the final report can be compiled comprehensively.

        Key behavior (important for "iteration" to feel real):
        - If a later round only outputs a short patch, we MERGE it into the existing draft
          instead of overwriting and making the final report shorter.
        - If the later round outputs a full rewrite (similar length), we replace.
        """
        if not isinstance(sess, dict):
            return
        content = (content or "").strip()
        if not content:
            return

        lst = sess.get("step_outputs")
        if not isinstance(lst, list):
            lst = []

        title = (step_title or "").strip() or (step_head or f"Step {int(step_idx)+1}")
        head = (step_head or "").strip() or title

        # find by title (preferred)
        found = None
        for it in lst:
            if isinstance(it, dict) and str(it.get("title") or "").strip() == title:
                found = it
                break

        if found is None:
            lst.append(
                {
                    "step_idx": int(step_idx),
                    "title": title,
                    "head": head,
                    "content": content,
                    "updated_at": _now(),
                }
            )
        else:
            prev = str(found.get("content") or "").strip()

            if not prev:
                merged = content
            else:
                # Heuristic: if new content is close in size, treat it as a full rewrite.
                prev_len = len(prev)
                new_len = len(content)
                is_full_rewrite = new_len >= int(prev_len * 0.75)

                if is_full_rewrite:
                    merged = content
                else:
                    # Likely a short patch; merge it if it's not already included.
                    if content in prev:
                        merged = prev
                    else:
                        merged = prev.rstrip() + "\n\n【补充更新】\n" + content

            found.update(
                {
                    "step_idx": int(step_idx),
                    "title": title,
                    "head": head,
                    "content": merged.strip(),
                    "updated_at": _now(),
                }
            )

        # stable sort by step order
        try:
            lst.sort(
                key=lambda x: int(x.get("step_idx", 0)) if isinstance(x, dict) else 0
            )
        except Exception:
            pass

        sess["step_outputs"] = lst

    def _build_compile_payload(
        self,
        scoped: Dict[str, Any],
        plan_obj: Dict[str, Any],
        sess: Dict[str, Any],
        raw_final: str,
        time_context: str,
        step_output_max_chars: int,
    ) -> Dict[str, Any]:
        plan_list = plan_obj.get("plan") if isinstance(plan_obj, dict) else None
        if not isinstance(plan_list, list):
            plan_list = []

        step_outputs = sess.get("step_outputs") if isinstance(sess, dict) else None
        if not isinstance(step_outputs, list):
            step_outputs = []

        # trim step outputs to keep compile context bounded
        trimmed: List[dict] = []
        for it in step_outputs:
            if not isinstance(it, dict):
                continue
            c = str(it.get("content") or "")
            if step_output_max_chars > 0 and len(c) > step_output_max_chars:
                c = c[: step_output_max_chars - 1] + "…"
            trimmed.append(
                {
                    "step_idx": int(it.get("step_idx", 0) or 0),
                    "title": str(it.get("title") or ""),
                    "head": str(it.get("head") or ""),
                    "content": c,
                }
            )

        payload = {
            "target": str(scoped.get("target") or ""),
            "goal": str(scoped.get("goal") or ""),
            "assumptions": (
                scoped.get("assumptions")
                if isinstance(scoped.get("assumptions"), list)
                else []
            ),
            "focus": (
                scoped.get("focus") if isinstance(scoped.get("focus"), list) else []
            ),
            "plan": plan_list,
            "step_outputs": trimmed,
            "best_answer_so_far": (raw_final or "").strip(),
            "time_context": (time_context or "").strip(),
            "chart_blocks_count": (
                len(sess.get("chart_blocks") or [])
                if isinstance(sess, dict) and isinstance(sess.get("chart_blocks"), list)
                else 0
            ),
        }
        return payload

    async def _compile_final_report(
        self,
        __request__,
        __event_emitter__,
        ui_mode: str,
        sid: str,
        user_obj,
        compile_model: str,
        metadata_in: Dict[str, Any],
        scoped: Dict[str, Any],
        plan_obj: Dict[str, Any],
        sess: Dict[str, Any],
        time_context: str,
        raw_final: str,
        compile_max_continue_turns: int,
        final_min_chars: int,
        step_output_max_chars: int,
    ) -> Tuple[str, Dict[str, Any]]:
        """Finalize the best deliverable from per-step outputs (tools disabled)."""
        stats = {"llm_calls": 0, "llm_secs": 0.0}

        await self._status(
            __event_emitter__,
            ui_mode,
            sid,
            "🧩 汇总阶段：正在把各步骤内容整合成最终交付物…",
            done=False,
        )

        # Decide deliverable style (report / code / checklist / mixed...) so we can apply
        # stronger completion rules for large code.
        deliverable_spec = await self._deliverable_spec(
            __request__,
            __event_emitter__,
            ui_mode,
            sid,
            user_obj,
            compile_model,
            metadata_in,
            str(sess.get("user_text") or "").strip(),
            scoped,
            plan_obj,
        )
        sess["deliverable_spec"] = deliverable_spec
        code_lang = str(deliverable_spec.get("code_language") or "").strip() or "text"
        need_eof = bool(deliverable_spec.get("need_eof_marker"))
        eof_marker = (
            str(
                deliverable_spec.get("eof_marker")
                or self.valves.DEFAULT_CODE_EOF_MARKER
            ).strip()
            or self.valves.DEFAULT_CODE_EOF_MARKER
        )
        eof_line = (
            _format_eof_marker_line(eof_marker, code_lang)
            if (need_eof and eof_marker)
            else ""
        )
        require_marker_line = eof_line.strip() if (need_eof and eof_line) else None
        if need_eof:
            compile_max_continue_turns = max(
                int(compile_max_continue_turns or 0),
                int(self.valves.DEFAULT_COMPILE_MAX_OUTPUT_CONTINUE_TURNS_CODE),
            )

        # 对代码类交付物：不要为了“凑字数”而二次扩写，避免破坏可复制运行的代码
        final_min_chars_eff = 0 if need_eof else int(final_min_chars or 0)

        payload = self._build_compile_payload(
            scoped=scoped,
            plan_obj=plan_obj,
            sess=sess,
            raw_final=raw_final,
            time_context=time_context,
            step_output_max_chars=step_output_max_chars,
        )

        msgs: List[dict] = [
            {"role": "system", "content": self.valves.COMPILE_SYSTEM_PROMPT}
        ]
        if time_context:
            msgs.append({"role": "system", "content": time_context.strip()})
        if isinstance(deliverable_spec, dict) and deliverable_spec:
            dt = str(deliverable_spec.get("deliverable_type") or "").strip() or "auto"
            cl = str(deliverable_spec.get("code_language") or "").strip() or "-"
            full_code = bool(deliverable_spec.get("must_include_full_code"))
            single_file = bool(deliverable_spec.get("is_single_file"))
            cp = bool(deliverable_spec.get("must_be_copy_paste"))

            spec_line = (
                f"【预期最终交付】type={dt}"
                f"；full_code={full_code}"
                f"；single_file={single_file}"
                f"；code_language={cl}"
                f"；copy_paste={cp}"
            )
            if need_eof and eof_marker:
                spec_line += f"；eof_marker={eof_marker}"

            # Strong but short instruction to avoid report-style when code is required.
            if full_code:
                spec_line += "。最终必须输出【完整可运行代码】（优先给代码，说明不超过10行；不要写长报告/长研报）。"
            else:
                spec_line += "。请严格按该形态输出最终交付物。"

            if need_eof and eof_marker:
                spec_line += (
                    "代码文件末尾请按语言注释形式加入 eof_marker 作为结束标记。"
                )

            msgs.append({"role": "system", "content": spec_line})

            # Extra requirement hints (model-decided) to avoid over-simplification.
            req_summary = str(
                deliverable_spec.get("requirements_summary") or ""
            ).strip()
            must_have = deliverable_spec.get("must_have") or []
            complexity = str(deliverable_spec.get("complexity") or "").strip()
            build_workflow = str(deliverable_spec.get("build_workflow") or "").strip()
            if req_summary:
                msgs.append({"role": "system", "content": f"【需求摘要】{req_summary}"})
            if isinstance(must_have, list) and must_have:
                # Keep it short to avoid overloading the context.
                mh = "\n".join(
                    f"- {str(x).strip()}" for x in must_have[:12] if str(x).strip()
                )
                if mh.strip():
                    msgs.append(
                        {"role": "system", "content": "【必须包含/实现】\n" + mh}
                    )
            if complexity or build_workflow:
                msgs.append(
                    {
                        "role": "system",
                        "content": (
                            "【生成策略】"
                            f"complexity={complexity or 'auto'}；build_workflow={build_workflow or 'auto'}。"
                            "若内容较大，可在内部多轮生成后拼接/补丁修复，但最终必须一次性输出完整交付物。"
                        ),
                    }
                )

        # If charts exist, remind the model to place placeholders.
        charts = sess.get("chart_blocks") if isinstance(sess, dict) else None
        if isinstance(charts, list) and charts:
            hint = _build_chart_render_system_hint(charts)
            if hint:
                msgs.append({"role": "system", "content": hint})

        msgs.append(
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)}
        )

        internal_md = self._build_internal_metadata(
            metadata_in, compile_model, "compile", 0, sid
        )

        t0 = _now()
        res = await self._call_llm(
            __request__,
            user_obj,
            compile_model,
            msgs,
            internal_md,
            forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
            temperature=0.25,
            tool_choice_override="none",
            tools_override=[],
        )
        stats["llm_secs"] += _now() - t0
        stats["llm_calls"] += 1

        out = _extract_text(res)
        fr = _extract_finish_reason(res)

        # Continue stitching if truncated / code fence broken / needs EOF marker.
        need_stitch = False
        if out:
            if fr == "length" or _code_fence_unbalanced(out):
                need_stitch = True
            elif require_marker_line and not _marker_literal_is_terminal(
                out, require_marker_line
            ):
                # 代码交付物：即使 finish_reason=stop，也要确保拿到完整文件
                need_stitch = True

        if need_stitch:
            await self._status(
                __event_emitter__,
                ui_mode,
                sid,
                "✍️ 输出较长，自动续写拼接…",
                done=False,
            )
            out2, more = await self._auto_continue(
                __request__,
                __event_emitter__,
                ui_mode,
                sid,
                user_obj,
                compile_model,
                msgs,
                internal_md,
                forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
                partial=out,
                max_turns=compile_max_continue_turns,
                require_marker=require_marker_line,
            )
            out = out2
            stats["llm_calls"] += int(more.get("llm_calls", 0))
            stats["llm_secs"] += float(more.get("llm_secs", 0.0))

        # Insert chart blocks. For code-only deliverables, only replace placeholders to avoid polluting code.
        if isinstance(charts, list) and charts:
            if (not need_eof) or ("[[CHART" in (out or "")):
                out = _inject_chart_blocks(out or "", charts)

        # If still too short, force one extra rewrite/expand pass (bounded to 1).
        if (
            final_min_chars_eff > 0
            and isinstance(out, str)
            and len(out.strip()) < final_min_chars_eff
        ):
            await self._status(
                __event_emitter__,
                ui_mode,
                sid,
                "📌 最终交付偏短：继续补全扩写（不省略任何步骤）…",
                done=False,
            )

            expand_msgs = [
                {"role": "system", "content": self.valves.COMPILE_SYSTEM_PROMPT},
            ]
            if time_context:
                expand_msgs.append({"role": "system", "content": time_context.strip()})
            if isinstance(charts, list) and charts:
                hint = _build_chart_render_system_hint(charts)
                if hint:
                    expand_msgs.append({"role": "system", "content": hint})

            expand_msgs.append(
                {"role": "user", "content": json.dumps(payload, ensure_ascii=False)}
            )
            expand_msgs.append({"role": "assistant", "content": out})
            expand_msgs.append(
                {
                    "role": "system",
                    "content": (
                        "你刚才的最终交付物过短/信息密度不足。请在不依赖任何外部工具的前提下：\\n"
                        "- 按计划逐步补全所有章节（每步至少包含：关键事实/数据口径/结论/风险或验证点）。\\n"
                        "- 增加可执行的决策参考（情景、仓位、止损/止盈、跟踪指标）。\\n"
                        "- 输出【完整最终交付物】（重写融合，而不是只补一小段）。\\n"
                        "注意：不要把图表占位符或图表 Markdown 放进代码块。"
                    ),
                }
            )

            internal_md2 = self._build_internal_metadata(
                metadata_in, compile_model, "compile_expand", 0, sid
            )
            t1 = _now()
            res2 = await self._call_llm(
                __request__,
                user_obj,
                compile_model,
                expand_msgs,
                internal_md2,
                forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
                temperature=0.25,
                tool_choice_override="none",
                tools_override=[],
            )
            stats["llm_secs"] += _now() - t1
            stats["llm_calls"] += 1

            out2 = _extract_text(res2)
            fr2 = _extract_finish_reason(res2)
            if out2 and (fr2 == "length" or _code_fence_unbalanced(out2)):
                out3, more2 = await self._auto_continue(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    compile_model,
                    expand_msgs,
                    internal_md2,
                    forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
                    partial=out2,
                    max_turns=compile_max_continue_turns,
                )
                out2 = out3
                stats["llm_calls"] += int(more2.get("llm_calls", 0))
                stats["llm_secs"] += float(more2.get("llm_secs", 0.0))

            if isinstance(charts, list) and charts:
                out2 = _inject_chart_blocks(out2 or "", charts)

            if out2 and len(out2.strip()) >= len((out or "").strip()):
                out = out2

        out = (out or "").strip()
        if isinstance(charts, list) and charts:
            await self._status(
                __event_emitter__,
                ui_mode,
                sid,
                f"📊 已生成图表：{len(charts)} 个（最终将自动插入）",
                done=False,
            )

        return out, stats

    # ----------------------------
    # Code deliverable finalization (stitch review & patch)
    # ----------------------------
    async def _finalize_code_deliverable(
        self,
        __request__,
        user_obj: dict,
        model: str,
        metadata_in: dict,
        scoped: Optional[dict],
        __event_emitter__,
        ui_mode: str,
        sid: str,
        deliverable_spec: Dict[str, Any],
        text: str,
    ) -> str:
        """Extract best code from mixed output, repair stitching issues via patches, and wrap into a single fenced block."""
        raw = text or ""
        lang = str(deliverable_spec.get("code_language") or "text").strip() or "text"
        eof_marker = str(deliverable_spec.get("eof_marker") or "").strip()

        picked_lang, code = _extract_code_candidate(raw, prefer_lang=lang)
        if lang == "text" and picked_lang and picked_lang != "text":
            lang = picked_lang

        code = _normalize_code_candidate(code, lang)

        # Ensure EOF marker (append as a safe comment for this language)
        if eof_marker:
            eof_line = _format_eof_marker_line(eof_marker, lang)
            code = _ensure_eof_marker(code, eof_marker, eof_line=eof_line)

        # Quick scan for stitch artifacts
        issues = _code_integrity_issues(code, lang, eof_marker)

        build_workflow = (
            str(deliverable_spec.get("build_workflow") or "auto").strip().lower()
        )
        force_review = build_workflow in {
            "patch",
            "staged_patch",
            "iterative_patch",
            "multi_patch",
            "patch_build",
            "staged",
        }

        # No hard length threshold: let workflow/issue signals decide.
        if self.valves.ENABLE_CODE_STITCH_REVIEW and (
            issues
            or force_review
            or not self.valves.CODE_STITCH_REVIEW_ONLY_WHEN_ISSUES
        ):
            code = await self._code_stitch_review_and_patch(
                __request__,
                user_obj,
                model,
                metadata_in,
                scoped,
                __event_emitter__,
                ui_mode,
                sid,
                code,
                lang,
                eof_marker,
            )

        # Final EOF marker guarantee
        if eof_marker:
            eof_line = _format_eof_marker_line(eof_marker, lang)
            code = _ensure_eof_marker(code, eof_marker, eof_line=eof_line)

        # Wrap with a single fence (force single block even if upstream produced many)
        return f"```{lang}\n{code.rstrip()}\n```"

    async def _code_stitch_review_and_patch(
        self,
        __request__,
        user_obj: dict,
        model: str,
        metadata_in: dict,
        scoped: Optional[dict],
        __event_emitter__,
        ui_mode: str,
        sid: str,
        code: str,
        lang: str,
        eof_marker: str,
    ) -> str:
        """Ask the model to generate *patches* (replace/delete/insert) to fix stitching issues, then apply patches."""

        max_passes = max(1, int(self.valves.CODE_STITCH_REVIEW_MAX_PASSES))
        prompt_max_chars = max(
            4000, int(self.valves.CODE_STITCH_REVIEW_PROMPT_MAX_CHARS)
        )

        sys_prompt = (self.valves.CODE_STITCH_REVIEW_SYSTEM_PROMPT or "").strip()
        if not sys_prompt:
            # Fallback (should rarely happen because valves has a default)
            sys_prompt = (
                "你是超长代码拼接复查器，只输出 JSON 补丁，不要输出修复后的完整代码。"
            )

        # Choose model for patching
        repair_model = (self.valves.CODE_STITCH_REVIEW_MODEL or "").strip() or model

        last_code = code
        for pass_no in range(1, max_passes + 1):
            issues = _code_integrity_issues(code, lang, eof_marker)
            if not issues:
                break

            await self._status(
                __event_emitter__,
                ui_mode,
                sid,
                f"🧩 代码拼接复查：检测到 {len(issues)} 项问题，尝试补丁修复（第 {pass_no}/{max_passes} 轮）…",
                done=False,
            )

            excerpt = _build_code_repair_excerpt(
                code,
                max_chars=prompt_max_chars,
                lang=lang,
                eof_marker=eof_marker,
            )
            user_payload = {
                "language": lang,
                "eof_marker": eof_marker,
                "issues": issues,
                "instructions": "请只输出 JSON 补丁对象，不要输出 markdown，不要输出修复后的完整代码。",
                "code": excerpt,
            }
            user_msg = json.dumps(user_payload, ensure_ascii=False)

            msgs = [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_msg},
            ]

            t0 = _now()
            internal_md = self._build_internal_metadata(
                metadata_in,
                repair_model,
                "code_stitch_review",
                pass_no,
                sid,
            )
            res = await self._call_llm(
                __request__,
                user_obj,
                repair_model,
                msgs,
                internal_md,
                forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
                temperature=0.0,
                tool_choice_override="none",
                tools_override=[],
            )
            resp_text = _extract_text(res)
            t1 = _now()

            patch_obj = _extract_json_object(resp_text)
            if not isinstance(patch_obj, dict):
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"🧩 补丁修复：模型未返回有效 JSON（已用 {t1 - t0:.1f}s），停止自动修复。",
                    done=False,
                )
                break

            patches = patch_obj.get("patches")
            if not isinstance(patches, list) or not patches:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    "🧩 补丁修复：未返回 patches，停止自动修复。",
                    done=False,
                )
                break

            new_code, applied, skipped = _apply_code_patches(code, patches)

            if applied:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"🧩 补丁修复：已应用 {len(applied)} 条补丁，跳过 {len(skipped)} 条。",
                    done=False,
                )
            else:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"🧩 补丁修复：没有补丁被应用（跳过 {len(skipped)} 条），停止自动修复。",
                    done=False,
                )
                break

            code = new_code

            # Stop if no progress
            if code == last_code:
                break
            last_code = code

        return code

    async def _final_audit_deliverable(
        self,
        __request__,
        user_obj: dict,
        model: str,
        metadata_in: dict,
        scoped: Optional[dict],
        __event_emitter__,
        ui_mode: str,
        sid: str,
        deliverable_spec: Dict[str, Any],
        final_text: str,
    ) -> str:
        """Final audit gate: for code & ultra-long deliverables, ask the model to re-check and emit patches.

        This is a generic audit that works for both code and long reports, and is designed to catch
        stitching/duplication/truncation artifacts that can slip through normal compilation.
        """

        if not _as_bool(getattr(self.valves, "ENABLE_FINAL_AUDIT", True), True):
            return final_text

        spec = deliverable_spec or {}
        mode = str(spec.get("mode") or "").strip().lower()
        dtype = str(spec.get("deliverable_type") or "").strip().lower()
        is_code = bool(spec.get("need_eof_marker")) or mode == "code" or dtype == "code"

        trigger_chars = max(
            0, int(getattr(self.valves, "FINAL_AUDIT_TRIGGER_CHARS", 0) or 0)
        )
        max_passes = max(1, int(getattr(self.valves, "FINAL_AUDIT_MAX_PASSES", 1) or 1))
        prompt_max_chars = max(
            4000,
            int(getattr(self.valves, "FINAL_AUDIT_PROMPT_MAX_CHARS", 60000) or 60000),
        )
        only_when_issues = _as_bool(
            getattr(self.valves, "FINAL_AUDIT_ONLY_WHEN_ISSUES", False), False
        )

        audit_model = (
            str(getattr(self.valves, "FINAL_AUDIT_MODEL", "") or "").strip() or model
        )
        sys_prompt = str(
            getattr(self.valves, "FINAL_AUDIT_SYSTEM_PROMPT", "") or ""
        ).strip()
        if not sys_prompt:
            sys_prompt = "你是终审审计器，只输出 JSON（pass/issues/patches/notes）。"

        text = final_text or ""

        # Decide whether to run audit.
        if is_code:
            lang = str(spec.get("code_language") or "text").strip() or "text"
            eof_marker = str(spec.get("eof_marker") or "").strip()

            blocks = _extract_fenced_code_blocks(text)
            if blocks:
                best = _pick_best_code_block(
                    blocks,
                    prefer_lang=lang,
                    file_kind=("html" if lang == "html" else "code"),
                )
                code = str(best.get("code") or "")
                bl = str(best.get("lang") or "").strip().lower()
                if (lang == "text") and bl and bl != "text":
                    lang = bl
            else:
                code = text

            code = _normalize_code_candidate(code, lang)
            if eof_marker:
                eof_line = _format_eof_marker_line(eof_marker, lang)
                code = _ensure_eof_marker(code, eof_marker, eof_line=eof_line)

            build_workflow = (
                str(deliverable_spec.get("build_workflow") or "auto").strip().lower()
            )
            complexity = (
                str(deliverable_spec.get("complexity") or "auto").strip().lower()
            )
            req_summary = str(
                deliverable_spec.get("requirements_summary") or ""
            ).strip()
            must_have = deliverable_spec.get("must_have") or []
            if isinstance(must_have, str):
                must_have = [
                    x.strip() for x in re.split(r"[\n,;，；]+", must_have) if x.strip()
                ]
            if not isinstance(must_have, list):
                must_have = []
            must_have = [str(x).strip() for x in must_have if str(x).strip()]
            force_audit = True

            issues = _code_integrity_issues(code, lang, eof_marker)
            need_audit = bool(issues) or force_audit or (not only_when_issues)
            if only_when_issues and not issues and not force_audit:
                need_audit = False
            if not need_audit:
                return final_text

            for pass_no in range(1, max_passes + 1):
                issues = _code_integrity_issues(code, lang, eof_marker)
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"🔎 终审审计：代码可用性复查（第 {pass_no}/{max_passes} 轮）…",
                    done=False,
                )
                excerpt = _build_code_repair_excerpt(
                    code,
                    max_chars=prompt_max_chars,
                    lang=lang,
                    eof_marker=eof_marker,
                )
                user_payload = {
                    "deliverable_mode": "code",
                    "deliverable_type": dtype or "code",
                    "language": lang,
                    "eof_marker": eof_marker,
                    "heuristic_issues": issues,
                    "requirements_summary": req_summary,
                    "must_have": must_have,
                    "complexity": complexity,
                    "build_workflow": build_workflow,
                    "deliverable_notes": str(
                        deliverable_spec.get("notes") or ""
                    ).strip(),
                    "instructions": "只输出 JSON。若 pass=false，请给出最小必要 patches；不要输出完整修复后代码。",
                    "content": excerpt,
                }
                msgs = [
                    {"role": "system", "content": sys_prompt},
                    {
                        "role": "user",
                        "content": json.dumps(user_payload, ensure_ascii=False),
                    },
                ]

                internal_md = self._build_internal_metadata(
                    metadata_in, audit_model, "final_audit_code", pass_no, sid
                )
                res = await self._call_llm(
                    __request__,
                    user_obj,
                    audit_model,
                    msgs,
                    internal_md,
                    forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
                    temperature=0.0,
                    tool_choice_override="none",
                    tools_override=[],
                )
                resp_text = _extract_text(res)
                obj = _extract_json_object(resp_text)
                if not isinstance(obj, dict):
                    break

                if _as_bool(obj.get("pass"), False):
                    break

                patches = obj.get("patches")
                if not isinstance(patches, list) or not patches:
                    break

                new_code, applied, _skipped = _apply_code_patches(code, patches)
                if not applied:
                    break
                code = new_code

            # Post-audit normalization
            code = _normalize_code_candidate(code, lang)
            if eof_marker:
                eof_line = _format_eof_marker_line(eof_marker, lang)
                code = _ensure_eof_marker(code, eof_marker, eof_line=eof_line)
            return f"```{lang}\n{code.rstrip()}\n```"

        # --- Report / other deliverables ---
        build_workflow = (
            str(deliverable_spec.get("build_workflow") or "auto").strip().lower()
        )
        complexity = str(deliverable_spec.get("complexity") or "auto").strip().lower()
        req_summary = str(deliverable_spec.get("requirements_summary") or "").strip()
        must_have = deliverable_spec.get("must_have") or []
        if isinstance(must_have, str):
            must_have = [
                x.strip() for x in re.split(r"[\n,;，；]+", must_have) if x.strip()
            ]
        if not isinstance(must_have, list):
            must_have = []
        must_have = [str(x).strip() for x in must_have if str(x).strip()]

        issues = _text_integrity_issues(text)
        force_audit = build_workflow in (
            "patch",
            "staged_patch",
            "iterative_patch",
            "multi_patch",
            "patch_build",
            "staged",
        ) or (complexity in ("complex", "high", "large", "huge"))
        need_audit = bool(issues) or force_audit or (not only_when_issues)
        if only_when_issues and not issues and not force_audit:
            need_audit = False
        if not need_audit:
            return final_text

        out = text
        for pass_no in range(1, max_passes + 1):
            issues = _text_integrity_issues(out)
            await self._status(
                __event_emitter__,
                ui_mode,
                sid,
                f"🔎 终审审计：长文一致性/完整性复查（第 {pass_no}/{max_passes} 轮）…",
                done=False,
            )
            excerpt = _build_text_repair_excerpt(out, max_chars=prompt_max_chars)
            user_payload = {
                "deliverable_mode": "report" if (dtype or mode) != "code" else "report",
                "deliverable_type": dtype or ("report" if mode != "code" else "report"),
                "heuristic_issues": issues,
                "requirements_summary": req_summary,
                "must_have": must_have,
                "complexity": complexity,
                "build_workflow": build_workflow,
                "deliverable_notes": str(deliverable_spec.get("notes") or "").strip(),
                "instructions": "只输出 JSON。若 pass=false，请给出最小必要 patches；不要整篇重写。",
                "content": excerpt,
            }
            msgs = [
                {"role": "system", "content": sys_prompt},
                {
                    "role": "user",
                    "content": json.dumps(user_payload, ensure_ascii=False),
                },
            ]

            internal_md = self._build_internal_metadata(
                metadata_in, audit_model, "final_audit_report", pass_no, sid
            )
            res = await self._call_llm(
                __request__,
                user_obj,
                audit_model,
                msgs,
                internal_md,
                forward_opts={"tool_choice": "none", "parallel_tool_calls": False},
                temperature=0.0,
                tool_choice_override="none",
                tools_override=[],
            )
            resp_text = _extract_text(res)
            obj = _extract_json_object(resp_text)
            if not isinstance(obj, dict):
                break

            if _as_bool(obj.get("pass"), False):
                break

            patches = obj.get("patches")
            if not isinstance(patches, list) or not patches:
                break

            new_out, applied, _skipped = _apply_code_patches(out, patches)
            if not applied:
                break
            out = new_out

        return out

    # ----------------------------
    # Formatting
    # ----------------------------
    def _format_step_info(
        self, plan_obj: Dict[str, Any], step_idx: int
    ) -> Tuple[str, str]:
        plan = plan_obj.get("plan")
        if not isinstance(plan, list) or not plan:
            return "", ""
        step_idx = max(0, min(step_idx, len(plan) - 1))
        it = plan[step_idx]
        if not isinstance(it, dict):
            return "", ""
        title = str(it.get("title") or f"Step {step_idx+1}").strip()
        goal = str(it.get("goal") or "").strip()
        done = str(it.get("done") or "").strip()

        head = f"{step_idx+1}/{len(plan)}：{title}"
        s = f"【当前步骤】{head}\n"
        if goal:
            s += f"- 目标：{goal}\n"
        if done:
            s += f"- 完成标准：{done}\n"
        return head, s.strip()

    def _apply_chunking(
        self,
        sess: Dict[str, Any],
        text: str,
        chunk_max_chars: int,
        chunk_policy: str = "manual",
    ) -> str:
        policy = (chunk_policy or "manual").strip().lower()
        # 对于代码交付：禁用分段输出（分段页脚会破坏代码块与可复制性）
        try:
            spec = sess.get("deliverable_spec") or {}
            is_code_deliverable = bool(spec.get("need_eof_marker")) or (
                str(spec.get("mode") or "").strip().lower() == "code"
            )
        except Exception:
            is_code_deliverable = False
        if is_code_deliverable:
            sess.pop("pending_chunks", None)
            sess.pop("pending_chunk_i", None)
            sess.pop("pending_chunk_n", None)
            return text

        if policy in ("auto", "none", "off"):
            sess.pop("pending_chunks", None)
            sess.pop("pending_chunk_i", None)
            sess.pop("pending_chunk_n", None)
            return text

        chunks = _chunk_text_safely(text, max_chars=chunk_max_chars)
        if len(chunks) <= 1:
            sess.pop("pending_chunks", None)
            sess.pop("pending_chunk_i", None)
            sess.pop("pending_chunk_n", None)
            return text

        sess["pending_chunks"] = chunks
        sess["pending_chunk_i"] = 1
        sess["pending_chunk_n"] = len(chunks)
        first = chunks[0]
        footer = self.valves.CHUNK_FOOTER.format(i=1, n=len(chunks))
        return first + footer

    def _pop_next_chunk(self, sess: Dict[str, Any]) -> Optional[str]:
        chunks = sess.get("pending_chunks")
        if not isinstance(chunks, list) or not chunks:
            return None
        i = int(sess.get("pending_chunk_i", 0))
        n = int(sess.get("pending_chunk_n", len(chunks)))
        if i >= len(chunks):
            return None
        part = chunks[i]
        sess["pending_chunk_i"] = i + 1

        if i + 1 < len(chunks):
            footer = self.valves.CHUNK_FOOTER.format(i=i + 1, n=n)
            return (part or "") + footer
        else:
            sess.pop("pending_chunks", None)
            sess.pop("pending_chunk_i", None)
            sess.pop("pending_chunk_n", None)
            return part

    # ----------------------------
    # Main pipe entry
    # ----------------------------
    async def pipe(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __request__=None,
        __event_emitter__=None,
        __metadata__: Optional[dict] = None,
        __tools__: Optional[dict] = None,
        __files__: Optional[list] = None,
        __model__: Optional[dict] = None,
        **kwargs,
    ) -> str:
        _cleanup_sess()

        if __request__ is None:
            return "Deep Research: 缺少 __request__，无法调用 generate_chat_completion"

        __user__ = __user__ or {}
        user_obj = _get_user_obj(__user__)

        body = body if isinstance(body, dict) else {}
        messages_in = deepcopy(body.get("messages") or [])
        user_text_now = _safe_last_user_text(messages_in)

        # quick reset
        if _looks_like_reset(user_text_now):
            metadata_tmp = _safe_get_md(body, __metadata__)
            router_tmp = _parse_router_tag(messages_in)
            sid_tmp = _resolve_session_id(
                metadata_tmp,
                router_tmp,
                body=body,
                messages=messages_in,
                user=user_obj,
                request=__request__,
            )
            _SESS.pop(sid_tmp, None)
            return "Deep Research：已重置本次研究状态。"

        metadata_in = _safe_get_md(body, __metadata__)
        router = _parse_router_tag(messages_in)
        sid = _resolve_session_id(
            metadata_in,
            router,
            body=body,
            messages=messages_in,
            user=user_obj,
            request=__request__,
        )
        base_model, base_src = _resolve_base_model(metadata_in, router)
        cfg, ui = _resolve_config(metadata_in, router)

        # UI
        ui_mode = str(ui.get("mode") or "simple").strip().lower()
        if ui_mode not in ("simple", "debug"):
            ui_mode = "simple"
        show_timing = _as_bool(ui.get("show_timing"), True)
        append_timing_to_final = _as_bool(ui.get("append_timing_to_final"), False)
        show_tool_detail = _as_bool(ui.get("show_tool_detail"), True)
        show_alignment = _as_bool(ui.get("show_alignment"), True)

        # config numbers
        max_rounds = _clamp_min(
            cfg.get("max_rounds"), 1, self.valves.DEFAULT_MAX_ROUNDS
        )
        min_rounds = _clamp_min(
            cfg.get("min_rounds"), 1, self.valves.DEFAULT_MIN_ROUNDS
        )
        if min_rounds > max_rounds:
            min_rounds = max_rounds

        max_tool_turns = _clamp_min(
            cfg.get("max_tool_turns_per_round"),
            1,
            self.valves.DEFAULT_MAX_TOOL_TURNS_PER_ROUND,
        )
        max_tool_calls_per_turn = _clamp_min(
            cfg.get("max_tool_calls_per_turn"),
            0,
            self.valves.DEFAULT_MAX_TOOL_CALLS_PER_TURN,
        )
        max_output_continue_turns = _clamp_min(
            cfg.get("max_output_continue_turns"),
            0,
            self.valves.DEFAULT_MAX_OUTPUT_CONTINUE_TURNS,
        )
        chunk_max_chars = _as_int(
            cfg.get("chunk_max_chars"), self.valves.DEFAULT_CHUNK_MAX_CHARS
        )

        chunk_policy = (
            str(cfg.get("chunk_policy") or self.valves.DEFAULT_CHUNK_POLICY)
            .strip()
            .lower()
        )
        if chunk_policy not in ("auto", "manual"):
            chunk_policy = self.valves.DEFAULT_CHUNK_POLICY

        # final report compilation (to avoid 'many rounds but short final answer')
        enable_final_compile = _as_bool(cfg.get("enable_final_compile"), True)
        compile_model = str(cfg.get("compile_model") or "").strip() or base_model
        compile_max_output_continue_turns = _clamp_min(
            cfg.get("compile_max_output_continue_turns"),
            0,
            self.valves.DEFAULT_COMPILE_MAX_OUTPUT_CONTINUE_TURNS,
        )
        final_min_chars = _clamp_min(
            cfg.get("final_min_chars"),
            0,
            self.valves.DEFAULT_FINAL_MIN_CHARS,
        )
        compile_step_output_max_chars = _clamp_min(
            cfg.get("compile_step_output_max_chars"),
            1000,
            8000,
        )

        enable_clarify = _as_bool(cfg.get("enable_clarify"), True)
        clarify_max_repeat = _clamp_min(cfg.get("clarify_max_repeat_same_q"), 0, 1)
        enable_assumption_mode = _as_bool(cfg.get("enable_assumption_mode"), True)

        enable_dynamic_plan = _as_bool(cfg.get("enable_dynamic_plan"), True)
        enable_judge_rewrite = _as_bool(cfg.get("enable_judge_rewrite"), True)

        confirm_policy = str(cfg.get("confirm_policy") or "pause").strip().lower()
        if confirm_policy not in ("auto", "always", "never", "pause"):
            confirm_policy = "auto"

        judge_model = str(cfg.get("judge_model") or "").strip() or base_model
        planner_model = str(cfg.get("planner_model") or "").strip() or base_model

        if not base_model:
            fb = str(cfg.get("fallback_base_model") or "").strip()
            if fb:
                base_model = fb
                base_src = "cfg.fallback_base_model"

        if not base_model:
            return "Deep Research: 未找到 base_model（请确保 Switch Filter 已写入 metadata，或在开关设置里填写 fallback_base_model）"

        # time context (computed later after we have merged user supplements)
        time_context = ""

        # session state
        existing_sess = _SESS.get(sid)
        is_new_session = existing_sess is None
        sess = existing_sess or {
            "updated_at": _now(),
            "paused": False,
            "pause_reason": "",
            "pending_kind": "",
            "pending_question": "",
            "pending_q_repeat": 0,
            "assumption_mode": False,
            "user_text": "",
            "conversation_context": "",
            "scoped": None,
            "locked_target": "",
            "confirmed": False,
            "plan": None,
            "step_idx": 0,
            "last_answer": "",
            "improve_brief": "",
            "step_outputs": [],
            "chart_blocks": [],
            "deferred_chart_calls": [],
            "request_seq": 0,
            "round_no": 0,
            "final_done": False,
            "artifact_fields": {},
            "context_pack": None,
            "timing": {
                "llm_calls": 0,
                "tool_calls": 0,
                "llm_secs": 0.0,
                "tool_secs": 0.0,
                "started_at": _now(),
            },
            "need_rescope": False,
            "need_replan": False,
        }
        sess["updated_at"] = _now()

        # ---- history store init (for "全看") ----
        if not isinstance(sess.get("history_store"), list):
            sess["history_store"] = []
        if not isinstance(sess.get("history_store_meta"), dict):
            sess["history_store_meta"] = {}

        # ---- history store load from disk (optional) ----
        try:
            persist_enabled = _as_bool(
                cfg.get("context_stitch_state_persist_enabled"),
                getattr(self.valves, "CONTEXT_STITCH_STATE_PERSIST_ENABLED", True),
            )
            if persist_enabled and is_new_session and not sess.get("history_store"):
                root_dir = _pick_artifact_store_root(
                    cfg.get("artifact_cache_dir")
                    or getattr(self.valves, "ARTIFACT_CACHE_DIR", "")
                )
                state_data = _state_cache_load(root_dir, sid)
                hs = (
                    state_data.get("history_store")
                    if isinstance(state_data, dict)
                    else None
                )
                if isinstance(hs, list) and hs:
                    sess["history_store"] = hs
                    sess["history_store_meta"]["loaded_from_disk"] = True
        except Exception:
            pass

        # ---- normalize timing keys (backward compatible; prevents KeyError: 'llm_secs') ----
        try:
            timing = sess.get("timing") if isinstance(sess.get("timing"), dict) else {}
            # migrate legacy keys
            if "llm_secs" not in timing and "llm_time" in timing:
                timing["llm_secs"] = timing.get("llm_time", 0.0)
            if "tool_secs" not in timing and "tool_time" in timing:
                timing["tool_secs"] = timing.get("tool_time", 0.0)
            timing["llm_calls"] = int(timing.get("llm_calls", 0) or 0)
            timing["tool_calls"] = int(timing.get("tool_calls", 0) or 0)
            timing["llm_secs"] = float(timing.get("llm_secs", 0.0) or 0.0)
            timing["tool_secs"] = float(timing.get("tool_secs", 0.0) or 0.0)
            timing["started_at"] = float(timing.get("started_at", _now()) or _now())
            sess["timing"] = timing
        except Exception:
            sess["timing"] = {
                "llm_calls": 0,
                "tool_calls": 0,
                "llm_secs": 0.0,
                "tool_secs": 0.0,
                "started_at": _now(),
            }
        try:
            sess["request_seq"] = int(sess.get("request_seq", 0)) + 1
        except Exception:
            sess["request_seq"] = 1

        # persist early (so manual stop can be resumed safely)
        _SESS[sid] = sess

        # ---- context receipt + conversation context (refreshed) ----
        # Some frontends do NOT send the full chat history every request.
        # This block helps you *see what we actually received* and avoids stale context.
        try:
            # 1) receipt diagnostics (why we sometimes "ask again" even when UI shows context)
            receipt_mode = (
                str(cfg.get("context_receipt_mode") or "auto").strip().lower()
            )
            if receipt_mode not in ("off", "auto", "always"):
                receipt_mode = "auto"

            stats = _messages_context_stats(messages_in)
            msg_n = int(stats.get("msg_n") or 0)

            prev_n = sess.get("_last_received_msg_count")
            sess["_last_received_msg_count"] = msg_n

            # 1.5) ingest into history store (for “全看” even if the frontend truncates)
            try:
                hs_enabled = _as_bool(
                    cfg.get("context_stitch_history_store_enabled"),
                    getattr(self.valves, "CONTEXT_STITCH_HISTORY_STORE_ENABLED", True),
                )
                if hs_enabled:
                    hs_max_messages = _as_int(
                        cfg.get("context_stitch_history_store_max_messages"),
                        getattr(
                            self.valves,
                            "CONTEXT_STITCH_HISTORY_STORE_MAX_MESSAGES",
                            240,
                        ),
                    )
                    hs_max_chars = _as_int(
                        cfg.get("context_stitch_history_store_max_chars"),
                        getattr(
                            self.valves,
                            "CONTEXT_STITCH_HISTORY_STORE_MAX_CHARS",
                            240000,
                        ),
                    )
                    hs_per_msg = _as_int(
                        cfg.get("context_stitch_history_store_per_message_max_chars"),
                        getattr(
                            self.valves,
                            "CONTEXT_STITCH_HISTORY_STORE_PER_MESSAGE_MAX_CHARS",
                            12000,
                        ),
                    )
                    sess["history_store"] = _history_store_ingest(
                        sess.get("history_store"),
                        messages_in,
                        max_messages=hs_max_messages,
                        max_chars=hs_max_chars,
                        per_message_max_chars=hs_per_msg,
                    )
                    if isinstance(sess.get("history_store_meta"), dict):
                        sess["history_store_meta"]["count"] = len(
                            sess.get("history_store") or []
                        )

                    persist_enabled = _as_bool(
                        cfg.get("context_stitch_state_persist_enabled"),
                        getattr(
                            self.valves, "CONTEXT_STITCH_STATE_PERSIST_ENABLED", True
                        ),
                    )
                    if persist_enabled:
                        root_dir = _pick_artifact_store_root(
                            cfg.get("artifact_cache_dir")
                            or getattr(self.valves, "ARTIFACT_CACHE_DIR", "")
                        )
                        _state_cache_save(
                            root_dir,
                            sid,
                            {
                                "updated_at": _now(),
                                "history_store": sess.get("history_store") or [],
                            },
                        )
            except Exception:
                pass

            # Heuristics: if message count drops a lot, the upstream likely sent a truncated window.
            anomaly = False
            if msg_n <= 2:
                anomaly = True
            if isinstance(prev_n, int) and prev_n > 0 and (msg_n + 2 < prev_n):
                anomaly = True

            show_receipt = receipt_mode == "always" or (
                receipt_mode == "auto" and (anomaly or ui_mode == "debug")
            )

            if show_receipt:
                max_items = _as_int(cfg.get("context_receipt_max_items"), 8)
                snippet_chars = _as_int(cfg.get("context_receipt_snippet_chars"), 80)
                tail = _summarize_messages_tail(
                    messages_in, max_items=max_items, snippet_chars=snippet_chars
                )

                prefix = "⚠️ " if anomaly else ""
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"{prefix}🧾 上下文接收：历史{msg_n}条（user {stats.get('user_n')}/assistant {stats.get('assistant_n')}），文本{stats.get('text_chars')}c，图片占位{stats.get('image_placeholders')}",
                    done=False,
                )
                if tail:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"🧾 上下文尾部：{tail}",
                        done=False,
                    )

            _SESS[sid] = sess
        except Exception:
            pass

        try:
            # 2) build/refresh conversation context excerpt (for enabling DR mid-chat)
            h_max_messages = _as_int(cfg.get("history_context_max_messages"), 24)
            h_max_chars = _as_int(cfg.get("history_context_max_chars"), 18000)
            h_per_msg = _as_int(cfg.get("history_context_per_message_max_chars"), 2000)
            h_include_asst = _as_bool(
                cfg.get("history_context_include_assistant", True), True
            )
            refresh_mode = (
                str(cfg.get("history_context_refresh_mode") or "always").strip().lower()
            )
            if refresh_mode not in ("always", "on_change", "never"):
                refresh_mode = "always"

            # clamp to avoid accidental token bombs
            h_max_messages = max(0, min(200, int(h_max_messages or 0)))
            h_max_chars = max(0, min(80000, int(h_max_chars or 0)))
            h_per_msg = max(200, min(20000, int(h_per_msg or 0)))

            want_ctx = h_max_messages > 0 and h_max_chars > 0
            if want_ctx:
                msg_n = len(messages_in) if isinstance(messages_in, list) else 0
                last_user_hash = hashlib.sha1(
                    (user_text_now or "").encode("utf-8", "ignore")
                ).hexdigest()[:12]
                prev_meta = (
                    sess.get("conversation_context_meta")
                    if isinstance(sess.get("conversation_context_meta"), dict)
                    else {}
                )

                rebuild = False
                if refresh_mode == "always":
                    rebuild = True
                elif refresh_mode == "on_change":
                    if (
                        prev_meta.get("msg_n") != msg_n
                        or prev_meta.get("last_user_hash") != last_user_hash
                    ):
                        rebuild = True
                    if not sess.get("conversation_context"):
                        rebuild = True
                elif refresh_mode == "never":
                    rebuild = not bool(sess.get("conversation_context"))

                if rebuild:
                    sess["conversation_context"] = _build_conversation_context(
                        messages_in,
                        max_messages=h_max_messages,
                        max_chars=h_max_chars,
                        per_message_max_chars=h_per_msg,
                        include_assistant=h_include_asst,
                        exclude_last_user=True,
                    )
                    sess["conversation_context_meta"] = {
                        "built_at": _now(),
                        "msg_n": msg_n,
                        "last_user_hash": last_user_hash,
                        "max_messages": h_max_messages,
                        "max_chars": h_max_chars,
                        "per_msg": h_per_msg,
                        "include_assistant": h_include_asst,
                    }
                    _SESS[sid] = sess
        except Exception:
            sess["conversation_context"] = sess.get("conversation_context") or ""

        # ---- ingest code pasted in THIS user message as baseline deliverable (e.g., user shares logs/code) ----
        try:
            # reset per-turn flag
            sess["_ingested_user_deliverable_this_turn"] = False
            if user_text_now:
                # Heuristic: if message contains a big code block / full HTML / DR_EOF, treat it as the baseline artifact.
                prefer_lang = (
                    "html"
                    if (
                        "<!doctype" in user_text_now.lower()
                        or "<html" in user_text_now.lower()
                    )
                    else "text"
                )
                lang_guess, code_guess = _extract_code_candidate(
                    user_text_now, prefer_lang=prefer_lang
                )
                code_norm = _normalize_code_candidate(
                    code_guess, lang_guess or prefer_lang or "text"
                )
                lowc = (code_norm or "").lower()
                looks_full = False
                if code_norm:
                    # Strong signals
                    if "dr_eof" in lowc:
                        looks_full = True
                    # HTML full document
                    elif (
                        (("<!doctype" in lowc) or ("<html" in lowc))
                        and ("</html>" in lowc or "</body>" in lowc)
                        and (len(code_norm) >= 1200)
                    ):
                        looks_full = True
                    # Code fenced block is usually intentional
                    elif "```" in user_text_now and len(code_norm) >= 400:
                        looks_full = True
                    else:
                        # Generic code-blob heuristic (avoid treating long random text / base64 as code)
                        lines = code_norm.count("\n") + 1
                        punct = sum(1 for ch in code_norm if ch in "{}[]();<>")
                        tokens = (
                            "function",
                            "const ",
                            "let ",
                            "var ",
                            "class ",
                            "def ",
                            "import ",
                            "from ",
                            "=>",
                            "</script",
                            "<script",
                            "<canvas",
                            "ctx.",
                            "requestanimationframe",
                        )
                        score = sum(1 for tok in tokens if tok in lowc)
                        if score >= 2 and (
                            lines >= 8 or punct >= 80 or len(code_norm) >= 2000
                        ):
                            looks_full = True
                        elif lines >= 30 and punct >= 60 and len(code_norm) >= 1200:
                            looks_full = True

                    # Safety: if it looks like an inlined multimodal payload (stringified image_url), do NOT ingest.
                    if looks_full:
                        if (
                            ("image_url" in lowc and "data:image" in lowc)
                            and ("<html" not in lowc)
                            and ("dr_eof" not in lowc)
                        ):
                            looks_full = False

                        # Also guard against raw data-URI / base64 blobs being mis-detected as "code"
                        if (
                            ("data:image" in lowc)
                            and ("<html" not in lowc)
                            and ("<script" not in lowc)
                            and ("function" not in lowc)
                            and ("dr_eof" not in lowc)
                        ):
                            try:
                                if (
                                    len(
                                        re.findall(
                                            r"data:image/[a-z0-9.+-]+;base64,",
                                            lowc,
                                            flags=re.IGNORECASE,
                                        )
                                    )
                                    >= 1
                                    and len(code_norm or "") > 2000
                                ):
                                    looks_full = False
                            except Exception:
                                pass

                if looks_full:
                    code_lang = (
                        lang_guess or prefer_lang or "text"
                    ).strip().lower() or "text"
                    spec0 = {
                        "deliverable_type": "code",
                        "code_language": code_lang,
                        "need_eof_marker": True,
                        "eof_marker": "DR_EOF",
                        "eof_marker_line": _format_eof_marker_line("DR_EOF", code_lang),
                        "must_have": [],
                        "requirements_summary": "",
                        "complexity": "medium",
                        "build_workflow": "patch",
                    }

                    sess["deliverable"] = code_norm
                    sess["last_deliverable"] = code_norm
                    sess["deliverable_spec"] = spec0
                    sess["last_deliverable_spec"] = spec0
                    sess["last_deliverable_at"] = _now()
                    sess["final_done"] = True
                    sess["_ingested_user_deliverable_this_turn"] = True

                    # Try lock target from <title>
                    try:
                        mtitle = re.search(
                            r"<title>\s*([^<]{2,120})\s*</title>", code_norm, flags=re.I
                        )
                        if mtitle:
                            sess["locked_target"] = mtitle.group(1).strip()
                    except Exception:
                        pass

                    _SESS[sid] = sess

                    # Remove huge code from user_text_now so downstream prompts stay focused.
                    cleaned = user_text_now
                    cleaned = re.sub(r"```[\s\S]*?```", "\n", cleaned)
                    low2 = cleaned.lower()
                    st = low2.find("<!doctype")
                    if st == -1:
                        st = low2.find("<html")
                    ed = low2.rfind("</html>")
                    if st != -1 and ed != -1 and ed > st:
                        cleaned = cleaned[:st] + "\n" + cleaned[ed + len("</html>") :]
                    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned).strip()
                    user_text_now = (
                        cleaned
                        or "请基于我刚粘贴的代码继续修复/增加内容，输出完整可运行的单文件。"
                    )

                    if __event_emitter__:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "📌 已识别到你在本条消息中粘贴了完整代码：将把它作为新的‘基线交付物’（会覆盖先前恢复的版本），后续会在其基础上分阶段打补丁（不会另起炉灶重写）。",
                            done=False,
                        )

                    # Persist baseline to artifact cache immediately (best-effort)
                    # so later "交付物修改模式" can recover even if OpenWebUI truncates chat history.
                    try:
                        cache_enabled = _as_bool(
                            (
                                cfg.get("artifact_cache_enabled")
                                if isinstance(cfg, dict)
                                and "artifact_cache_enabled" in cfg
                                else (
                                    cfg.get("artifact_persist_enabled")
                                    if isinstance(cfg, dict)
                                    and "artifact_persist_enabled" in cfg
                                    else None
                                )
                            ),
                            bool(getattr(self.valves, "ARTIFACT_CACHE_ENABLED", True)),
                        )
                        if cache_enabled:
                            cache_dir = ""
                            if isinstance(cfg, dict):
                                cache_dir = str(
                                    cfg.get("artifact_cache_dir")
                                    or cfg.get("artifact_store_dir")
                                    or ""
                                ).strip()
                            if not cache_dir:
                                cache_dir = str(
                                    getattr(self.valves, "ARTIFACT_CACHE_DIR", "") or ""
                                ).strip()
                            max_chars = int(
                                (
                                    cfg.get("artifact_cache_max_chars")
                                    if isinstance(cfg, dict)
                                    and cfg.get("artifact_cache_max_chars") is not None
                                    else getattr(
                                        self.valves, "ARTIFACT_CACHE_MAX_CHARS", 400000
                                    )
                                )
                                or 400000
                            )
                            history_max = int(
                                (
                                    cfg.get("artifact_cache_history_max")
                                    if isinstance(cfg, dict)
                                    and cfg.get("artifact_cache_history_max")
                                    is not None
                                    else getattr(
                                        self.valves, "ARTIFACT_CACHE_HISTORY_MAX", 20
                                    )
                                )
                                or 20
                            )
                            root = _pick_artifact_store_root(cache_dir)
                            chat_id = ""
                            try:
                                drmd = (
                                    metadata_in.get("deep_research")
                                    if isinstance(
                                        metadata_in.get("deep_research"), dict
                                    )
                                    else {}
                                )
                                chat_id = str(drmd.get("chat_id") or "").strip()
                            except Exception:
                                chat_id = ""
                            spec_to_save = (
                                sess.get("deliverable_spec")
                                if isinstance(sess.get("deliverable_spec"), dict)
                                else {}
                            )
                            meta = _artifact_cache_write(
                                root=root,
                                session_id=sid,
                                chat_id=chat_id,
                                content=sess.get("deliverable")
                                or sess.get("last_deliverable")
                                or "",
                                spec=spec_to_save,
                                max_chars=max_chars,
                                history_max=history_max,
                            )
                            if isinstance(meta, dict):
                                sess["artifact_cache_last_id"] = str(
                                    meta.get("id") or ""
                                )
                                sess["artifact_cache_last_path"] = str(
                                    meta.get("path") or ""
                                )
                                _SESS[sid] = sess
                    except Exception:
                        pass
        except Exception:
            pass

        # ---- pre-router baseline restore (history/cache) ----
        # 说明：有些前端/网关会截断 messages 窗口，或进程重启导致内存会话丢失。
        # 如果本轮用户是在“已有交付物”基础上继续修改，但本次请求里刚好没带上那份大代码，
        # 旧逻辑会先让模型做路由，容易把“修改”误判成“新任务”，进而重写/丢内容。
        # 这里在路由前，先尽力从：1) 本次请求携带的聊天记录 2) 本地交付物缓存 中恢复基线交付物。
        try:
            if not (sess.get("last_deliverable") or sess.get("deliverable")):
                # 1) 从本次请求携带的 messages 中恢复（最可靠）
                restored = _extract_prev_deliverable_from_chat_history(messages_in)
                if isinstance(restored, dict):
                    prev_text = str(restored.get("deliverable") or "")
                    prev_spec = (
                        restored.get("spec")
                        if isinstance(restored.get("spec"), dict)
                        else {}
                    )
                    if prev_text.strip():
                        sess["deliverable"] = prev_text
                        sess["last_deliverable"] = prev_text
                        if prev_spec:
                            sess["deliverable_spec"] = prev_spec
                            sess["last_deliverable_spec"] = prev_spec
                        sess["last_deliverable_at"] = _now()
                        # 将其视为“已完成的基线交付物”，后续 follow-up 意图识别仍可选择 new_task
                        sess["final_done"] = True
                        sess["restored_from_history"] = True
                        _SESS[sid] = sess
                        if __event_emitter__:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                "🔁 已从聊天记录恢复上一版交付物（预路由）：本轮将优先走“修改/补丁”流程",
                                done=False,
                            )

                # 2) 如果本次请求窗口里没带上大代码，尝试从本地缓存恢复
                if not (sess.get("last_deliverable") or sess.get("deliverable")):
                    cache_enabled = _as_bool(
                        (
                            cfg.get("artifact_cache_enabled")
                            if isinstance(cfg, dict) and "artifact_cache_enabled" in cfg
                            else (
                                cfg.get("artifact_persist_enabled")
                                if isinstance(cfg, dict)
                                and "artifact_persist_enabled" in cfg
                                else None
                            )
                        ),
                        bool(getattr(self.valves, "ARTIFACT_CACHE_ENABLED", True)),
                    )
                    if cache_enabled:
                        cache_dir = ""
                        if isinstance(cfg, dict):
                            cache_dir = str(
                                cfg.get("artifact_cache_dir")
                                or cfg.get("artifact_store_dir")
                                or ""
                            ).strip()
                        if not cache_dir:
                            cache_dir = str(
                                getattr(self.valves, "ARTIFACT_CACHE_DIR", "") or ""
                            ).strip()
                        root = _pick_artifact_store_root(cache_dir)

                        chat_id = ""
                        try:
                            drmd = (
                                metadata_in.get("deep_research")
                                if isinstance(metadata_in.get("deep_research"), dict)
                                else {}
                            )
                            chat_id = str(drmd.get("chat_id") or "").strip()
                        except Exception:
                            chat_id = ""

                        prev_text_disk, meta_disk = _artifact_cache_load_last(
                            root=root,
                            session_id=sid,
                            chat_id=chat_id,
                        )
                        if prev_text_disk.strip():
                            spec_disk = {}
                            if isinstance(meta_disk, dict) and isinstance(
                                meta_disk.get("spec"), dict
                            ):
                                spec_disk = meta_disk.get("spec")
                            sess["deliverable"] = prev_text_disk
                            sess["last_deliverable"] = prev_text_disk
                            if spec_disk:
                                sess["deliverable_spec"] = spec_disk
                                sess["last_deliverable_spec"] = spec_disk
                            sess["last_deliverable_at"] = _now()
                            sess["final_done"] = True
                            sess["restored_from_cache"] = True
                            _SESS[sid] = sess

                            if __event_emitter__:
                                await self._status(
                                    __event_emitter__,
                                    ui_mode,
                                    sid,
                                    "🔁 已从本地缓存恢复上一版交付物（预路由）：本轮将优先走“修改/补丁”流程",
                                    done=False,
                                )
        except Exception:
            # 预路由恢复失败不应阻塞主流程
            pass

        # ---- model-based turn routing (LLM) ----
        # We use the model to decide whether this turn is:
        #   - a chunk continuation,
        #   - an answer to a pending pause/clarify question,
        #   - a follow-up modification on the previous deliverable,
        #   - or a brand-new task.
        # This avoids hard-coded heuristics that can mis-route mid-chat enablement.
        turn_route_obj = None
        try:
            has_deliverable_now = bool(
                sess.get("last_deliverable") or sess.get("deliverable")
            )

            # derive deliverable_type from spec if possible
            deliverable_type = ""
            try:
                _sp = (
                    sess.get("deliverable_spec")
                    or sess.get("last_deliverable_spec")
                    or {}
                )
                if isinstance(_sp, dict):
                    deliverable_type = (
                        str(_sp.get("deliverable_type") or "").strip().lower()
                    )
                    if not deliverable_type:
                        if _sp.get("need_eof_marker") or (
                            str(_sp.get("mode") or "").strip().lower() == "code"
                        ):
                            deliverable_type = "code"
            except Exception:
                deliverable_type = ""

            pending_kind = str(sess.get("pending_kind") or "")
            pending_question = str(sess.get("pending_question") or "")
            has_pending_chunks = bool(
                chunk_policy == "manual" and sess.get("pending_chunks")
            )

            need_router = bool(user_text_now) and (
                bool(sess.get("paused")) or (not has_deliverable_now)
            )
            if need_router:
                turn_route_obj = await self._classify_turn_route(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    base_model,
                    metadata_in,
                    user_text_now,
                    has_deliverable=has_deliverable_now,
                    deliverable_type=deliverable_type,
                    pending_kind=pending_kind,
                    pending_question=pending_question,
                    has_pending_chunks=has_pending_chunks,
                    conversation_context=str(sess.get("conversation_context") or ""),
                    context_pack=context_pack or sess.get("context_pack") or None,
                )
                sess["last_turn_route"] = turn_route_obj
                _SESS[sid] = sess

            # ---- restore last deliverable from chat history (when missing) ----
            has_deliverable_now = bool(
                sess.get("last_deliverable") or sess.get("deliverable")
            )
            if (
                (not has_deliverable_now)
                and turn_route_obj
                and str(turn_route_obj.get("route") or "") == "code_modify"
            ):
                restored = _extract_prev_deliverable_from_chat_history(messages_in)
                if isinstance(restored, dict):
                    prev_text = str(restored.get("deliverable") or "")
                    prev_spec = (
                        restored.get("spec")
                        if isinstance(restored.get("spec"), dict)
                        else {}
                    )
                    if prev_text.strip():
                        sess["deliverable"] = prev_text
                        sess["last_deliverable"] = prev_text
                        if prev_spec:
                            sess["deliverable_spec"] = prev_spec
                            sess["last_deliverable_spec"] = prev_spec
                        sess["last_deliverable_at"] = _now()
                        sess["final_done"] = True
                        sess["restored_from_history"] = True
                        _SESS[sid] = sess
                        if __event_emitter__:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                "🔁 已从聊天记录恢复上一版交付物：可直接继续修改/修复",
                                done=False,
                            )
                    else:
                        # Debug: we attempted restore but didn't find a usable artifact
                        if __event_emitter__ and ui_mode == "debug":
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                "⚠️ 尝试从聊天记录恢复交付物但未找到可用代码块（可能不在本次请求的 messages 窗口内，或被前端截断）。",
                                done=False,
                            )

            # If still missing (often because the UI/server truncated a long code block), try restoring from disk artifact cache.
            if (
                turn_route_obj
                and str(turn_route_obj.get("route") or "") == "code_modify"
            ) and (not (sess.get("last_deliverable") or sess.get("deliverable"))):
                try:
                    cache_enabled = _as_bool(
                        (
                            cfg.get("artifact_cache_enabled")
                            if isinstance(cfg, dict) and "artifact_cache_enabled" in cfg
                            else (
                                cfg.get("artifact_persist_enabled")
                                if isinstance(cfg, dict)
                                and "artifact_persist_enabled" in cfg
                                else None
                            )
                        ),
                        bool(self.valves.ARTIFACT_CACHE_ENABLED),
                    )
                    if cache_enabled:
                        cache_dir = ""
                        if isinstance(cfg, dict):
                            cache_dir = str(
                                cfg.get("artifact_cache_dir")
                                or cfg.get("artifact_store_dir")
                                or ""
                            ).strip()
                        if not cache_dir:
                            cache_dir = str(
                                getattr(self.valves, "ARTIFACT_CACHE_DIR", "") or ""
                            ).strip()
                        root = _pick_artifact_store_root(cache_dir)

                        chat_id = ""
                        try:
                            drmd = (
                                metadata_in.get("deep_research")
                                if isinstance(metadata_in.get("deep_research"), dict)
                                else {}
                            )
                            chat_id = str(drmd.get("chat_id") or "").strip()
                        except Exception:
                            chat_id = ""

                        prev_text_disk, meta_disk = _artifact_cache_load_last(
                            root=root,
                            session_id=sid,
                            chat_id=chat_id,
                        )
                        if prev_text_disk.strip():
                            spec_disk = {}
                            if isinstance(meta_disk, dict) and isinstance(
                                meta_disk.get("spec"), dict
                            ):
                                spec_disk = meta_disk.get("spec")
                            sess["deliverable"] = prev_text_disk
                            sess["last_deliverable"] = prev_text_disk
                            if spec_disk:
                                sess["deliverable_spec"] = spec_disk
                                sess["last_deliverable_spec"] = spec_disk
                            sess["last_deliverable_at"] = _now()
                            sess["final_done"] = True
                            sess["restored_from_cache"] = True
                            _SESS[sid] = sess

                            if __event_emitter__:
                                await self._status(
                                    __event_emitter__,
                                    ui_mode,
                                    sid,
                                    "🔁 已从本地缓存恢复上一版交付物：可直接继续修改/修复",
                                    done=False,
                                )
                except Exception:
                    pass

            # 4.5) Context stitcher: extract stable fields & build a compact context pack.
            # This makes routing/intent robust even if the chat drifted into unrelated topics.
            context_pack: Optional[Dict[str, Any]] = None
            if user_text_now:
                ctx_enabled = _as_bool(
                    cfg.get("context_stitch_enabled"),
                    getattr(self.valves, "CONTEXT_STITCH_ENABLED", True),
                )
                if ctx_enabled:
                    try:
                        active_deliverable = (
                            sess.get("last_deliverable")
                            or sess.get("deliverable")
                            or ""
                        )
                        active_spec = (
                            sess.get("last_deliverable_spec")
                            or sess.get("deliverable_spec")
                            or {}
                        )
                        active_scoped = (
                            sess.get("last_scoped") or sess.get("scoped") or {}
                        )
                        active_locked = (
                            sess.get("locked_target")
                            or sess.get("last_locked_target")
                            or {}
                        )
                        recent_excerpt = sess.get("conversation_context") or ""

                        if active_deliverable:
                            context_pack = self._build_context_pack(
                                sess,
                                deliverable=active_deliverable,
                                spec=active_spec,
                                scoped=active_scoped,
                                locked_target=active_locked,
                                recent_context_excerpt=recent_excerpt,
                                user_message=user_text_now,
                                cfg=cfg,
                            )
                        else:
                            # still build a minimal pack (helps the router avoid asking for missing context)
                            recent_chars = _as_int(
                                cfg.get("context_stitch_recent_chars"),
                                getattr(
                                    self.valves, "CONTEXT_STITCH_RECENT_CHARS", 2400
                                ),
                            )
                            recent_chars = max(400, min(8000, recent_chars))
                            context_pack = {
                                "active_artifact": {},
                                "requirements_summary": "",
                                "must_have": [],
                                "locked_target": active_locked,
                                "baseline_outline": [],
                                "anchor_candidates": [],
                                "recent_context_excerpt": (recent_excerpt or "")[
                                    :recent_chars
                                ],
                                "user_message": (user_text_now or "")[:2000],
                            }
                            sess["context_pack"] = context_pack

                        if __event_emitter__ and _as_bool(
                            cfg.get("context_stitch_show_status"),
                            getattr(self.valves, "CONTEXT_STITCH_SHOW_STATUS", True),
                        ):
                            meta = (context_pack or {}).get("active_artifact") or {}
                            title = str(meta.get("title") or "").strip()
                            title_part = f"｜title={title}" if title else ""
                            await self._status(
                                __event_emitter__,
                                f"🧩 上下文拼接：交付物={meta.get('deliverable_type','?')}/{meta.get('code_language','?')}{title_part}"
                                + f"｜outline={len((context_pack or {}).get('baseline_outline') or [])}"
                                + f"｜anchors={len((context_pack or {}).get('anchor_candidates') or [])}"
                                + f"｜hist={(((context_pack or {}).get('history_store') or {}).get('count') or 0)}"
                                + f"｜rel={len((context_pack or {}).get('relevant_history_excerpt') or '')}"
                                + f"｜slices={len((context_pack or {}).get('artifact_slices') or [])}",
                            )
                    except Exception:
                        context_pack = context_pack or None

            # ---- if paused, let the model decide whether to keep pause or switch to code-modify/new-task ----
            if sess.get("paused") and user_text_now:
                route = str((turn_route_obj or {}).get("route") or "").strip().lower()
                if route in ("code_modify", "new_task"):
                    sess["paused"] = False
                    sess["pause_reason"] = ""
                    sess["pending_kind"] = ""
                    sess["pending_question"] = ""
                    sess["pending_q_repeat"] = 0
                    # cancel chunk mode if any (edit/new task should supersede)
                    if sess.get("pending_chunks"):
                        sess.pop("pending_chunks", None)
                        sess.pop("pending_chunks_meta", None)
                        sess.pop("pending_chunks_total", None)
                    _SESS[sid] = sess
                    if __event_emitter__:
                        mode_label = "代码修改" if route == "code_modify" else "新任务"
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🧭 模型路由：判定为「{mode_label}」请求，将跳过暂停直接处理",
                            done=False,
                        )

            # ---- If this is a code-modify request but we still cannot find any prior deliverable, ask for the artifact ----
            if user_text_now and (
                not (sess.get("last_deliverable") or sess.get("deliverable"))
            ):
                route = str((turn_route_obj or {}).get("route") or "").strip().lower()

                restored_now = False
                if route == "code_modify":
                    # Last-chance restore: even if the router forgot to set should_restore_deliverable,
                    # we still try to recover the baseline from recent chat messages.
                    restored = _extract_prev_deliverable_from_chat_history(messages_in)
                    if isinstance(restored, dict):
                        prev_text = str(restored.get("deliverable") or "")
                        prev_spec = (
                            restored.get("spec")
                            if isinstance(restored.get("spec"), dict)
                            else {}
                        )
                        if prev_text.strip():
                            sess["deliverable"] = prev_text
                            sess["last_deliverable"] = prev_text
                            if prev_spec:
                                sess["deliverable_spec"] = prev_spec
                                sess["last_deliverable_spec"] = prev_spec
                            sess["last_deliverable_at"] = _now()
                            sess["final_done"] = True
                            sess["restored_from_history"] = True
                            _SESS[sid] = sess
                            restored_now = True
                            if __event_emitter__:
                                await self._status(
                                    __event_emitter__,
                                    ui_mode,
                                    sid,
                                    "🔁 已从聊天记录恢复上一版交付物：可直接继续修改/修复",
                                    done=False,
                                )

                # Also try disk cache as a last-chance restore (handles cases where chat history was truncated and cannot be re-sent).
                if (
                    route == "code_modify"
                    and (not restored_now)
                    and (not (sess.get("last_deliverable") or sess.get("deliverable")))
                ):
                    try:
                        cache_enabled = _as_bool(
                            (
                                cfg.get("artifact_cache_enabled")
                                if isinstance(cfg, dict)
                                and "artifact_cache_enabled" in cfg
                                else (
                                    cfg.get("artifact_persist_enabled")
                                    if isinstance(cfg, dict)
                                    and "artifact_persist_enabled" in cfg
                                    else None
                                )
                            ),
                            bool(self.valves.ARTIFACT_CACHE_ENABLED),
                        )
                        if cache_enabled:
                            cache_dir = ""
                            if isinstance(cfg, dict):
                                cache_dir = str(
                                    cfg.get("artifact_cache_dir")
                                    or cfg.get("artifact_store_dir")
                                    or ""
                                ).strip()
                            if not cache_dir:
                                cache_dir = str(
                                    getattr(self.valves, "ARTIFACT_CACHE_DIR", "") or ""
                                ).strip()
                            root = _pick_artifact_store_root(cache_dir)

                            chat_id = ""
                            try:
                                drmd = (
                                    metadata_in.get("deep_research")
                                    if isinstance(
                                        metadata_in.get("deep_research"), dict
                                    )
                                    else {}
                                )
                                chat_id = str(drmd.get("chat_id") or "").strip()
                            except Exception:
                                chat_id = ""

                            prev_text_disk, meta_disk = _artifact_cache_load_last(
                                root=root, session_id=sid, chat_id=chat_id
                            )
                            if prev_text_disk.strip():
                                spec_disk = {}
                                if isinstance(meta_disk, dict) and isinstance(
                                    meta_disk.get("spec"), dict
                                ):
                                    spec_disk = meta_disk.get("spec")

                                sess["deliverable"] = prev_text_disk
                                sess["last_deliverable"] = prev_text_disk
                                if spec_disk:
                                    sess["deliverable_spec"] = spec_disk
                                    sess["last_deliverable_spec"] = spec_disk
                                sess["last_deliverable_at"] = _now()
                                sess["final_done"] = True
                                sess["restored_from_cache"] = True
                                _SESS[sid] = sess
                                restored_now = True

                                if __event_emitter__:
                                    await self._status(
                                        __event_emitter__,
                                        ui_mode,
                                        sid,
                                        "🔁 已从本地缓存恢复上一版交付物：可直接继续修改/修复",
                                        done=False,
                                    )
                    except Exception:
                        pass

                if (
                    route == "code_modify"
                    and (not restored_now)
                    and (not (sess.get("last_deliverable") or sess.get("deliverable")))
                ):
                    sess["paused"] = True
                    sess["pause_reason"] = "missing_deliverable"
                    sess["pending_kind"] = "need_artifact"
                    sess["pending_question"] = (
                        "我这次没有拿到你“要继续/修复”的那份内容（可能聊天记录未随请求传入或被截断）。\n"
                        "请把需要修改的代码/文件内容粘贴出来（尽量完整；如果是 HTML 请从 <!DOCTYPE html> 到 <!-- DR_EOF -->），或说明是哪一个文件/仓库。"
                    )
                    _SESS[sid] = sess
                    if __event_emitter__:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "⏸ 需要补充：缺少上一版交付物（请粘贴代码/文件）",
                            done=False,
                        )
                    return sess["pending_question"]

        except Exception:
            # Fall back to legacy behavior if router fails
            turn_route_obj = None

        # ---- status heartbeat: if a single step runs long, emit current status every 2 minutes ----
        heartbeat_task = None
        try:
            hb_interval_sec = _as_int(cfg.get("status_heartbeat_interval_sec"), 120)
        except Exception:
            hb_interval_sec = 120
        if hb_interval_sec < 0:
            hb_interval_sec = 0
        if __event_emitter__ and hb_interval_sec > 0:
            __event_emitter__, heartbeat_task = self._install_status_heartbeat(
                __event_emitter__,
                sess,
                ui_mode=ui_mode,
                sid=sid,
                interval_sec=hb_interval_sec,
            )

        try:
            # chunk continuation (only used in manual chunk policy)
            if chunk_policy != "manual":
                sess.pop("pending_chunks", None)
                sess.pop("pending_chunks_meta", None)
                sess.pop("pending_chunks_total", None)

            # Determine whether this turn is asking to continue chunked output.
            # Prefer model-based routing (turn_route_obj) over hard-coded command matching.
            route_now = str((turn_route_obj or {}).get("route") or "").strip().lower()
            wants_chunk_continue = False
            if chunk_policy == "manual" and sess.get("pending_chunks"):
                if route_now:
                    wants_chunk_continue = route_now == "chunk_continue"
                else:
                    wants_chunk_continue = _looks_like_continue(user_text_now)

            if (
                chunk_policy == "manual"
                and sess.get("pending_chunks")
                and wants_chunk_continue
            ):
                # Compute chunk progress BEFORE popping (pending_chunk_i points to the next chunk index)
                try:
                    _idx0 = int(sess.get("pending_chunk_i", 0))
                    _n0 = int(sess.get("pending_chunk_n", 0) or 0)
                except Exception:
                    _idx0, _n0 = 0, 0
                chunk_no = _idx0 + 1
                total_chunks = (
                    _n0
                    if _n0 > 0
                    else (
                        len(sess.get("pending_chunks") or [])
                        if isinstance(sess.get("pending_chunks"), list)
                        else 0
                    )
                )

                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"▶️ 已识别到继续：将输出剩余内容（第 {chunk_no}/{total_chunks} 段）",
                    done=False,
                )

                next_piece = self._pop_next_chunk(sess)
                if next_piece:
                    # Optional: show a short summary of this chunk in the status bar (uses a model)
                    try:
                        prog_enabled = _as_bool(
                            cfg.get("progress_summary_enabled"), False
                        )
                    except Exception:
                        prog_enabled = False

                    if prog_enabled and __event_emitter__:
                        try:
                            prog_every_n = max(
                                1,
                                _as_int(
                                    cfg.get("progress_summary_every_n_continues"), 1
                                ),
                            )
                        except Exception:
                            prog_every_n = 1

                        do_sum = (
                            (chunk_no % prog_every_n) == 0 if prog_every_n > 0 else True
                        )

                        if do_sum:
                            prog_model = (
                                str(cfg.get("progress_summary_model") or "").strip()
                                or str(cfg.get("judge_model") or "").strip()
                                or base_model
                            )
                            try:
                                prog_max_chars = max(
                                    400,
                                    _as_int(
                                        cfg.get("progress_summary_prompt_max_chars"),
                                        2500,
                                    ),
                                )
                            except Exception:
                                prog_max_chars = 2500
                            try:
                                prog_max_tokens = max(
                                    64,
                                    _as_int(
                                        cfg.get("progress_summary_max_tokens"), 220
                                    ),
                                )
                            except Exception:
                                prog_max_tokens = 220

                            piece_clean = str(next_piece or "")
                            try:
                                footer_pos = piece_clean.rfind("——\n（第")
                                if footer_pos != -1:
                                    piece_clean = piece_clean[:footer_pos].rstrip()
                            except Exception:
                                pass

                            summary_sys = (
                                "你是 Deep Research 的‘续写进度摘要器’。用户只能看到状态栏消息。\n"
                                "请根据输入的 chunk_excerpt，用中文给出：\n"
                                "1) summary：本段新增内容的极短摘要（<=80字）\n"
                                "2) progress：进度句（<=80字，包含第几段/总段数、累计字符等）\n"
                                '严格输出 JSON：{"summary":"...","progress":"..."}，不要输出多余文本。'
                            )
                            payload = {
                                "kind": "manual_chunk",
                                "chunk_no": int(chunk_no),
                                "total_chunks": int(total_chunks),
                                "chunk_chars": len(piece_clean),
                                "chunk_excerpt": _smart_truncate_middle(
                                    piece_clean, max_chars=prog_max_chars
                                ),
                            }

                            internal_md_sum = self._build_internal_metadata(
                                metadata_in,
                                base_model,
                                "chunk_progress_summary",
                                0,
                                sid,
                            )
                            try:
                                sum_res = await self._call_llm(
                                    __request__,
                                    user_obj,
                                    prog_model,
                                    [
                                        {"role": "system", "content": summary_sys},
                                        {
                                            "role": "user",
                                            "content": json.dumps(
                                                payload, ensure_ascii=False
                                            ),
                                        },
                                    ],
                                    internal_md_sum,
                                    forward_opts={
                                        "tool_choice": "none",
                                        "parallel_tool_calls": False,
                                        "max_tokens": prog_max_tokens,
                                    },
                                    temperature=0.2,
                                    tool_choice_override="none",
                                    tools_override=[],
                                )
                                sum_txt = str(_extract_text(sum_res) or "").strip()
                                j = _safe_json_loads(sum_txt) if sum_txt else None
                                if isinstance(j, dict):
                                    s1 = str(j.get("summary") or "").strip()
                                    s2 = str(j.get("progress") or "").strip()
                                    if s1:
                                        await self._status(
                                            __event_emitter__,
                                            ui_mode,
                                            sid,
                                            f"🧾 本段摘要：{s1}",
                                            done=False,
                                        )
                                    if s2:
                                        await self._status(
                                            __event_emitter__,
                                            ui_mode,
                                            sid,
                                            f"📈 {s2}",
                                            done=False,
                                        )
                            except Exception:
                                pass

                    return next_piece

            # If user typed something else while chunks pending, cancel chunk mode
            if (
                chunk_policy == "manual"
                and sess.get("pending_chunks")
                and user_text_now
                and (not wants_chunk_continue)
            ):
                sess.pop("pending_chunks", None)
                sess.pop("pending_chunks_meta", None)
                sess.pop("pending_chunks_total", None)
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    "🟟 已退出分段续写模式：将继续正常深度研究",
                    done=False,
                )

            # assumption mode from user
            if enable_assumption_mode and _looks_like_accept_any(user_text_now):
                sess["assumption_mode"] = True

            # paused handling (including manual stop)
            consumed_now = False
            if sess.get("paused") and user_text_now:
                pk = str(sess.get("pending_kind") or "")

                # Manual stop: user clicked "Stop" in UI, then sends a new message.
                if pk == "user_pause":
                    # Prefer model route to decide resume vs new instructions (no hard command).
                    wants_resume = False
                    if route_now:
                        wants_resume = route_now == "chunk_continue"
                    else:
                        wants_resume = _looks_like_continue(user_text_now)
                    if wants_resume:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "▶️ 已从用户暂停恢复：已识别“继续”，将从当前位置继续推进",
                            done=False,
                        )
                        # Do not append "继续" into user_text.
                        # Keep existing plan/step/answer; continue the loop as-is.
                    else:
                        short = self._shorten(user_text_now, 60)
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"▶️ 已从用户暂停恢复：收到补充内容「{short}」",
                            done=False,
                        )
                        sess["user_text"] = (
                            sess.get("user_text", "")
                            + "\n\n【用户补充】\n"
                            + user_text_now
                        ).strip()
                        sess["need_rescope"] = True
                        sess["need_replan"] = True

                # Other pauses (confirm/clarify)
                else:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        "▶️ 已收到你的回复：继续执行",
                        done=False,
                    )
                    sess["user_text"] = (
                        sess.get("user_text", "") + "\n\n【用户补充】\n" + user_text_now
                    ).strip()

                    if pk == "confirm":
                        if _looks_like_negative(user_text_now):
                            sess["confirmed"] = False
                            sess["need_rescope"] = True
                        else:
                            sess["confirmed"] = True

                    sess["need_rescope"] = True
                    sess["need_replan"] = True

                sess["paused"] = False
                sess["pause_reason"] = ""
                sess["pending_kind"] = ""
                sess["pending_question"] = ""
                consumed_now = True

            # Follow-up routing (patch / regenerate / supplement / new_task)
            if (
                (not consumed_now)
                and user_text_now
                and sess.get("final_done")
                and (sess.get("last_deliverable") or sess.get("deliverable"))
            ):
                try:
                    prev_spec = (
                        sess.get("last_deliverable_spec")
                        or sess.get("deliverable_spec")
                        or {}
                    )
                    prev_deliverable = (
                        sess.get("last_deliverable") or sess.get("deliverable") or ""
                    )
                    prev_target = sess.get("locked_target") or sess.get("target") or ""
                    prev_scoped = sess.get("scoped") or {}
                    prev_locked_target = sess.get("locked_target") or prev_target or ""

                    intent_obj = await self._classify_followup_intent(
                        __request__,
                        __event_emitter__,
                        ui_mode,
                        sid,
                        user_obj,
                        base_model,
                        metadata_in,
                        user_text_now,
                        prev_scoped=prev_scoped,
                        prev_locked_target=prev_locked_target,
                        prev_spec=prev_spec,
                        prev_deliverable=prev_deliverable,
                        context_pack=context_pack or sess.get("context_pack") or None,
                    )

                    intent = str(intent_obj.get("intent") or "patch").strip().lower()
                    pipeline = (
                        str(
                            intent_obj.get("pipeline")
                            or ("full" if intent == "new_task" else "fast")
                        )
                        .strip()
                        .lower()
                    )
                    # Prefer patch over regenerate for code unless user explicitly asks to rewrite.
                    prefer_patch = _as_bool(
                        cfg.get("prefer_patch_over_regenerate", True), True
                    )
                    if prefer_patch and intent == "regenerate":
                        try:
                            is_code_prev = (
                                isinstance(prev_spec, dict)
                                and str(prev_spec.get("deliverable_type") or "").lower()
                                == "code"
                            )
                        except Exception:
                            is_code_prev = False
                        low_msg = (user_text_now or "").lower()
                        explicit_rewrite = any(
                            k in low_msg
                            for k in [
                                "重写",
                                "重新写",
                                "从头",
                                "推翻",
                                "另起炉灶",
                                "完全重构",
                                "全量重构",
                                "rewrite",
                                "rebuild",
                                "start over",
                            ]
                        )
                        if is_code_prev and not explicit_rewrite:
                            intent = "patch"
                            pipeline = "fast"
                            intent_obj["intent"] = "patch"
                            intent_obj["pipeline"] = "fast"

                    # If we just ingested a full artifact from this user message, force patch/fast.
                    if (
                        sess.get("_ingested_user_deliverable_this_turn")
                        and intent == "new_task"
                    ):
                        intent = "patch"
                        pipeline = "fast"
                        intent_obj["intent"] = "patch"
                        intent_obj["pipeline"] = "fast"

                    # Fast path: do not restart deep-research loops for follow-up edits / regenerations / supplements
                    if (
                        intent in {"patch", "regenerate", "supplement"}
                        and pipeline == "fast"
                    ):
                        await self._emit_status(
                            __event_emitter__,
                            "Deep Research：🛠️ 追问处理：进入交付物修改模式（分阶段补丁并自动应用，不重跑全流程）",
                            "info",
                            include_timing=False,
                            ui_mode=ui_mode,
                        )

                        # Low-confidence safeguard: avoid accidentally treating an edit request as a new task / full rewrite.
                        try:
                            _conf = float(intent_obj.get("confidence") or 0.0)
                        except Exception:
                            _conf = 0.0

                        new_task_thr = _as_float(
                            cfg.get("followup_new_task_min_confidence"), 0.78
                        )
                        regen_thr = _as_float(
                            cfg.get("followup_regenerate_min_confidence"), 0.75
                        )

                        if (
                            intent == "new_task"
                            and _conf < new_task_thr
                            and prev_deliverable
                        ):
                            # If ambiguous, default to patch (safer; preserves the existing artifact).
                            msg = (user_text_now or "").strip()
                            msg_low = msg.lower()
                            looks_like_edit = any(
                                k in msg_low
                                for k in [
                                    "改",
                                    "修",
                                    "修复",
                                    "补丁",
                                    "patch",
                                    "fix",
                                    "bug",
                                    "优化",
                                    "update",
                                    "修改",
                                ]
                            )
                            if looks_like_edit or len(msg) <= 40:
                                intent = "patch"
                                pipeline = "fast"
                                intent_obj["intent"] = intent
                                intent_obj["pipeline"] = pipeline
                                intent_obj["reason"] = (
                                    (intent_obj.get("reason") or "")
                                    + f" | low-conf new_task({_conf:.2f}) => fallback patch"
                                )
                                if __event_emitter__:
                                    await self._status(
                                        __event_emitter__,
                                        f"⚠️ 追问意图(new_task)置信度偏低({_conf:.2f})，默认按修改上一版交付物处理。",
                                    )

                        if (
                            intent == "regenerate"
                            and _conf < regen_thr
                            and prev_is_code
                        ):
                            intent = "patch"
                            pipeline = "fast"
                            intent_obj["intent"] = intent
                            intent_obj["pipeline"] = pipeline
                            intent_obj["reason"] = (
                                (intent_obj.get("reason") or "")
                                + f" | low-conf regenerate({_conf:.2f}) => fallback patch"
                            )
                            if __event_emitter__:
                                await self._status(
                                    __event_emitter__,
                                    f"⚠️ 追问意图(regenerate)置信度偏低({_conf:.2f})，默认采用补丁式修改。",
                                )

                        # Clear any pending chunk state; user is issuing a new follow-up
                        sess["pending_chunks"] = []
                        sess["paused"] = False
                        sess["pending_kind"] = None
                        sess["pending_question"] = None

                        use_edit_mode = _as_bool(
                            cfg.get("enable_artifact_edit_mode", None),
                            bool(
                                getattr(self.valves, "ENABLE_ARTIFACT_EDIT_MODE", True)
                            ),
                        )
                        if use_edit_mode and intent in {"patch", "regenerate"}:
                            result = await self._handle_followup_edit_mode(
                                __request__,
                                __event_emitter__,
                                ui_mode,
                                sid,
                                user_obj,
                                base_model,
                                metadata_in,
                                intent_obj,
                                user_text_now,
                                max_output_continue_turns=max_output_continue_turns,
                                prev_target=prev_target,
                                prev_spec=prev_spec,
                                prev_deliverable=prev_deliverable,
                                prev_scoped=prev_scoped,
                            )
                        else:
                            result = await self._handle_followup_fast(
                                __request__,
                                __event_emitter__,
                                ui_mode,
                                sid,
                                user_obj,
                                base_model,
                                metadata_in,
                                intent_obj,
                                user_text_now,
                                max_output_continue_turns=max_output_continue_turns,
                                prev_target=prev_target,
                                prev_spec=prev_spec,
                                prev_deliverable=prev_deliverable,
                                prev_scoped=prev_scoped,
                            )

                        answer_text = str(result.get("answer") or "").strip()

                        # If the edit-mode flow requests a pause (e.g., needs one clarify answer), honor it and stop here.
                        if bool(result.get("pause")):
                            pause_kind = (
                                str(result.get("pause_kind") or "clarify")
                                .strip()
                                .lower()
                                or "clarify"
                            )
                            sess["paused"] = True
                            sess["pending_kind"] = pause_kind
                            sess["pending_question"] = answer_text
                            return answer_text

                        # Update deliverable only when we actually produced an updated deliverable
                        if bool(result.get("update_deliverable")):
                            new_spec = (
                                result.get("deliverable_spec")
                                if isinstance(result.get("deliverable_spec"), dict)
                                else (prev_spec or {})
                            )
                            new_deliverable = str(
                                result.get("deliverable") or answer_text
                            ).strip()

                            sess["last_deliverable"] = new_deliverable
                            sess["deliverable"] = new_deliverable
                            sess["last_deliverable_spec"] = new_spec
                            sess["deliverable_spec"] = new_spec
                            sess["final_done"] = True

                            # append history
                            hist = sess.get("deliverable_history")
                            if not isinstance(hist, list):
                                hist = []
                            hist.append(new_deliverable)
                            sess["deliverable_history"] = hist
                        else:
                            # Keep the last deliverable, but store supplement text for convenience
                            sess["last_supplement"] = answer_text

                        # Apply chunking only for non-code outputs (fast path returns directly).
                        # If we did NOT update the deliverable (supplement), treat the output as text.
                        if result.get("update_deliverable"):
                            spec_for_chunk = (
                                result.get("deliverable_spec")
                                if isinstance(result.get("deliverable_spec"), dict)
                                else (sess.get("deliverable_spec") or {})
                            )
                            is_code = (
                                str(spec_for_chunk.get("deliverable_type") or "")
                                .strip()
                                .lower()
                                == "code"
                            )
                        else:
                            is_code = False

                        if not is_code:
                            chunks = _chunk_text(
                                answer_text,
                                int(chunk_max_chars),
                                policy=chunk_policy,
                                is_code=False,
                            )
                            if chunks and len(chunks) > 1:
                                sess["pending_chunks"] = chunks[1:]
                                sess["paused"] = True
                                sess["pending_kind"] = "chunk"
                                sess["pending_question"] = (
                                    "（回复任意继续指令获取剩余内容，如：继续/接着/ok）"
                                )
                                return chunks[0] + "\n\n" + sess["pending_question"]

                        return answer_text

                    # Full path: treat as new task (or explicit full re-run)
                    if intent == "new_task" or pipeline == "full":
                        await self._emit_status(
                            __event_emitter__,
                            "Deep Research：🆕 追问处理：识别为新目标，将重新规划并生成",
                            "info",
                            include_timing=False,
                            ui_mode=ui_mode,
                        )

                        # Reset run state and run full Deep Research pipeline
                        def _reset_run_state(keep_target: bool) -> None:
                            sess["round_no"] = 0
                            sess["step_outputs"] = []
                            sess["plan_obj"] = None
                            sess["scoped"] = None
                            sess["deliverable_spec"] = None
                            sess["deliverable"] = None
                            sess["pending_chunks"] = []
                            sess["paused"] = False
                            sess["final_done"] = False
                            sess["need_rescope"] = True
                            sess["need_replan"] = True
                            sess["confirm_needed"] = False
                            sess["pending_question"] = None
                            sess["pending_kind"] = None
                            sess["timing"] = {
                                "started_at": time.time(),
                                "llm_calls": 0,
                                "llm_secs": 0.0,
                                "tool_calls": 0,
                                "tool_secs": 0.0,
                            }
                            if not keep_target:
                                sess["target"] = ""
                                sess["locked_target"] = ""

                        _reset_run_state(keep_target=False)
                        sess["user_text"] = user_text_now
                        consumed_now = True
                except Exception as e:
                    # Do NOT fall back to full deep-research loop when follow-up edit fails;
                    # that would waste time and often produces irrelevant reports.
                    # Instead, fall back to a safe follow-up generator (still based on last deliverable),
                    # or finally return the baseline deliverable unchanged.
                    try:
                        if __event_emitter__:
                            await self._status(
                                __event_emitter__,
                                ui_mode,
                                sid,
                                f"⚠️ 追问修改流程内部异常：{type(e).__name__}: {e}；将回退到安全修改生成（仍在上一版基础上输出完整交付物）",
                                done=False,
                            )
                    except Exception:
                        pass

                    try:
                        prev_spec = (
                            sess.get("last_deliverable_spec")
                            or sess.get("deliverable_spec")
                            or {}
                        )
                        prev_deliverable = (
                            sess.get("last_deliverable")
                            or sess.get("deliverable")
                            or ""
                        )
                        prev_target = (
                            sess.get("locked_target") or sess.get("target") or ""
                        )
                        prev_scoped = sess.get("scoped") or {}
                        intent_obj = {
                            "intent": "patch",
                            "pipeline": "fast",
                            "context_injection": "full",
                        }
                        result = await self._handle_followup_fast(
                            __request__,
                            __event_emitter__,
                            ui_mode,
                            sid,
                            user_obj,
                            base_model,
                            metadata_in,
                            intent_obj,
                            user_text_now,
                            max_output_continue_turns=max_output_continue_turns,
                            prev_target=prev_target,
                            prev_spec=prev_spec,
                            prev_deliverable=prev_deliverable,
                            prev_scoped=prev_scoped,
                        )
                        answer_text = str(result.get("answer") or "").strip()
                        if bool(result.get("update_deliverable")):
                            new_spec = (
                                result.get("deliverable_spec")
                                if isinstance(result.get("deliverable_spec"), dict)
                                else (prev_spec or {})
                            )
                            new_deliverable = str(
                                result.get("deliverable") or answer_text
                            ).strip()
                            sess["last_deliverable"] = new_deliverable
                            sess["deliverable"] = new_deliverable
                            sess["last_deliverable_spec"] = new_spec
                            sess["deliverable_spec"] = new_spec
                            sess["final_done"] = True
                        return answer_text
                    except Exception:
                        baseline = str(
                            sess.get("last_deliverable")
                            or sess.get("deliverable")
                            or ""
                        ).strip()
                        if baseline:
                            return baseline
                        return "⚠️ 追问修改失败：未能获取上一版交付物。请把需要修改的代码/文本粘贴出来，我会在同一份内容上打补丁修改并输出完整结果。"

            # first user_text init / append
            if not sess.get("user_text"):
                sess["user_text"] = user_text_now.strip()
                sess["need_rescope"] = True
            else:
                if (
                    (not consumed_now)
                    and user_text_now
                    and (not _looks_like_continue(user_text_now))
                    and (not sess.get("paused"))
                ):
                    sess["user_text"] = (
                        sess.get("user_text", "") + "\n\n【用户补充】\n" + user_text_now
                    ).strip()
                    sess["need_rescope"] = True

            user_text = (sess.get("user_text") or "").strip()
            if not user_text:
                return "Deep Research: 没有用户输入"

            # time context (only when likely needed)
            time_context = (
                _build_time_context_block() if _looks_time_sensitive(user_text) else ""
            )

            # Friendly start/resume banner (avoid looking like a brand-new run after pause)
            if is_new_session or int(sess.get("request_seq", 1)) <= 1:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"🚀 启动（base={base_model}，来源={base_src}）",
                    done=False,
                )
            else:
                # If we have no meaningful progress yet (e.g. an early crash before any round),
                # avoid showing a confusing "恢复会话" banner.
                done_r = int(sess.get("round_no", 0))
                has_history = False
                try:
                    timing = (
                        sess.get("timing")
                        if isinstance(sess.get("timing"), dict)
                        else {}
                    )
                    has_history = bool(
                        done_r > 0
                        or bool(sess.get("deliverable") or sess.get("last_deliverable"))
                        or bool(sess.get("scoped") or sess.get("planned"))
                        or bool(sess.get("paused"))
                        or bool(
                            (timing or {}).get("llm_calls")
                            or (timing or {}).get("tool_calls")
                        )
                    )
                except Exception:
                    has_history = False

                if not has_history:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"🚀 启动（base={base_model}，来源={base_src}）",
                        done=False,
                    )
                else:
                    next_r = done_r + 1
                    extra = ""
                    if sess.get("need_rescope") or sess.get("need_replan"):
                        extra = "；将根据补充更新需求/计划"
                    if done_r >= int(max_rounds):
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🔁 已恢复会话（已达最大{max_rounds}轮；将直接汇总输出）",
                            done=False,
                        )
                    else:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🔁 已恢复会话：已完成{done_r}轮，将从第{next_r}轮继续（最大{max_rounds}轮）{extra}",
                            done=False,
                        )

            # tools
            tool_registry = _build_tool_registry(__tools__)
            tools_payload = _tools_to_openai(__tools__) if tool_registry else None
            forward_opts = self._extract_forward_opts(body, tools_payload)

            # tool enabling: if max_tool_calls_per_turn <=0, disable tools
            if max_tool_calls_per_turn <= 0:
                forward_opts["tool_choice"] = "none"
                forward_opts["tools"] = []

            injection = {
                "__event_emitter__": __event_emitter__,
                "__request__": __request__,
                "__user__": __user__,
                "user": user_obj,
                "__metadata__": metadata_in,
                "__files__": __files__ or [],
                "__model__": __model__ or {},
            }

            # ----------------------------
            # Scope
            # ----------------------------
            if sess.get("need_rescope") or not sess.get("scoped"):
                scoped = await self._scope(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    base_model,
                    metadata_in,
                    user_text,
                    str(sess.get("locked_target") or ""),
                    bool(sess.get("assumption_mode")),
                    str(sess.get("conversation_context") or ""),
                )

                target = str(scoped.get("target") or "").strip()
                target_locked = _as_bool(scoped.get("target_locked"), False)
                need_clar = _as_bool(scoped.get("need_clarification"), False)
                q = str(scoped.get("question") or "").strip()
                confirm_rec = _as_bool(scoped.get("confirm_recommended"), False)
                cq = str(scoped.get("confirm_question") or "").strip()

                if target:
                    sess["locked_target"] = (
                        target
                        if (target_locked or not sess.get("locked_target"))
                        else str(sess.get("locked_target") or target)
                    )
                elif sess.get("locked_target"):
                    scoped["target"] = str(sess.get("locked_target"))
                else:
                    need_clar = True
                    q = q or "你要研究的标的是哪一个？（例如：公司/股票名称或代码）"

                # confirm policy override
                if confirm_policy == "never":
                    confirm_rec = False
                elif confirm_policy in ("always", "pause"):
                    confirm_rec = True

                # show alignment
                if show_alignment:
                    align_txt = self._make_alignment_status(
                        scoped, str(sess.get("locked_target") or "")
                    )
                    if align_txt:
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "✅ " + align_txt,
                            done=False,
                        )

                # default confirm question
                if confirm_rec and (not cq):
                    cq = self._default_confirm_question(
                        scoped, str(sess.get("locked_target") or "")
                    )

                if confirm_rec and sess.get("assumption_mode"):
                    confirm_rec = False

                sess["scoped"] = scoped
                sess["need_rescope"] = False

                # hard clarification
                if enable_clarify and need_clar and q:
                    if sess.get("pending_question") == q:
                        sess["pending_q_repeat"] = (
                            int(sess.get("pending_q_repeat", 0)) + 1
                        )
                    else:
                        sess["pending_q_repeat"] = 0
                    if int(sess.get("pending_q_repeat", 0)) > int(clarify_max_repeat):
                        sess["assumption_mode"] = True
                    else:
                        sess["paused"] = True
                        sess["pending_kind"] = "clarify"
                        sess["pending_question"] = q
                        _SESS[sid] = sess
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "⏸️ 需要补充：已暂停等待你的回答",
                            done=True,
                        )
                        return q

                # confirmation policy
                confirm_pause = confirm_policy == "pause"
                if (
                    enable_clarify
                    and confirm_pause
                    and confirm_rec
                    and (not sess.get("confirmed"))
                ):
                    if cq and self._is_generic_confirm_question(cq):
                        cq = ""
                    confirm_msg = self._compose_confirm_message(
                        scoped=scoped,
                        locked_target=str(sess.get("locked_target") or ""),
                        extra_question=cq,
                    )
                    sess["paused"] = True
                    sess["pending_kind"] = "confirm"
                    sess["pending_question"] = confirm_msg
                    _SESS[sid] = sess
                    if ui_mode == "debug":
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "⏸️ 已暂停：等待你回复任意继续指令（继续/接着/ok）或补充修改点",
                            done=True,
                        )
                    return confirm_msg

                if confirm_rec and (not confirm_pause):
                    sess["confirmed"] = True

            scoped = sess.get("scoped") or {}
            locked_target = str(
                sess.get("locked_target") or scoped.get("target") or ""
            ).strip()
            if locked_target:
                scoped["target"] = locked_target

            # ----------------------------
            # Plan
            # ----------------------------
            if (not sess.get("plan")) or sess.get("need_replan"):
                plan_obj = await self._plan(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    planner_model,
                    metadata_in,
                    scoped,
                    time_context=time_context,
                )
                sess["plan"] = plan_obj
                sess["step_idx"] = 0
                sess["need_replan"] = False

            plan_obj = sess.get("plan") or {}
            step_idx = int(sess.get("step_idx", 0))
            last_answer = str(sess.get("last_answer") or "").strip()

            # ----------------------------
            # Loop
            # ----------------------------
            final_text = ""

            # Continue round numbering across resume (so UI does not look like a new run)
            start_round = int(sess.get("round_no", 0)) + 1
            if start_round > int(max_rounds):
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"⚠️ 已达到最大轮数（{max_rounds}轮），将直接汇总输出",
                    done=False,
                )
            for r in range(max(1, start_round), max_rounds + 1):
                step_head, step_info = self._format_step_info(
                    plan_obj if isinstance(plan_obj, dict) else {}, step_idx
                )

                # If we have a stored "improve brief" from last round, show it so the iteration looks/feels incremental.
                _ib = str(sess.get("improve_brief") or "").strip()
                if _ib:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        "🎯 本轮改进：" + self._shorten(_ib, 140),
                        done=False,
                    )

                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"🔄 第{r}轮开始（最大{max_rounds}轮）：{step_head or '研究推进'}",
                    done=False,
                )

                gen_msgs: List[dict] = [
                    {"role": "system", "content": self.valves.GENERATE_SYSTEM_PROMPT}
                ]

                if time_context:
                    gen_msgs.append({"role": "system", "content": time_context.strip()})

                if locked_target:
                    gen_msgs.append(
                        {
                            "role": "system",
                            "content": f"【锁定研究对象】{locked_target}\n除非用户明确要求更换，否则禁止更换研究对象。",
                        }
                    )

                if step_info:
                    gen_msgs.append({"role": "system", "content": step_info})

                assumptions = (
                    scoped.get("assumptions")
                    if isinstance(scoped.get("assumptions"), list)
                    else []
                )
                focus = (
                    scoped.get("focus") if isinstance(scoped.get("focus"), list) else []
                )
                if assumptions:
                    gen_msgs.append(
                        {
                            "role": "system",
                            "content": "【默认假设】\n"
                            + "\n".join(
                                [f"- {str(a)}" for a in assumptions if str(a).strip()]
                            ),
                        }
                    )
                if focus:
                    gen_msgs.append(
                        {
                            "role": "system",
                            "content": "【研究侧重点】\n"
                            + "\n".join(
                                [f"- {str(f)}" for f in focus if str(f).strip()]
                            ),
                        }
                    )

                if sess.get("assumption_mode"):
                    gen_msgs.append(
                        {
                            "role": "system",
                            "content": "用户表示“随便/都可以/你决定”。请用默认假设推进，并给出可调整项。",
                        }
                    )
                # Provide iteration focus + current step draft (if any) so this round is clearly an improvement, not a restart.
                _improve = str(sess.get("improve_brief") or "").strip()
                if _improve:
                    gen_msgs.append(
                        {"role": "system", "content": "【本轮改进重点】\n" + _improve}
                    )

                _draft = self._get_step_output_draft(
                    sess, int(step_idx), max_chars=6000
                )
                if _draft:
                    gen_msgs.append(
                        {
                            "role": "system",
                            "content": "【当前步骤已有草稿（请在其基础上补全并输出完整改进版；不要只输出补丁）】\n"
                            + _draft,
                        }
                    )

                gen_msgs.append({"role": "user", "content": user_text})

                internal_md = self._build_internal_metadata(
                    metadata_in, base_model, "generate", r, sid
                )

                candidate, gstats = await self._generate_with_tool_loop(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    base_model,
                    gen_msgs,
                    internal_md,
                    forward_opts,
                    tool_registry,
                    injection,
                    sess,
                    max_tool_turns=max_tool_turns,
                    max_tool_calls_per_turn=max_tool_calls_per_turn,
                    max_output_continue_turns=max_output_continue_turns,
                    show_tool_detail=show_tool_detail,
                )

                # timing accumulate
                _t = sess.get("timing") if isinstance(sess.get("timing"), dict) else {}
                _t["llm_calls"] = int(_t.get("llm_calls", 0) or 0) + int(
                    gstats.get("llm_calls", 0) or 0
                )
                _t["tool_calls"] = int(_t.get("tool_calls", 0) or 0) + int(
                    gstats.get("tool_calls", 0) or 0
                )
                _t["llm_secs"] = float(_t.get("llm_secs", 0.0) or 0.0) + float(
                    gstats.get("llm_secs", 0.0) or 0.0
                )
                _t["tool_secs"] = float(_t.get("tool_secs", 0.0) or 0.0) + float(
                    gstats.get("tool_secs", 0.0) or 0.0
                )
                _t["started_at"] = float(_t.get("started_at", _now()) or _now())
                sess["timing"] = _t

                candidate = (candidate or "").strip()
                if not candidate:
                    sess["round_no"] = int(r)
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        "⚠️ 本轮生成为空，尝试下一轮…",
                        done=False,
                    )
                    continue

                last_answer = candidate
                sess["last_answer"] = last_answer

                # store per-step output for final compile (so final answer is not just the last step)
                try:
                    step_title = ""
                    plan_list = (
                        plan_obj.get("plan") if isinstance(plan_obj, dict) else None
                    )
                    if (
                        isinstance(plan_list, list)
                        and 0 <= int(step_idx) < len(plan_list)
                        and isinstance(plan_list[int(step_idx)], dict)
                    ):
                        step_title = str(
                            plan_list[int(step_idx)].get("title") or ""
                        ).strip()
                    self._upsert_step_output(
                        sess=sess,
                        step_idx=int(step_idx),
                        step_head=str(step_head or ""),
                        step_title=step_title,
                        content=candidate,
                    )
                except Exception:
                    pass

                # judge (with plan + step idx so it can update)
                internal_md_j = self._build_internal_metadata(
                    metadata_in, base_model, "judge", r, sid
                )
                j = await self._judge(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    judge_model,
                    internal_md_j,
                    scoped,
                    step_head,
                    candidate,
                    current_plan=(plan_obj if isinstance(plan_obj, dict) else {}),
                    step_idx=step_idx,
                )

                # pretty UI status from judge
                ui_status_text = str(j.get("ui_status") or "").strip()
                if ui_status_text:
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        "🧾 评审：" + self._shorten(ui_status_text, 120),
                        done=False,
                    )

                # decider
                internal_md_d = self._build_internal_metadata(
                    metadata_in, base_model, "decide", r, sid
                )
                d = await self._decide(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    base_model,
                    internal_md_d,
                    scoped,
                    step_head,
                    candidate,
                    j,
                    bool(sess.get("assumption_mode")),
                )

                decision = str(d.get("decision") or "").strip().lower()
                question = str(d.get("question") or "").strip()
                advance_step = _as_bool(d.get("advance_step"), False) or _as_bool(
                    j.get("step_done"), False
                )
                next_instr = str(d.get("next_instructions") or "").strip()

                if ui_mode == "debug":
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        f"decider={decision} advance_step={advance_step}",
                        done=False,
                    )

                # minimum rounds
                if r < min_rounds and decision == "done":
                    decision = "continue"

                if decision == "ask" and enable_clarify and question:
                    if sess.get("pending_question") == question:
                        sess["pending_q_repeat"] = (
                            int(sess.get("pending_q_repeat", 0)) + 1
                        )
                    else:
                        sess["pending_q_repeat"] = 0
                    if int(sess.get("pending_q_repeat", 0)) > int(clarify_max_repeat):
                        sess["assumption_mode"] = True
                        decision = "continue"
                    else:
                        sess["round_no"] = int(r)
                        sess["paused"] = True
                        sess["pending_kind"] = "clarify"
                        sess["pending_question"] = question
                        _SESS[sid] = sess
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "⏸️ 需要补充：已暂停等待你的回答",
                            done=True,
                        )
                        return question

                if decision == "done":
                    sess["round_no"] = int(r)
                    sess["improve_brief"] = ""
                    final_text = candidate
                    sess["final_done"] = True
                    break

                # Dynamic plan update (from judge)
                plan_updated = False
                if enable_dynamic_plan:
                    plan_update = j.get("plan_update")
                    if (
                        isinstance(plan_update, dict)
                        and isinstance(plan_update.get("plan"), list)
                        and plan_update.get("plan")
                    ):
                        plan_obj = plan_update
                        sess["plan"] = plan_obj
                        plan_updated = True

                        ns = j.get("next_step_idx")
                        ns_int = None
                        try:
                            ns_int = int(ns)
                        except Exception:
                            ns_int = None
                        if ns_int is not None and ns_int >= 0:
                            step_idx = max(0, min(ns_int, len(plan_update["plan"]) - 1))
                        else:
                            step_idx = max(
                                0, min(step_idx, len(plan_update["plan"]) - 1)
                            )
                        sess["step_idx"] = step_idx

                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            f"🗺️ 计划已更新：{len(plan_update['plan'])}步；下一步={step_idx+1}/{len(plan_update['plan'])}",
                            done=False,
                        )

                # continue: advance step if requested (only when plan NOT updated this round)
                if (
                    (not plan_updated)
                    and advance_step
                    and isinstance(plan_obj.get("plan"), list)
                ):
                    step_idx = min(step_idx + 1, len(plan_obj["plan"]) - 1)
                    sess["step_idx"] = step_idx
                # Build a concise improve brief so later rounds feel incremental (and the model focuses on fixing gaps).
                try:
                    _missing0 = str(j.get("missing") or "").strip()
                    _sugs0 = j.get("suggestions")
                    if not isinstance(_sugs0, list):
                        _sugs0 = []
                    _sugs0 = [str(x).strip() for x in _sugs0 if str(x).strip()]
                    _parts = []
                    if _missing0:
                        _parts.append("缺口：" + _missing0)
                    if _sugs0:
                        _parts.append("建议：" + "；".join(_sugs0[:3]))
                    if next_instr:
                        _parts.append("决策：" + str(next_instr).strip())
                    sess["improve_brief"] = self._shorten(
                        "；".join([p for p in _parts if p]), 260
                    )
                    if sess.get("improve_brief"):
                        await self._status(
                            __event_emitter__,
                            ui_mode,
                            sid,
                            "🛠️ 下一轮重点："
                            + self._shorten(sess.get("improve_brief"), 140),
                            done=False,
                        )
                except Exception:
                    pass

                # Next user_text update (judge can rewrite)
                next_user_text = str(j.get("next_user_text") or "").strip()
                if enable_judge_rewrite and next_user_text:
                    user_text = next_user_text
                    sess["user_text"] = user_text
                    await self._status(
                        __event_emitter__,
                        ui_mode,
                        sid,
                        "🪄 已应用评审改写：下一轮将使用优化后的请求",
                        done=False,
                    )
                else:
                    refine = ""
                    missing = str(j.get("missing") or "").strip()
                    suggestions = j.get("suggestions")
                    if not isinstance(suggestions, list):
                        suggestions = []
                    suggestions = [
                        str(x).strip() for x in suggestions if str(x).strip()
                    ]

                    if missing or suggestions or next_instr:
                        refine += "\n\n【补全指令】\n"
                        if missing:
                            refine += "缺口：\n" + missing + "\n"
                        if suggestions:
                            refine += (
                                "建议：\n"
                                + "\n".join([f"- {s}" for s in suggestions])
                                + "\n"
                            )
                        if next_instr:
                            refine += "主模型决定：\n" + next_instr + "\n"
                        refine += "\n请输出：当前步骤的【完整改进版章节文本】（已合并上轮草稿 + 本轮补全点；不要只输出补丁）。\n"

                    if not refine:
                        refine = "\n\n请继续推进当前步骤：在不删减已完成信息的前提下，补全缺口并输出【当前步骤的完整交付片段】（可直接用于最终交付物）。\n"

                    user_text = (user_text + refine).strip()
                    sess["user_text"] = user_text

                # Mark this round as completed (for stable resume UI numbering)
                sess["round_no"] = int(r)

            # if not done, return best-so-far
            if not final_text:
                final_text = last_answer or "（模型未输出正文）"
                sess["final_done"] = False

            raw_final = final_text

            # final compile: stitch all step outputs into a comprehensive report
            if enable_final_compile:
                compiled, cstats = await self._compile_final_report(
                    __request__,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    user_obj,
                    compile_model,
                    metadata_in,
                    scoped,
                    plan_obj,
                    sess,
                    time_context,
                    raw_final,
                    compile_max_output_continue_turns,
                    final_min_chars,
                    compile_step_output_max_chars,
                )
                if compiled:
                    final_text = compiled
                    sess["final_done"] = True
                    try:
                        sess["timing"]["llm_calls"] = int(
                            sess["timing"].get("llm_calls", 0)
                        ) + int(cstats.get("llm_calls", 0))
                        sess["timing"]["llm_secs"] = float(
                            sess["timing"].get("llm_secs", 0.0)
                        ) + float(cstats.get("llm_secs", 0.0))
                    except Exception:
                        pass

            # Postprocess code deliverable (ultra-long stitch review & patch)
            try:
                spec = sess.get("deliverable_spec") or {}
                is_code = bool(spec.get("need_eof_marker")) or (
                    str(spec.get("mode") or "").lower() == "code"
                )
                if is_code:
                    final_text = await self._finalize_code_deliverable(
                        __request__,
                        user_obj,
                        compile_model,
                        metadata_in,
                        scoped,
                        __event_emitter__,
                        ui_mode,
                        sid,
                        spec,
                        final_text,
                    )
            except Exception:
                pass

            # Safety: inject any charts once more (even if generated in early rounds).
            try:
                charts_all = (
                    sess.get("chart_blocks")
                    if isinstance(sess.get("chart_blocks"), list)
                    else []
                )
                if charts_all:
                    spec = (
                        sess.get("deliverable_spec")
                        if isinstance(sess.get("deliverable_spec"), dict)
                        else {}
                    )
                    is_code = bool(spec.get("need_eof_marker"))
                    if (not is_code) or ("[[CHART" in (final_text or "")):
                        final_text = _inject_chart_blocks(final_text or "", charts_all)
            except Exception:
                pass

            # Final audit gate (code + ultra-long deliverables): model self-check + patch until audit passes.
            try:
                spec = (
                    sess.get("deliverable_spec")
                    if isinstance(sess.get("deliverable_spec"), dict)
                    else {}
                )
                final_text = await self._final_audit_deliverable(
                    __request__,
                    user_obj,
                    compile_model,
                    metadata_in,
                    scoped,
                    __event_emitter__,
                    ui_mode,
                    sid,
                    spec,
                    final_text,
                )
            except Exception:
                pass

            # timing summary
            elapsed = _now() - float(sess["timing"].get("started_at", _now()))
            llm_calls = int(sess["timing"].get("llm_calls", 0))
            tool_calls_n = int(sess["timing"].get("tool_calls", 0))
            llm_secs = float(sess["timing"].get("llm_secs", 0.0))
            tool_secs = float(sess["timing"].get("tool_secs", 0.0))

            if show_timing:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"✅ 完成：耗时 {elapsed:.2f}s（LLM {llm_calls}次/{llm_secs:.2f}s，工具 {tool_calls_n}次/{tool_secs:.2f}s）",
                    done=True,
                )
            else:
                await self._status(
                    __event_emitter__, ui_mode, sid, "✅ 完成", done=True
                )

            if append_timing_to_final:
                final_text = (
                    final_text.rstrip()
                    + f"\n\n——\n耗时 {elapsed:.2f}s（LLM {llm_calls}次/{llm_secs:.2f}s，工具 {tool_calls_n}次/{tool_secs:.2f}s）"
                )

            # Persist the latest deliverable for follow-ups
            try:
                sess["last_deliverable"] = final_text
                sess["last_deliverable_spec"] = sess.get("deliverable_spec")
                sess["last_deliverable_at"] = _now()
            except Exception:
                pass

            # Persist a normalized copy to disk artifact cache (best-effort). This helps when the front-end/server
            # truncates long code blocks and the follow-up request no longer contains the baseline deliverable.
            try:
                cache_enabled = _as_bool(
                    (
                        cfg.get("artifact_cache_enabled")
                        if isinstance(cfg, dict) and "artifact_cache_enabled" in cfg
                        else (
                            cfg.get("artifact_persist_enabled")
                            if isinstance(cfg, dict)
                            and "artifact_persist_enabled" in cfg
                            else None
                        )
                    ),
                    bool(self.valves.ARTIFACT_CACHE_ENABLED),
                )
                if cache_enabled:
                    cache_dir = ""
                    if isinstance(cfg, dict):
                        cache_dir = str(
                            cfg.get("artifact_cache_dir")
                            or cfg.get("artifact_store_dir")
                            or ""
                        ).strip()
                    if not cache_dir:
                        cache_dir = str(
                            getattr(self.valves, "ARTIFACT_CACHE_DIR", "") or ""
                        ).strip()

                    max_chars = int(
                        (
                            cfg.get("artifact_cache_max_chars")
                            if isinstance(cfg, dict)
                            and cfg.get("artifact_cache_max_chars") is not None
                            else getattr(
                                self.valves, "ARTIFACT_CACHE_MAX_CHARS", 400000
                            )
                        )
                        or 400000
                    )
                    history_max = int(
                        (
                            cfg.get("artifact_cache_history_max")
                            if isinstance(cfg, dict)
                            and cfg.get("artifact_cache_history_max") is not None
                            else getattr(self.valves, "ARTIFACT_CACHE_HISTORY_MAX", 20)
                        )
                        or 20
                    )

                    root = _pick_artifact_store_root(cache_dir)

                    # Choose normalized content to save: strip code fences if needed.
                    spec_to_save = (
                        sess.get("deliverable_spec")
                        if isinstance(sess.get("deliverable_spec"), dict)
                        else {}
                    )
                    content_to_save = final_text or ""
                    if (
                        isinstance(spec_to_save, dict)
                        and str(spec_to_save.get("mode") or "").lower() == "code"
                    ):
                        prefer_lang = str(
                            spec_to_save.get("code_language") or "text"
                        ).lower()
                        cand = _extract_code_candidate(
                            content_to_save, prefer_lang=prefer_lang
                        )
                        if cand.strip():
                            code_norm = _normalize_code_candidate(cand, prefer_lang)
                            eof_marker = str(
                                getattr(
                                    self.valves, "DEFAULT_CODE_EOF_MARKER", "DR_EOF"
                                )
                            )
                            eof_line = _eof_line_for_lang(prefer_lang, eof_marker)
                            code_norm = _ensure_eof_marker(
                                code_norm, eof_marker, eof_line=eof_line
                            )
                            content_to_save = code_norm

                    chat_id = ""
                    try:
                        drmd = (
                            metadata_in.get("deep_research")
                            if isinstance(metadata_in.get("deep_research"), dict)
                            else {}
                        )
                        chat_id = str(drmd.get("chat_id") or "").strip()
                    except Exception:
                        chat_id = ""

                    meta = _artifact_cache_write(
                        root=root,
                        session_id=sid,
                        chat_id=chat_id,
                        content=content_to_save,
                        spec=spec_to_save,
                        max_chars=max_chars,
                        history_max=history_max,
                    )
                    if isinstance(meta, dict):
                        sess["artifact_cache_last_id"] = meta.get("id") or ""
                        sess["artifact_cache_last_path"] = meta.get("path") or ""
            except Exception:
                pass
            # chunking
            out = self._apply_chunking(sess, final_text, chunk_max_chars, chunk_policy)
            _SESS[sid] = sess

            # keep session for follow-ups; it will be GC'ed by TTL cleanup
            if sess.get("final_done") and (not sess.get("pending_chunks")):
                try:
                    sess["completed_at"] = _now()
                except Exception:
                    pass

            return out

        except asyncio.CancelledError:
            # User clicked "Stop" in OpenWebUI. Persist state and mark paused.
            try:
                sess["paused"] = True
                sess["pause_reason"] = "user_pause"
                sess["pending_kind"] = "user_pause"
                sess["pending_question"] = ""
                sess["updated_at"] = _now()
                _SESS[sid] = sess
            except Exception:
                pass

            # Try to notify front-end (best-effort; may fail if cancellation is hard)
            try:
                if __event_emitter__:
                    await asyncio.shield(
                        self._emit(
                            __event_emitter__,
                            "Deep Research：⏸️ 暂停原因：用户暂停（你可以直接补充内容后发送）",
                            done=True,
                        )
                    )
            except Exception:
                pass

            # Give underlying HTTP transports a brief moment to close gracefully,
            # which can reduce noisy warnings like "Event loop is closed" on some runtimes.
            try:
                await asyncio.shield(asyncio.sleep(0.05))
            except Exception:
                pass

            # IMPORTANT: do not re-raise CancelledError here; return gracefully so the runner can finish cleanly.
            return ""

        except Exception as e:
            # Unexpected runtime error: pause the session and notify the front-end.
            err = f"{type(e).__name__}: {e}"
            try:
                sess["paused"] = True
                sess["pause_reason"] = "error"
                sess["pending_kind"] = "error"
                sess["pending_question"] = ""
                sess["last_error"] = err
                sess["updated_at"] = _now()
                _SESS[sid] = sess
            except Exception:
                pass

            short_err = err
            try:
                short_err = self._shorten(err, 200)
            except Exception:
                pass

            try:
                await self._status(
                    __event_emitter__,
                    ui_mode,
                    sid,
                    f"❌ 发生错误：{short_err}（已暂停，可直接补充后发送，或 /reset 重置）",
                    done=True,
                )
            except Exception:
                pass

            # Also return a visible assistant message (some UIs may not keep status history).
            return (
                "❌ 本次深度研究在执行中发生错误，已暂停。\n\n"
                f"错误信息：{short_err}\n\n"
                "你可以：\n"
                "- 直接补充/改写需求后再次发送（会从当前会话继续）\n"
                "- 或发送 /reset 清空本次会话后重新开始\n"
            )

        finally:
            # Stop heartbeat task (if any) to avoid background tasks leaking across requests.
            try:
                if heartbeat_task:
                    heartbeat_task.cancel()
                    await asyncio.gather(heartbeat_task, return_exceptions=True)
            except Exception:
                pass
